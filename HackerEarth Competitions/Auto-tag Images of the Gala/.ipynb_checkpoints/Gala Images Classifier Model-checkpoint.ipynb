{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningratefinder import LearningRateFinder\n",
    "from clr_callback import CyclicLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set file path for training and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = \"dataset/Train Images\"\n",
    "test_img_path = \"dataset/Test Images\"\n",
    "train_true_label = \"dataset/train.csv\"\n",
    "prediction_file = \"dataset/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read true label data for training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_true_label_df = pd.read_csv(train_true_label)\n",
    "train_true_label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countplot for different classes in training data\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Class\", data=train_true_label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical classes into numerical representations\n",
    "train_true_label_df['Class_enc'] = train_true_label_df['Class'].factorize()[0]\n",
    "train_true_label_df.groupby(['Class','Class_enc']).size().reset_index().rename(columns={0:'Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create feature matrix for training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set scaling values for height and width of image\n",
    "scale_height = 32\n",
    "scale_width = 32\n",
    "\n",
    "# Initialize list to store tensor variables for different images\n",
    "scaled_image_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in train_true_label_df['Image']:\n",
    "    \n",
    "    # Set file path for input image\n",
    "    file_path = train_img_path + \"/\" + file\n",
    "    \n",
    "    # Read input image using OpenCV\n",
    "    orig_img = cv2.imread(file_path)\n",
    "    orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Scale the input image, using scaling values\n",
    "    scaled_img = cv2.resize(orig_img, (int(scale_width), int(scale_height)))\n",
    "    \n",
    "    # Once scaled, add the scaled_img variable to list\n",
    "    scaled_image_list.append(scaled_img)\n",
    "    \n",
    "Xtrain = np.array(scaled_image_list)\n",
    "print(\"Xtrain shape: {}\".format(Xtrain.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create feature matrix for test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read prediction file\n",
    "prediction_df = pd.read_csv(prediction_file)\n",
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store tensor variables for different images\n",
    "scaled_image_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in prediction_df['Image']:\n",
    "    \n",
    "    # Set file path for input image\n",
    "    file_path = test_img_path + \"/\" + file\n",
    "    \n",
    "    # Read input image using OpenCV\n",
    "    orig_img = cv2.imread(file_path)\n",
    "    orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Scale the input image, using scaling values\n",
    "    scaled_img = cv2.resize(orig_img, (int(scale_width), int(scale_height)))\n",
    "    \n",
    "    # Once scaled, add the scaled_img variable to list\n",
    "    scaled_image_list.append(scaled_img)\n",
    "    \n",
    "Xpredict = np.array(scaled_image_list)\n",
    "print(\"Xpredict shape: {}\".format(Xpredict.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create true labels vector for training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = np.array([train_true_label_df['Class_enc'].to_numpy()]).T\n",
    "print(\"Ytrain shape: {}\".format(Ytrain.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data into train/validation/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=1)\n",
    "for train_index, validation_index in sss.split(Xtrain, Ytrain):\n",
    "    train_x, validation_x = Xtrain[train_index], Xtrain[validation_index]\n",
    "    train_y, validation_y = Ytrain[train_index], Ytrain[validation_index]\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=1)\n",
    "for train_index, test_index in sss.split(train_x, train_y):\n",
    "    train_x, test_x = train_x[train_index], train_x[test_index]\n",
    "    train_y, test_y = train_y[train_index], train_y[test_index]\n",
    "\n",
    "print(\"------------------------- Training Dataset -------------------------\")\n",
    "print(\"train_x shape: {}\".format(train_x.shape))\n",
    "print(\"train_y shape: {}\".format(train_y.shape))\n",
    "\n",
    "print(\"\\n------------------------- Validation Dataset -------------------------\")\n",
    "print(\"validation_x shape: {}\".format(validation_x.shape))\n",
    "print(\"validation_y shape: {}\".format(validation_y.shape))\n",
    "\n",
    "print(\"\\n------------------------- Test Dataset -------------------------\")\n",
    "print(\"test_x shape: {}\".format(test_x.shape))\n",
    "print(\"test_y shape: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'Class': train_y[:, 0]})\n",
    "train_df.groupby(['Class']).size().reset_index().rename(columns={0:'Count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.DataFrame({'Class': validation_y[:, 0]})\n",
    "validation_df.groupby(['Class']).size().reset_index().rename(columns={0:'Count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'Class': test_y[:, 0]})\n",
    "test_df.groupby(['Class']).size().reset_index().rename(columns={0:'Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE()\n",
    "train_x_flatten = train_x.reshape(train_x.shape[0], -1)\n",
    "sm_x, sm_y = sm.fit_sample(train_x_flatten, train_y.ravel())\n",
    "train_x = sm_x.reshape(sm_x.shape[0], train_x.shape[1], train_x.shape[2], train_x.shape[3])\n",
    "train_y = np.array([sm_y]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'Class': train_y[:, 0]})\n",
    "train_df.groupby(['Class']).size().reset_index().rename(columns={0:'Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert train_y/validation_y/test_y into one-hot encode matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = tf.keras.utils.to_categorical(train_y, 4)\n",
    "validation_y = tf.keras.utils.to_categorical(validation_y, 4)\n",
    "test_y = tf.keras.utils.to_categorical(test_y, 4)\n",
    "\n",
    "print(\"------------------------- Training Dataset -------------------------\")\n",
    "print(\"train_x shape: {}\".format(train_x.shape))\n",
    "print(\"train_y shape: {}\".format(train_y.shape))\n",
    "\n",
    "print(\"\\n------------------------- Validation Dataset -------------------------\")\n",
    "print(\"validation_x shape: {}\".format(validation_x.shape))\n",
    "print(\"validation_y shape: {}\".format(validation_y.shape))\n",
    "\n",
    "print(\"\\n------------------------- Test Dataset -------------------------\")\n",
    "print(\"test_x shape: {}\".format(test_x.shape))\n",
    "print(\"test_y shape: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the datasets in NPZ file (for reusability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('dataset/Auto_Tag_Gala_Images_dataset.npz',\n",
    "                    Xtrain=train_x, Ytrain=train_y,\n",
    "                    Xvalidation=validation_x, Yvalidation=validation_y,\n",
    "                    Xtest=test_x, Ytest=test_y,\n",
    "                    Xpredict=Xpredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets from the NPZ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = np.load('dataset/Auto_Tag_Gala_Images_dataset.npz', allow_pickle=True)\n",
    "\n",
    "Xtrain, Ytrain = processed_dataset['Xtrain'], processed_dataset['Ytrain']\n",
    "Xvalidation, Yvalidation = processed_dataset['Xvalidation'], processed_dataset['Yvalidation']\n",
    "Xtest, Ytest = processed_dataset['Xtest'], processed_dataset['Ytest']\n",
    "Xpredict = processed_dataset['Xpredict']\n",
    "\n",
    "print(\"------------------------- Training Dataset -------------------------\")\n",
    "print(\"Xtrain shape: {}\".format(Xtrain.shape))\n",
    "print(\"Ytrain shape: {}\".format(Ytrain.shape))\n",
    "\n",
    "print(\"\\n------------------------- Validation Dataset -------------------------\")\n",
    "print(\"Xvalidation shape: {}\".format(Xvalidation.shape))\n",
    "print(\"Yvalidation shape: {}\".format(Yvalidation.shape))\n",
    "\n",
    "print(\"\\n------------------------- Test Dataset -------------------------\")\n",
    "print(\"Xtest shape: {}\".format(Xtest.shape))\n",
    "print(\"Ytest shape: {}\".format(Ytest.shape))\n",
    "\n",
    "print(\"\\n------------------------- Prediction Dataset -------------------------\")\n",
    "print(\"Xpredict shape: {}\".format(Xpredict.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(input_shape):\n",
    "    \n",
    "    # Input Layer\n",
    "    x_input = Input(shape=input_shape, name='INPUT')\n",
    "    x = ZeroPadding2D((2, 2))(x_input)\n",
    "\n",
    "    # CONV Layer 1\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', name='CONV-1A')(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', name='CONV-1B')(x)\n",
    "    x = BatchNormalization(axis=3, name='BN_CONV-1')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='MAXPOOL-1')(x)\n",
    "    x = Dropout(rate=0.25, name='DROPOUT_CONV-1')(x)\n",
    "\n",
    "    # CONV Layer 2\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', name='CONV-2A')(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', name='CONV-2B')(x)\n",
    "    x = BatchNormalization(axis=3, name='BN_CONV-2')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='MAXPOOL-2')(x)\n",
    "    x = Dropout(rate=0.25, name='DROPOUT_CONV-2')(x)\n",
    "    \n",
    "    # CONV Layer 3\n",
    "    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', name='CONV-3A')(x)\n",
    "    x = Conv2D(filters=1024, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', name='CONV-3B')(x)\n",
    "    x = BatchNormalization(axis=3, name='BN_CONV-3')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='MAXPOOL-3')(x)\n",
    "    x = Dropout(rate=0.25, name='DROPOUT_CONV-3')(x)\n",
    "\n",
    "    # CONV Layer 4\n",
    "    x = Conv2D(filters=2048, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', name='CONV-4A')(x)\n",
    "    x = Conv2D(filters=2048, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding='same', name='CONV-4B')(x)\n",
    "    x = BatchNormalization(axis=3, name='BN_CONV-4')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='MAXPOOL-4')(x)\n",
    "    x = Dropout(rate=0.25, name='DROPOUT_CONV-4')(x)\n",
    "\n",
    "    # Fully-connected Layer\n",
    "    x = Flatten(name='FLATTEN')(x)\n",
    "    x = Dense(units=512, activation='relu', name='FC-1')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-1')(x)\n",
    "    x = Dense(units=128, activation='relu', name='FC-2')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-2')(x)\n",
    "    x = Dense(units=32, activation='relu', name='FC-3')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-3')(x)\n",
    "\n",
    "    # Output Layer\n",
    "    x = Dense(units=4, activation='softmax', name='OUTPUT')(x)\n",
    "\n",
    "    # Create Keras Model instance\n",
    "    model = Model(inputs=x_input, outputs=x, name='GalaImagesClassifier')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model hyperparameters\n",
    "max_iterations = 50\n",
    "mini_batch_size = 64\n",
    "min_lr = 1e-5\n",
    "max_lr = 1e-2\n",
    "step_size = 8 * (Xtrain.shape[0] // mini_batch_size)\n",
    "clr_method = 'triangular2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input image shape\n",
    "input_img_shape = (Xtrain.shape[1], Xtrain.shape[2], Xtrain.shape[3])\n",
    "\n",
    "# Create the model\n",
    "model = cnn_model(input_img_shape)\n",
    "\n",
    "# Compile model to configure the learning process\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(lr=min_lr),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Triangular learning rate policy\n",
    "clr = CyclicLR(base_lr=min_lr, max_lr=max_lr, mode=clr_method, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display details of all CNN layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Finder\n",
    "lrf = LearningRateFinder(model)\n",
    "lrf.find((Xtrain, Ytrain),\n",
    "         startLR=1e-10, endLR=1e-1,\n",
    "         stepsPerEpoch=np.ceil((len(Xtrain) / float(mini_batch_size))),\n",
    "         batchSize=mini_batch_size)\n",
    "lrf.plot_loss()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit_generator(datagen.flow(Xtrain, Ytrain, batch_size=mini_batch_size),\n",
    "                              steps_per_epoch=int(np.ceil(Xtrain.shape[0] / float(mini_batch_size))),\n",
    "                              epochs=max_iterations,\n",
    "                              validation_data=(Xvalidation, Yvalidation),\n",
    "                              callbacks=[clr], workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/evaluate the model\n",
    "score = model.evaluate(x=Xtest, y=Ytest, verbose=0)\n",
    "print('Test loss: {}', format(score[0]))\n",
    "print('Test accuracy: {}', format(score[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the learning curves to show the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.plot(history.history['acc'], label='train_acc')\n",
    "plt.plot(history.history['val_acc'], label='val_acc')\n",
    "plt.ylabel('Cost/Accuracy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.title(\"Model Loss/Accuracy Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clr.history[\"lr\"])\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.title(\"Cyclical Learning Rate (CLR)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
