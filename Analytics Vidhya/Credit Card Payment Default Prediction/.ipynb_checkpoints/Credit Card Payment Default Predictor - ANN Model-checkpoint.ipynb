{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "\n",
    "Credit card is a flexible tool by which a customer can use a bank's money for a short period of time. \n",
    "\n",
    "Predicting accurately which customers are most probable to default represents a significant business opportunity for all banks. Bank cards are the most common credit card type in Taiwan, which emphasizes the impact of risk prediction on both the consumers and banks. \n",
    "\n",
    "This would inform the bank’s decisions on criteria to approve a credit card application and also decide upon what credit limit to provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. \n",
    "\n",
    "Using the information given, predict the probability of a customer defaulting in the next month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "- **ID**: Unique ID of each client\n",
    "- **LIMIT_BAL**: Amount of given credit (NT dollars):  It includes both the individual consumer credit and his/her family (supplementary) credit \n",
    "- **SEX**: Gender (1=male, 2=female)\n",
    "- **EDUCATION**: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "- **MARRIAGE**: Marital status (1=married, 2=single, 3=divorced)\n",
    "- **AGE**: Age of the client\n",
    "- **PAY_0**: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "- **PAY_2**: Repayment status in August, 2005 (scale same as above)\n",
    "- **PAY_3**: Repayment status in July, 2005 (scale same as above)\n",
    "- **PAY_4**: Repayment status in June, 2005 (scale same as above)\n",
    "- **PAY_5**: Repayment status in May, 2005 (scale same as above)\n",
    "- **PAY_6**: Repayment status in April, 2005 (scale same as above)\n",
    "- **BILL_AMT1**: Amount of bill statement in September, 2005 (NT dollar)\n",
    "- **BILL_AMT2**: Amount of bill statement in August, 2005 (NT dollar)\n",
    "- **BILL_AMT3**: Amount of bill statement in July, 2005 (NT dollar)\n",
    "- **BILL_AMT4**: Amount of bill statement in June, 2005 (NT dollar)\n",
    "- **BILL_AMT5**: Amount of bill statement in May, 2005 (NT dollar)\n",
    "- **BILL_AMT6**: Amount of bill statement in April, 2005 (NT dollar)\n",
    "- **PAY_AMT1**: Amount of previous payment in September, 2005 (NT dollar)\n",
    "- **PAY_AMT2**: Amount of previous payment in August, 2005 (NT dollar)\n",
    "- **PAY_AMT3**: Amount of previous payment in July, 2005 (NT dollar)\n",
    "- **PAY_AMT4**: Amount of previous payment in June, 2005 (NT dollar)\n",
    "- **PAY_AMT5**: Amount of previous payment in May, 2005 (NT dollar)\n",
    "- **PAY_AMT6**: Amount of previous payment in April, 2005 (NT dollar)\n",
    "- **default_payment_next_month**: Target Variable: Default payment (1=yes, 0=no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, KFold\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningratefinder import LearningRateFinder\n",
    "from clr_callback import CyclicLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set file path for train and predict datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"Dataset/train.csv\"\n",
    "predict_dataset = \"Dataset/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read train/predict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_dataset)\n",
    "predict_df = pd.read_csv(predict_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null columns in train and predict datasets\n",
    "print(\"Column with NaN value in train_df: {}\".format(train_df.columns[train_df.isnull().any()].tolist()))\n",
    "print(\"Column with NaN value in predict_df: {}\".format(predict_df.columns[predict_df.isnull().any()].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display countplot for different classes in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"default_payment_next_month\", data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate out the target variable in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = np.array([train_df['default_payment_next_month'].values]).T\n",
    "train_df.drop(['default_payment_next_month'], inplace=True, axis=1)\n",
    "print(\"Ytrain: {}\".format(Ytrain.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the train and predict datasets\n",
    "combined_df = train_df.append(predict_df, sort=False, ignore_index=True)\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for \"SEX\" field\n",
    "dummy_val = pd.get_dummies(combined_df['SEX'], prefix='SEX')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for \"EDUCATION\" field\n",
    "dummy_val = pd.get_dummies(combined_df['EDUCATION'], prefix='EDUCATION')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for \"MARRIAGE\" field\n",
    "dummy_val = pd.get_dummies(combined_df['MARRIAGE'], prefix='MARRIAGE')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for \"PAY_0\" field\n",
    "dummy_val = pd.get_dummies(combined_df['PAY_0'], prefix='PAY_0')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for \"PAY_2\" field\n",
    "dummy_val = pd.get_dummies(combined_df['PAY_2'], prefix='PAY_2')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for \"PAY_3\" field\n",
    "dummy_val = pd.get_dummies(combined_df['PAY_3'], prefix='PAY_3')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for \"PAY_4\" field\n",
    "dummy_val = pd.get_dummies(combined_df['PAY_4'], prefix='PAY_4')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for \"PAY_5\" field\n",
    "dummy_val = pd.get_dummies(combined_df['PAY_5'], prefix='PAY_5')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for \"PAY_6\" field\n",
    "dummy_val = pd.get_dummies(combined_df['PAY_6'], prefix='PAY_6')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"Pay-to-Bill ratio\" for (April, 2005)\n",
    "combined_df['pay_to_bill_april'] = combined_df['PAY_AMT5']/combined_df['BILL_AMT6']\n",
    "combined_df.loc[~np.isfinite(combined_df['pay_to_bill_april']), 'pay_to_bill_april'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"Pay-to-Bill ratio\" for (May, 2005)\n",
    "combined_df['pay_to_bill_may'] = combined_df['PAY_AMT4']/combined_df['BILL_AMT5']\n",
    "combined_df.loc[~np.isfinite(combined_df['pay_to_bill_may']), 'pay_to_bill_may'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"Pay-to-Bill ratio\" for (June, 2005)\n",
    "combined_df['pay_to_bill_june'] = combined_df['PAY_AMT3']/combined_df['BILL_AMT4']\n",
    "combined_df.loc[~np.isfinite(combined_df['pay_to_bill_june']), 'pay_to_bill_june'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"Pay-to-Bill ratio\" for (July, 2005)\n",
    "combined_df['pay_to_bill_july'] = combined_df['PAY_AMT2']/combined_df['BILL_AMT3']\n",
    "combined_df.loc[~np.isfinite(combined_df['pay_to_bill_july']), 'pay_to_bill_july'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"Pay-to-Bill ratio\" for (August, 2005)\n",
    "combined_df['pay_to_bill_aug'] = combined_df['PAY_AMT1']/combined_df['BILL_AMT2']\n",
    "combined_df.loc[~np.isfinite(combined_df['pay_to_bill_aug']), 'pay_to_bill_aug'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"% of credit limit used\" for (April, 2005)\n",
    "combined_df['pct_of_limit_used_april'] = combined_df['BILL_AMT6']/combined_df['LIMIT_BAL']\n",
    "combined_df.loc[~np.isfinite(combined_df['pct_of_limit_used_april']), 'pct_of_limit_used_april'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"% of credit limit used\" for (May, 2005)\n",
    "combined_df['pct_of_limit_used_may'] = combined_df['BILL_AMT5']/combined_df['LIMIT_BAL']\n",
    "combined_df.loc[~np.isfinite(combined_df['pct_of_limit_used_may']), 'pct_of_limit_used_may'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"% of credit limit used\" for (June, 2005)\n",
    "combined_df['pct_of_limit_used_jun'] = combined_df['BILL_AMT4']/combined_df['LIMIT_BAL']\n",
    "combined_df.loc[~np.isfinite(combined_df['pct_of_limit_used_jun']), 'pct_of_limit_used_jun'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"% of credit limit used\" for (July, 2005)\n",
    "combined_df['pct_of_limit_used_jul'] = combined_df['BILL_AMT3']/combined_df['LIMIT_BAL']\n",
    "combined_df.loc[~np.isfinite(combined_df['pct_of_limit_used_jul']), 'pct_of_limit_used_jul'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"% of credit limit used\" for (August, 2005)\n",
    "combined_df['pct_of_limit_used_aug'] = combined_df['BILL_AMT2']/combined_df['LIMIT_BAL']\n",
    "combined_df.loc[~np.isfinite(combined_df['pct_of_limit_used_aug']), 'pct_of_limit_used_aug'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"% of credit limit used\" for (September, 2005)\n",
    "combined_df['pct_of_limit_used_sep'] = combined_df['BILL_AMT1']/combined_df['LIMIT_BAL']\n",
    "combined_df.loc[~np.isfinite(combined_df['pct_of_limit_used_sep']), 'pct_of_limit_used_sep'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant fields\n",
    "combined_df.drop(['ID', 'SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'], inplace=True, axis=1)\n",
    "print(\"Number of features: {}\".format(combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segregate combined_df into train/predict datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = combined_df[:21000].values\n",
    "Xpredict = combined_df[21000:].values\n",
    "print(\"Xtrain: {}\".format(Xtrain.shape))\n",
    "print(\"Xpredict: {}\".format(Xpredict.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = StandardScaler().fit(Xtrain)\n",
    "Xtrain = scaler_x.transform(Xtrain)\n",
    "Xpredict = scaler_x.transform(Xpredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.07, random_state=1)\n",
    "for train_index, test_index in sss.split(Xtrain, Ytrain):\n",
    "    train_x, test_x = Xtrain[train_index], Xtrain[test_index]\n",
    "    train_y, test_y = Ytrain[train_index], Ytrain[test_index]\n",
    "\n",
    "print(\"------------------------- Training Dataset -------------------------\")\n",
    "print(\"train_x shape: {}\".format(train_x.shape))\n",
    "print(\"train_y shape: {}\".format(train_y.shape))\n",
    "\n",
    "print(\"\\n------------------------- Test Dataset -------------------------\")\n",
    "print(\"test_x shape: {}\".format(test_x.shape))\n",
    "print(\"test_y shape: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'Class': train_y[:, 0]})\n",
    "train_df.groupby(['Class']).size().reset_index().rename(columns={0:'Count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'Class': test_y[:, 0]})\n",
    "test_df.groupby(['Class']).size().reset_index().rename(columns={0:'Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(k_neighbors=1)\n",
    "sm_x, sm_y = sm.fit_sample(train_x, train_y.ravel())\n",
    "train_x = sm_x\n",
    "train_y = np.array([sm_y]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'Class': train_y[:, 0]})\n",
    "train_df.groupby(['Class']).size().reset_index().rename(columns={0:'Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the datasets in NPZ file (for reusability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('Dataset/Credit_Card_Payment_Default_Dataset.npz',\n",
    "                    Xtrain=train_x, Ytrain=train_y,\n",
    "                    Xtest=test_x, Ytest=test_y,\n",
    "                    Xpredict=Xpredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets from the NPZ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = np.load('Dataset/Credit_Card_Payment_Default_Dataset.npz', allow_pickle=True)\n",
    "\n",
    "Xtrain, Ytrain = processed_dataset['Xtrain'], processed_dataset['Ytrain']\n",
    "Xtest, Ytest = processed_dataset['Xtest'], processed_dataset['Ytest']\n",
    "Xpredict = processed_dataset['Xpredict']\n",
    "\n",
    "print(\"------------------------- Training Dataset -------------------------\")\n",
    "print(\"Xtrain shape: {}\".format(Xtrain.shape))\n",
    "print(\"Ytrain shape: {}\".format(Ytrain.shape))\n",
    "\n",
    "print(\"\\n------------------------- Test Dataset -------------------------\")\n",
    "print(\"Xtest shape: {}\".format(Xtest.shape))\n",
    "print(\"Ytest shape: {}\".format(Ytest.shape))\n",
    "\n",
    "print(\"\\n------------------------- Prediction Dataset -------------------------\")\n",
    "print(\"Xpredict shape: {}\".format(Xpredict.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(input_shape):\n",
    "    \n",
    "    # Input Layer\n",
    "    x_input = Input(shape=(input_shape, ), name='INPUT')\n",
    "    \n",
    "    # Fully-connected Layer 1\n",
    "    x = Dense(units=512, name='FC-1', activation='relu', kernel_regularizer=l2(0.1))(x_input)\n",
    "    x = BatchNormalization(name='BN_FC-1')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-1')(x)\n",
    "    \n",
    "    # Fully-connected Layer 2\n",
    "    x = Dense(units=512, name='FC-2', activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "    x = BatchNormalization(name='BN_FC-2')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-2')(x)\n",
    "    \n",
    "    # Fully-connected Layer 3\n",
    "    x = Dense(units=256, name='FC-3', activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "    x = BatchNormalization(name='BN_FC-3')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-3')(x)\n",
    "    \n",
    "    # Fully-connected Layer 4\n",
    "    x = Dense(units=256, name='FC-4', activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "    x = BatchNormalization(name='BN_FC-4')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-4')(x)\n",
    "    \n",
    "    # Fully-connected Layer 5\n",
    "    x = Dense(units=128, name='FC-5', activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "    x = BatchNormalization(name='BN_FC-5')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-5')(x)\n",
    "    \n",
    "    # Fully-connected Layer 6\n",
    "    x = Dense(units=128, name='FC-6', activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "    x = BatchNormalization(name='BN_FC-6')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-6')(x)\n",
    "    \n",
    "    # Fully-connected Layer 7\n",
    "    x = Dense(units=64, name='FC-7', activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "    x = BatchNormalization(name='BN_FC-7')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-7')(x)\n",
    "    \n",
    "    # Fully-connected Layer 8\n",
    "    x = Dense(units=64, name='FC-8', activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "    x = BatchNormalization(name='BN_FC-8')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-8')(x)\n",
    "    \n",
    "    # Fully-connected Layer 9\n",
    "    x = Dense(units=64, name='FC-9', activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "    x = BatchNormalization(name='BN_FC-9')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-9')(x)\n",
    "    \n",
    "    # Fully-connected Layer 10\n",
    "    x = Dense(units=64, name='FC-10', activation='relu', kernel_regularizer=l2(0.1))(x)\n",
    "    x = BatchNormalization(name='BN_FC-10')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-10')(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(units=1, activation='sigmoid', name='OUTPUT')(x)\n",
    "\n",
    "    # Create Keras Model instance\n",
    "    model = Model(inputs=x_input, outputs=x, name='Credit_Card_Payment_Default_Predictor')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model hyperparameters\n",
    "max_iterations = 10\n",
    "mini_batch_size = 128\n",
    "min_lr = 1e-4\n",
    "max_lr = 1e-2\n",
    "step_size = 8 * (Xtrain.shape[0] // mini_batch_size)\n",
    "clr_method = 'triangular2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = nn_model(Xtrain.shape[1])\n",
    "\n",
    "# Compile model to configure the learning process\n",
    "adam = Adam(lr=min_lr)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Triangular learning rate policy\n",
    "clr = CyclicLR(base_lr=min_lr, max_lr=max_lr, mode=clr_method, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Finder\n",
    "lrf = LearningRateFinder(model)\n",
    "lrf.find((Xtrain, Ytrain),\n",
    "         startLR=1e-10, endLR=1e+1,\n",
    "         stepsPerEpoch=np.ceil((len(Xtrain) / float(mini_batch_size))),\n",
    "         batchSize=mini_batch_size)\n",
    "lrf.plot_loss()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5-fold cross validation test harness\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "cvscores = []\n",
    "y_pred = 0\n",
    "Ypredict = 0\n",
    "loss_values = {}\n",
    "acc_values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using K-fold\n",
    "counter = 0\n",
    "\n",
    "for train, val in kfold.split(Xtrain, Ytrain):\n",
    "    counter += 1\n",
    "    train_x, train_y = Xtrain[train], Ytrain[train]\n",
    "    val_x, val_y = Xtrain[val], Ytrain[val]\n",
    "\n",
    "    # Create the model\n",
    "    model = nn_model(Xtrain.shape[1])\n",
    "\n",
    "    # Compile model to configure the learning process\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=min_lr), metrics=['accuracy'])\n",
    "\n",
    "    # Triangular learning rate policy\n",
    "    clr = CyclicLR(base_lr=min_lr, max_lr=max_lr, mode=clr_method, step_size=step_size)\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(x=train_x, y=train_y, \n",
    "                        batch_size=mini_batch_size, epochs=100, \n",
    "                        callbacks=[clr], workers=5,\n",
    "                        validation_data=(val_x, val_y))\n",
    "    \n",
    "    # Store the score values on validation dataset\n",
    "    scores = model.evaluate(x=Xtest, y=Ytest, verbose=0)\n",
    "    print(\"%s: %.2f\" % (model.metrics_names[1], scores[1]))\n",
    "    cvscores.append(scores[1])\n",
    "\n",
    "    # Run predictions\n",
    "    pred = model.predict(x=Xtest)\n",
    "    y_pred += pred\n",
    "\n",
    "    # Store the history object values for learning curves plotting\n",
    "    loss_values[\"train_loss_\"+str(counter)] = history.history['loss']\n",
    "    loss_values[\"val_loss_\"+str(counter)] = history.history['val_loss']\n",
    "    acc_values[\"train_acc_\"+str(counter)] = history.history['accuracy']\n",
    "    acc_values[\"val_acc_\"+str(counter)] = history.history['val_accuracy']\n",
    "\n",
    "print(\"%.2f (+/- %.2f)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "y_pred /= float(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(loss_values[\"train_loss_1\"], label='train_loss_1')\n",
    "plt.plot(loss_values[\"train_loss_2\"], label='train_loss_2')\n",
    "plt.plot(loss_values[\"train_loss_3\"], label='train_loss_3')\n",
    "plt.plot(loss_values[\"train_loss_4\"], label='train_loss_4')\n",
    "plt.plot(loss_values[\"train_loss_5\"], label='train_loss_5')\n",
    "plt.plot(loss_values[\"val_loss_1\"], label='val_loss_1')\n",
    "plt.plot(loss_values[\"val_loss_2\"], label='val_loss_2')\n",
    "plt.plot(loss_values[\"val_loss_3\"], label='val_loss_3')\n",
    "plt.plot(loss_values[\"val_loss_4\"], label='val_loss_4')\n",
    "plt.plot(loss_values[\"val_loss_5\"], label='val_loss_5')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.title(\"Model Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7c64e85344e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train_acc_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_acc_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train_acc_2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_acc_2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train_acc_3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_acc_3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train_acc_4\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_acc_4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(acc_values[\"train_acc_1\"], label='train_acc_1')\n",
    "plt.plot(acc_values[\"train_acc_2\"], label='train_acc_2')\n",
    "plt.plot(acc_values[\"train_acc_3\"], label='train_acc_3')\n",
    "plt.plot(acc_values[\"train_acc_4\"], label='train_acc_4')\n",
    "plt.plot(acc_values[\"train_acc_5\"], label='train_acc_5')\n",
    "plt.plot(acc_values[\"val_acc_1\"], label='val_acc_1')\n",
    "plt.plot(acc_values[\"val_acc_2\"], label='val_acc_2')\n",
    "plt.plot(acc_values[\"val_acc_3\"], label='val_acc_3')\n",
    "plt.plot(acc_values[\"val_acc_4\"], label='val_acc_4')\n",
    "plt.plot(acc_values[\"val_acc_5\"], label='val_acc_5')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.title(\"Model Accuracy Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clr.history[\"lr\"])\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.title(\"Cyclical Learning Rate (CLR)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print accuracy\n",
    "acc_score = accuracy_score(Ytest, y_pred_binary)\n",
    "print('Overall accuracy of Light GBM model:', acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Area Under Curve\n",
    "plt.figure()\n",
    "false_positive_rate, recall, thresholds = roc_curve(Ytest, y_pred_binary)\n",
    "roc_auc = auc(false_positive_rate, recall)\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Fall-out (1-Specificity)')\n",
    "plt.show()\n",
    "\n",
    "print('AUC score:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Confusion Matrix\n",
    "cm = confusion_matrix(Ytest, y_pred_binary)\n",
    "print(cm)\n",
    "labels = ['No Default', 'Default']\n",
    "sns.heatmap(cm, xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.5);\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(Xpredict)\n",
    "y_pred_binary = np.where(y_pred_prob > 0.5, 1, 0)\n",
    "temp_df = pd.DataFrame(y_pred_binary, columns=['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv(\"Dataset/sample_submission.csv\")\n",
    "submit_df['default_payment_next_month'] = temp_df['prediction']\n",
    "submit_df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
