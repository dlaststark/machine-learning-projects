{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acoustic-johnson",
   "metadata": {
    "papermill": {
     "duration": 0.026954,
     "end_time": "2021-06-19T08:51:43.583696",
     "exception": false,
     "start_time": "2021-06-19T08:51:43.556742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "leading-tours",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:43.641463Z",
     "iopub.status.busy": "2021-06-19T08:51:43.639943Z",
     "iopub.status.idle": "2021-06-19T08:51:49.620456Z",
     "shell.execute_reply": "2021-06-19T08:51:49.619861Z",
     "shell.execute_reply.started": "2021-06-17T11:11:15.989185Z"
    },
    "papermill": {
     "duration": 6.010752,
     "end_time": "2021-06-19T08:51:49.620603",
     "exception": false,
     "start_time": "2021-06-19T08:51:43.609851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Input, Dropout, Activation\n",
    "\n",
    "from transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig\n",
    "from transformers import XLMRobertaTokenizer, TFXLMRobertaModel, XLMRobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "closing-telescope",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:49.688269Z",
     "iopub.status.busy": "2021-06-19T08:51:49.677419Z",
     "iopub.status.idle": "2021-06-19T08:51:51.557680Z",
     "shell.execute_reply": "2021-06-19T08:51:51.557206Z",
     "shell.execute_reply.started": "2021-06-17T11:11:22.005442Z"
    },
    "papermill": {
     "duration": 1.91083,
     "end_time": "2021-06-19T08:51:51.557811",
     "exception": false,
     "start_time": "2021-06-19T08:51:49.646981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir \"./Roberta-Base\"\n",
    "! mkdir \"./XLM-Roberta-Base\"\n",
    "! mkdir \"./DistilRoberta-Base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-program",
   "metadata": {
    "papermill": {
     "duration": 0.025518,
     "end_time": "2021-06-19T08:51:51.609219",
     "exception": false,
     "start_time": "2021-06-19T08:51:51.583701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "synthetic-raise",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:51.667997Z",
     "iopub.status.busy": "2021-06-19T08:51:51.667482Z",
     "iopub.status.idle": "2021-06-19T08:51:51.802496Z",
     "shell.execute_reply": "2021-06-19T08:51:51.803076Z",
     "shell.execute_reply.started": "2021-06-17T11:11:23.908779Z"
    },
    "papermill": {
     "duration": 0.168511,
     "end_time": "2021-06-19T08:51:51.803267",
     "exception": false,
     "start_time": "2021-06-19T08:51:51.634756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (2834, 3)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>excerpt_wordlen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c12129c31</th>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85aa80a4c</th>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b69ac6792</th>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd1000b26</th>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37c1b32fb</th>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     excerpt    target  \\\n",
       "id                                                                       \n",
       "c12129c31  When the young people returned to the ballroom... -0.340259   \n",
       "85aa80a4c  All through dinner time, Mrs. Fayre was somewh... -0.315372   \n",
       "b69ac6792  As Roger had predicted, the snow departed as q... -0.580118   \n",
       "dd1000b26  And outside before the palace a great garden w... -1.054013   \n",
       "37c1b32fb  Once upon a time there were Three Bears who li...  0.247197   \n",
       "\n",
       "           excerpt_wordlen  \n",
       "id                          \n",
       "c12129c31              179  \n",
       "85aa80a4c              169  \n",
       "b69ac6792              166  \n",
       "dd1000b26              164  \n",
       "37c1b32fb              147  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n",
    "train_df[\"excerpt_wordlen\"] = train_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\n",
    "train_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\n",
    "train_df.set_index(\"id\", inplace=True)\n",
    "print(f\"train_df: {train_df.shape}\\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "married-worship",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:51.863657Z",
     "iopub.status.busy": "2021-06-19T08:51:51.863124Z",
     "iopub.status.idle": "2021-06-19T08:51:51.878460Z",
     "shell.execute_reply": "2021-06-19T08:51:51.878022Z",
     "shell.execute_reply.started": "2021-06-17T11:11:24.028777Z"
    },
    "papermill": {
     "duration": 0.047969,
     "end_time": "2021-06-19T08:51:51.878570",
     "exception": false,
     "start_time": "2021-06-19T08:51:51.830601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df: (7, 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>excerpt_wordlen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c0f722661</th>\n",
       "      <td>My hope lay in Jack's promise that he would ke...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0953f0a5</th>\n",
       "      <td>Dotty continued to go to Mrs. Gray's every nig...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0df072751</th>\n",
       "      <td>It was a bright and cheerful scene that greete...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04caf4e0c</th>\n",
       "      <td>Cell division is the process by which a parent...</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0e63f8bea</th>\n",
       "      <td>Debugging is the process of finding and resolv...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     excerpt  excerpt_wordlen\n",
       "id                                                                           \n",
       "c0f722661  My hope lay in Jack's promise that he would ke...              149\n",
       "f0953f0a5  Dotty continued to go to Mrs. Gray's every nig...              181\n",
       "0df072751  It was a bright and cheerful scene that greete...              174\n",
       "04caf4e0c  Cell division is the process by which a parent...              180\n",
       "0e63f8bea  Debugging is the process of finding and resolv...              168"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "test_df[\"excerpt_wordlen\"] = test_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\n",
    "test_df.drop(['url_legal','license'], inplace=True, axis=1)\n",
    "test_df.set_index(\"id\", inplace=True)\n",
    "print(f\"test_df: {test_df.shape}\\n\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-mining",
   "metadata": {
    "papermill": {
     "duration": 0.027089,
     "end_time": "2021-06-19T08:51:51.932671",
     "exception": false,
     "start_time": "2021-06-19T08:51:51.905582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Extract target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial-hands",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:51.991165Z",
     "iopub.status.busy": "2021-06-19T08:51:51.990674Z",
     "iopub.status.idle": "2021-06-19T08:51:51.998189Z",
     "shell.execute_reply": "2021-06-19T08:51:51.997553Z",
     "shell.execute_reply.started": "2021-06-17T11:11:24.050736Z"
    },
    "papermill": {
     "duration": 0.038878,
     "end_time": "2021-06-19T08:51:51.998370",
     "exception": false,
     "start_time": "2021-06-19T08:51:51.959492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ytrain: (2834,)\n"
     ]
    }
   ],
   "source": [
    "Ytrain = train_df['target'].values\n",
    "Ytrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\n",
    "train_df.drop(['target'], inplace=True, axis=1)\n",
    "print(f\"Ytrain: {Ytrain.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-replacement",
   "metadata": {
    "papermill": {
     "duration": 0.027279,
     "end_time": "2021-06-19T08:51:52.053348",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.026069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alive-anaheim",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:52.113220Z",
     "iopub.status.busy": "2021-06-19T08:51:52.112551Z",
     "iopub.status.idle": "2021-06-19T08:51:52.114877Z",
     "shell.execute_reply": "2021-06-19T08:51:52.115286Z",
     "shell.execute_reply.started": "2021-06-17T11:11:24.063362Z"
    },
    "papermill": {
     "duration": 0.034855,
     "end_time": "2021-06-19T08:51:52.115406",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.080551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLD = 5\n",
    "NUM_SEED = 1\n",
    "VERBOSE = 1\n",
    "MINI_BATCH_SIZE = 16\n",
    "NUM_EPOCH = 20\n",
    "MAX_LEN = max(train_df['excerpt_wordlen'].max(), \n",
    "              test_df['excerpt_wordlen'].max()) + 11\n",
    "\n",
    "ROBERTA_BASE = \"../input/huggingface-roberta-variants/roberta-base/roberta-base\"\n",
    "XLM_ROBERTA_BASE = \"../input/huggingface-roberta-variants/tf-xlm-roberta-base/tf-xlm-roberta-base\"\n",
    "DISTILROBERTA_BASE = \"../input/huggingface-roberta-variants/distilroberta-base/distilroberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-recommendation",
   "metadata": {
    "papermill": {
     "duration": 0.02716,
     "end_time": "2021-06-19T08:51:52.169542",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.142382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "missing-exhibit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:52.230314Z",
     "iopub.status.busy": "2021-06-19T08:51:52.229591Z",
     "iopub.status.idle": "2021-06-19T08:51:52.232186Z",
     "shell.execute_reply": "2021-06-19T08:51:52.231789Z",
     "shell.execute_reply.started": "2021-06-17T11:11:24.071057Z"
    },
    "papermill": {
     "duration": 0.035298,
     "end_time": "2021-06-19T08:51:52.232306",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.197008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sent_encode(texts, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for text in tqdm(texts):\n",
    "        tokens = tokenizer.encode_plus(text, max_length=MAX_LEN, truncation=True, \n",
    "                                       padding='max_length', add_special_tokens=True, \n",
    "                                       return_attention_mask=True, return_token_type_ids=True, \n",
    "                                       return_tensors='tf')\n",
    "        \n",
    "        input_ids.append(tokens['input_ids'])\n",
    "        attention_mask.append(tokens['attention_mask'])\n",
    "        token_type_ids.append(tokens['token_type_ids'])\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_mask), np.array(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lesbian-spotlight",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:52.291207Z",
     "iopub.status.busy": "2021-06-19T08:51:52.290552Z",
     "iopub.status.idle": "2021-06-19T08:51:52.292928Z",
     "shell.execute_reply": "2021-06-19T08:51:52.293372Z",
     "shell.execute_reply.started": "2021-06-17T11:11:24.084611Z"
    },
    "papermill": {
     "duration": 0.034036,
     "end_time": "2021-06-19T08:51:52.293489",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.259453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intermediate-carter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:52.357279Z",
     "iopub.status.busy": "2021-06-19T08:51:52.356488Z",
     "iopub.status.idle": "2021-06-19T08:51:52.359182Z",
     "shell.execute_reply": "2021-06-19T08:51:52.358653Z"
    },
    "papermill": {
     "duration": 0.038632,
     "end_time": "2021-06-19T08:51:52.359297",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.320665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def commonlit_model(transformer_model, use_tokens_type_ids=True):\n",
    "    \n",
    "    input_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    token_type_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "\n",
    "    if use_tokens_type_ids:\n",
    "        embed = transformer_model(input_id, token_type_ids=token_type_id, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    else:\n",
    "        embed = transformer_model(input_id, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    #x = embed[:, 0, :]\n",
    "    embed = LayerNormalization()(embed)\n",
    "    \n",
    "    x = WeightNormalization(\n",
    "            Conv1D(filters=384, kernel_size=5, \n",
    "                   strides=2, padding='same', \n",
    "                   kernel_regularizer=l2(0.0001),\n",
    "                   kernel_initializer='he_uniform'))(embed)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SpatialDropout1D(rate=0.25)(x)\n",
    "    \n",
    "    x = WeightNormalization(\n",
    "            Conv1D(filters=192, kernel_size=5, \n",
    "                   strides=2, padding='same', \n",
    "                   kernel_regularizer=l2(0.0001),\n",
    "                   kernel_initializer='he_uniform'))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SpatialDropout1D(rate=0.25)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    x = Dense(units=1, kernel_initializer='lecun_normal')(x)\n",
    "\n",
    "    model = Model(inputs=[input_id, attention_mask, token_type_id], outputs=x, \n",
    "                  name='CommonLit_Readability_Model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-grocery",
   "metadata": {
    "papermill": {
     "duration": 0.027164,
     "end_time": "2021-06-19T08:51:52.413499",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.386335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Roberta-Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-nancy",
   "metadata": {
    "papermill": {
     "duration": 0.027179,
     "end_time": "2021-06-19T08:51:52.467944",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.440765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate word tokens and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "respiratory-authorization",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:52.525727Z",
     "iopub.status.busy": "2021-06-19T08:51:52.525236Z",
     "iopub.status.idle": "2021-06-19T08:51:52.811016Z",
     "shell.execute_reply": "2021-06-19T08:51:52.809896Z",
     "shell.execute_reply.started": "2021-06-17T10:51:37.961463Z"
    },
    "papermill": {
     "duration": 0.315847,
     "end_time": "2021-06-19T08:51:52.811151",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.495304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lesser-africa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:51:52.872978Z",
     "iopub.status.busy": "2021-06-19T08:51:52.871632Z",
     "iopub.status.idle": "2021-06-19T08:52:00.363976Z",
     "shell.execute_reply": "2021-06-19T08:52:00.364558Z",
     "shell.execute_reply.started": "2021-06-17T10:51:39.031693Z"
    },
    "papermill": {
     "duration": 7.52583,
     "end_time": "2021-06-19T08:52:00.364755",
     "exception": false,
     "start_time": "2021-06-19T08:51:52.838925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [00:07<00:00, 384.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (2834, 216) \n",
      "Attention Mask: (2834, 216) \n",
      "Token-type-ids: (2834, 216)\n"
     ]
    }
   ],
   "source": [
    "Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\n",
    "Xtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n",
    "Xtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "complete-retail",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:52:00.457458Z",
     "iopub.status.busy": "2021-06-19T08:52:00.456912Z",
     "iopub.status.idle": "2021-06-19T08:52:00.479333Z",
     "shell.execute_reply": "2021-06-19T08:52:00.479913Z",
     "shell.execute_reply.started": "2021-06-17T10:51:46.151316Z"
    },
    "papermill": {
     "duration": 0.071223,
     "end_time": "2021-06-19T08:52:00.480104",
     "exception": false,
     "start_time": "2021-06-19T08:52:00.408881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 515.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (7, 216) \n",
      "Attention Mask: (7, 216) \n",
      "Token-type-ids: (7, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\n",
    "Xtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\n",
    "Xtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-group",
   "metadata": {
    "papermill": {
     "duration": 0.044537,
     "end_time": "2021-06-19T08:52:00.569636",
     "exception": false,
     "start_time": "2021-06-19T08:52:00.525099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize the Bert-Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "personalized-tomorrow",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:52:00.662056Z",
     "iopub.status.busy": "2021-06-19T08:52:00.661533Z",
     "iopub.status.idle": "2021-06-19T08:52:13.898672Z",
     "shell.execute_reply": "2021-06-19T08:52:13.899230Z",
     "shell.execute_reply.started": "2021-06-17T10:52:41.712332Z"
    },
    "papermill": {
     "duration": 13.28559,
     "end_time": "2021-06-19T08:52:13.899439",
     "exception": false,
     "start_time": "2021-06-19T08:52:00.613849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-roberta-variants/roberta-base/roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/huggingface-roberta-variants/roberta-base/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(ROBERTA_BASE)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "transformer_model = TFRobertaModel.from_pretrained(ROBERTA_BASE, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "passing-carol",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:52:13.994747Z",
     "iopub.status.busy": "2021-06-19T08:52:13.994123Z",
     "iopub.status.idle": "2021-06-19T08:52:20.227265Z",
     "shell.execute_reply": "2021-06-19T08:52:20.226506Z",
     "shell.execute_reply.started": "2021-06-17T10:52:50.191998Z"
    },
    "papermill": {
     "duration": 6.284169,
     "end_time": "2021-06-19T08:52:20.227397",
     "exception": false,
     "start_time": "2021-06-19T08:52:13.943228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CommonLit_Readability_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 124645632   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 216, 768)     1536        tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization (WeightNor (None, 108, 384)     2950273     layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 108, 384)     768         weight_normalization[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 108, 384)     0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 108, 384)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_1 (WeightN (None, 54, 192)      737857      spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 54, 192)      384         weight_normalization_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 54, 192)      0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 54, 192)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10368)        0           spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 10368)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            10369       dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 128,346,819\n",
      "Trainable params: 126,503,041\n",
      "Non-trainable params: 1,843,778\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = commonlit_model(transformer_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-scroll",
   "metadata": {
    "papermill": {
     "duration": 0.043147,
     "end_time": "2021-06-19T08:52:20.315044",
     "exception": false,
     "start_time": "2021-06-19T08:52:20.271897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fit the model with K-Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "supposed-links",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T08:52:20.416023Z",
     "iopub.status.busy": "2021-06-19T08:52:20.415378Z",
     "iopub.status.idle": "2021-06-19T10:04:46.791415Z",
     "shell.execute_reply": "2021-06-19T10:04:46.792003Z",
     "shell.execute_reply.started": "2021-06-17T10:52:56.223095Z"
    },
    "papermill": {
     "duration": 4346.433685,
     "end_time": "2021-06-19T10:04:46.792208",
     "exception": false,
     "start_time": "2021-06-19T08:52:20.358523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "142/142 [==============================] - 89s 494ms/step - loss: 1.5818 - rmse: 1.3827 - val_loss: 1.1180 - val_rmse: 0.9032\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.90316, saving model to ./Roberta-Base/CLRP_Roberta_Base_1C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 471ms/step - loss: 1.1318 - rmse: 0.9143 - val_loss: 0.9311 - val_rmse: 0.7141\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.90316 to 0.71407, saving model to ./Roberta-Base/CLRP_Roberta_Base_1C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 471ms/step - loss: 0.9923 - rmse: 0.7725 - val_loss: 0.9013 - val_rmse: 0.6812\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.71407 to 0.68124, saving model to ./Roberta-Base/CLRP_Roberta_Base_1C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.8547 - rmse: 0.6356 - val_loss: 1.0723 - val_rmse: 0.8576\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.68124\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.8044 - rmse: 0.5836 - val_loss: 0.8431 - val_rmse: 0.6241\n",
      "\n",
      "Epoch 00005: val_rmse improved from 0.68124 to 0.62412, saving model to ./Roberta-Base/CLRP_Roberta_Base_1C.h5\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 471ms/step - loss: 0.8056 - rmse: 0.5906 - val_loss: 0.8421 - val_rmse: 0.6226\n",
      "\n",
      "Epoch 00006: val_rmse improved from 0.62412 to 0.62257, saving model to ./Roberta-Base/CLRP_Roberta_Base_1C.h5\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.7382 - rmse: 0.5226 - val_loss: 0.7998 - val_rmse: 0.5819\n",
      "\n",
      "Epoch 00007: val_rmse improved from 0.62257 to 0.58189, saving model to ./Roberta-Base/CLRP_Roberta_Base_1C.h5\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.7243 - rmse: 0.5031 - val_loss: 0.9245 - val_rmse: 0.7049\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.58189\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.6806 - rmse: 0.4610 - val_loss: 0.8131 - val_rmse: 0.5956\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.58189\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.6265 - rmse: 0.4044 - val_loss: 0.8108 - val_rmse: 0.5928\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.58189\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5911 - rmse: 0.3693 - val_loss: 0.8471 - val_rmse: 0.6289\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.58189\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.5535 - rmse: 0.3325 - val_loss: 0.8319 - val_rmse: 0.6143\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.58189\n",
      "Epoch 00012: early stopping\n",
      "Seed-83 | Fold-0 | OOF Score: 0.5818927901011692\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 85s 491ms/step - loss: 1.3090 - rmse: 1.1002 - val_loss: 0.7226 - val_rmse: 0.5015\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.50147, saving model to ./Roberta-Base/CLRP_Roberta_Base_2C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.9760 - rmse: 0.7597 - val_loss: 0.5796 - val_rmse: 0.3570\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.50147 to 0.35699, saving model to ./Roberta-Base/CLRP_Roberta_Base_2C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.8569 - rmse: 0.6392 - val_loss: 0.6409 - val_rmse: 0.4162\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.35699\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.7714 - rmse: 0.5488 - val_loss: 0.7755 - val_rmse: 0.5534\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.35699\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.7070 - rmse: 0.4857 - val_loss: 0.5909 - val_rmse: 0.3677\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.35699\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6845 - rmse: 0.4630 - val_loss: 0.7010 - val_rmse: 0.4828\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.35699\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6396 - rmse: 0.4177 - val_loss: 0.5382 - val_rmse: 0.3160\n",
      "\n",
      "Epoch 00007: val_rmse improved from 0.35699 to 0.31599, saving model to ./Roberta-Base/CLRP_Roberta_Base_2C.h5\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6374 - rmse: 0.4157 - val_loss: 0.6412 - val_rmse: 0.4213\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.31599\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6121 - rmse: 0.3968 - val_loss: 0.6067 - val_rmse: 0.3872\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.31599\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5885 - rmse: 0.3656 - val_loss: 0.5961 - val_rmse: 0.3754\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.31599\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5957 - rmse: 0.3738 - val_loss: 0.6177 - val_rmse: 0.3964\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.31599\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5676 - rmse: 0.3453 - val_loss: 0.6216 - val_rmse: 0.4001\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.31599\n",
      "Epoch 00012: early stopping\n",
      "Seed-83 | Fold-1 | OOF Score: 0.3159899519247878\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 86s 494ms/step - loss: 1.2485 - rmse: 1.0553 - val_loss: 0.5037 - val_rmse: 0.2786\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.27861, saving model to ./Roberta-Base/CLRP_Roberta_Base_3C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.8792 - rmse: 0.6578 - val_loss: 0.5614 - val_rmse: 0.3369\n",
      "\n",
      "Epoch 00002: val_rmse did not improve from 0.27861\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.8354 - rmse: 0.6191 - val_loss: 0.5695 - val_rmse: 0.3511\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.27861\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.7282 - rmse: 0.5060 - val_loss: 0.5528 - val_rmse: 0.3278\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.27861\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6690 - rmse: 0.4486 - val_loss: 0.4877 - val_rmse: 0.2639\n",
      "\n",
      "Epoch 00005: val_rmse improved from 0.27861 to 0.26386, saving model to ./Roberta-Base/CLRP_Roberta_Base_3C.h5\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6562 - rmse: 0.4354 - val_loss: 0.5604 - val_rmse: 0.3366\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.26386\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6193 - rmse: 0.3980 - val_loss: 0.6426 - val_rmse: 0.4215\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.26386\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6102 - rmse: 0.3892 - val_loss: 0.5268 - val_rmse: 0.3035\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.26386\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5985 - rmse: 0.3769 - val_loss: 0.5229 - val_rmse: 0.3019\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.26386\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5846 - rmse: 0.3630 - val_loss: 0.5208 - val_rmse: 0.2992\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.26386\n",
      "Epoch 00010: early stopping\n",
      "Seed-83 | Fold-2 | OOF Score: 0.26385922701852865\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 85s 492ms/step - loss: 1.1940 - rmse: 0.9871 - val_loss: 0.6069 - val_rmse: 0.3880\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.38800, saving model to ./Roberta-Base/CLRP_Roberta_Base_4C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.8830 - rmse: 0.6649 - val_loss: 0.5674 - val_rmse: 0.3458\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.38800 to 0.34576, saving model to ./Roberta-Base/CLRP_Roberta_Base_4C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.7916 - rmse: 0.5723 - val_loss: 0.5934 - val_rmse: 0.3706\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.34576\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6952 - rmse: 0.4728 - val_loss: 0.6285 - val_rmse: 0.4037\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.34576\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6650 - rmse: 0.4437 - val_loss: 0.5430 - val_rmse: 0.3192\n",
      "\n",
      "Epoch 00005: val_rmse improved from 0.34576 to 0.31923, saving model to ./Roberta-Base/CLRP_Roberta_Base_4C.h5\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6346 - rmse: 0.4121 - val_loss: 0.6289 - val_rmse: 0.4063\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.31923\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6050 - rmse: 0.3850 - val_loss: 0.6177 - val_rmse: 0.3929\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.31923\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5813 - rmse: 0.3592 - val_loss: 0.5507 - val_rmse: 0.3259\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.31923\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5756 - rmse: 0.3532 - val_loss: 0.4728 - val_rmse: 0.2483\n",
      "\n",
      "Epoch 00009: val_rmse improved from 0.31923 to 0.24832, saving model to ./Roberta-Base/CLRP_Roberta_Base_4C.h5\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.5770 - rmse: 0.3532 - val_loss: 0.4785 - val_rmse: 0.2559\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.24832\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5693 - rmse: 0.3494 - val_loss: 0.5269 - val_rmse: 0.3047\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.24832\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5347 - rmse: 0.3143 - val_loss: 0.5468 - val_rmse: 0.3248\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.24832\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.5386 - rmse: 0.3179 - val_loss: 0.5660 - val_rmse: 0.3443\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.24832\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5436 - rmse: 0.3223 - val_loss: 0.5607 - val_rmse: 0.3391\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00014: val_rmse did not improve from 0.24832\n",
      "Epoch 00014: early stopping\n",
      "Seed-83 | Fold-3 | OOF Score: 0.24831539858739213\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 86s 494ms/step - loss: 1.1407 - rmse: 0.9302 - val_loss: 0.6081 - val_rmse: 0.3825\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.38252, saving model to ./Roberta-Base/CLRP_Roberta_Base_5C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.8555 - rmse: 0.6363 - val_loss: 0.5574 - val_rmse: 0.3304\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.38252 to 0.33045, saving model to ./Roberta-Base/CLRP_Roberta_Base_5C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.7642 - rmse: 0.5419 - val_loss: 0.6754 - val_rmse: 0.4536\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.33045\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.7311 - rmse: 0.5114 - val_loss: 0.4770 - val_rmse: 0.2553\n",
      "\n",
      "Epoch 00004: val_rmse improved from 0.33045 to 0.25532, saving model to ./Roberta-Base/CLRP_Roberta_Base_5C.h5\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6703 - rmse: 0.4495 - val_loss: 0.4781 - val_rmse: 0.2587\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.25532\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6813 - rmse: 0.4629 - val_loss: 0.5855 - val_rmse: 0.3665\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.25532\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6191 - rmse: 0.3980 - val_loss: 0.5344 - val_rmse: 0.3137\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.25532\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5904 - rmse: 0.3693 - val_loss: 0.5037 - val_rmse: 0.2835\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.25532\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5659 - rmse: 0.3470 - val_loss: 0.4332 - val_rmse: 0.2107\n",
      "\n",
      "Epoch 00009: val_rmse improved from 0.25532 to 0.21069, saving model to ./Roberta-Base/CLRP_Roberta_Base_5C.h5\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5640 - rmse: 0.3433 - val_loss: 0.5298 - val_rmse: 0.3093\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.21069\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5524 - rmse: 0.3328 - val_loss: 0.4707 - val_rmse: 0.2530\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.21069\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5371 - rmse: 0.3173 - val_loss: 0.5087 - val_rmse: 0.2908\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.21069\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5314 - rmse: 0.3109 - val_loss: 0.4958 - val_rmse: 0.2773\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.21069\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5238 - rmse: 0.3035 - val_loss: 0.4555 - val_rmse: 0.2371\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00014: val_rmse did not improve from 0.21069\n",
      "Epoch 00014: early stopping\n",
      "Seed-83 | Fold-4 | OOF Score: 0.21069192041065657\n",
      "\n",
      "Seed: 83 | Aggregate OOF Score: 0.32414985760850684\n",
      "\n",
      "\n",
      "Aggregate OOF Score: 0.32414985760850684\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "seeds = np.random.randint(0, 100, size=NUM_SEED)\n",
    "\n",
    "counter = 0\n",
    "oof_score = 0\n",
    "y_pred_final1 = 0\n",
    "\n",
    "\n",
    "for sidx, seed in enumerate(seeds):\n",
    "    seed_score = 0\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n",
    "        counter += 1\n",
    "\n",
    "        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n",
    "        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n",
    "        train_y, val_y = Ytrain[train], Ytrain[val]\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        model = commonlit_model(transformer_model)\n",
    "        \n",
    "        model.compile(loss=rmse_loss,\n",
    "                      metrics=[RootMeanSquaredError(name='rmse')],\n",
    "                      optimizer=Adam(lr=8e-5))\n",
    "\n",
    "        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n",
    "                              restore_best_weights=True, \n",
    "                              patience=5, verbose=VERBOSE)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n",
    "                                      min_lr=1e-7, patience=2, \n",
    "                                      verbose=VERBOSE, mode='min')\n",
    "\n",
    "        chk_point = ModelCheckpoint(f'./Roberta-Base/CLRP_Roberta_Base_{counter}C.h5', \n",
    "                                    monitor='val_rmse', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='min',\n",
    "                                    save_weights_only=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            [train_x_id, train_x_mask, train_x_token], train_y, \n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            epochs=NUM_EPOCH, \n",
    "            verbose=VERBOSE, \n",
    "            callbacks=[reduce_lr, early, chk_point], \n",
    "            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n",
    "        )\n",
    "        \n",
    "        model.load_weights(f'./Roberta-Base/CLRP_Roberta_Base_{counter}C.h5')\n",
    "        \n",
    "        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n",
    "        y_pred_final1 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n",
    "        \n",
    "        score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "        oof_score += score\n",
    "        seed_score += score\n",
    "        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n",
    "    \n",
    "    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n",
    "\n",
    "\n",
    "y_pred_final1 = y_pred_final1 / float(counter)\n",
    "oof_score /= float(counter)\n",
    "print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-maker",
   "metadata": {
    "papermill": {
     "duration": 2.714081,
     "end_time": "2021-06-19T10:04:51.782733",
     "exception": false,
     "start_time": "2021-06-19T10:04:49.068652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## XLM-Roberta-Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-prayer",
   "metadata": {
    "papermill": {
     "duration": 2.578701,
     "end_time": "2021-06-19T10:04:56.689472",
     "exception": false,
     "start_time": "2021-06-19T10:04:54.110771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate word tokens and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fewer-foundation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T10:05:01.330657Z",
     "iopub.status.busy": "2021-06-19T10:05:01.330042Z",
     "iopub.status.idle": "2021-06-19T10:05:01.934457Z",
     "shell.execute_reply": "2021-06-19T10:05:01.933956Z",
     "shell.execute_reply.started": "2021-06-17T11:11:46.75913Z"
    },
    "papermill": {
     "duration": 2.863587,
     "end_time": "2021-06-19T10:05:01.934589",
     "exception": false,
     "start_time": "2021-06-19T10:04:59.071002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(XLM_ROBERTA_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "charitable-earthquake",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T10:05:06.730075Z",
     "iopub.status.busy": "2021-06-19T10:05:06.729114Z",
     "iopub.status.idle": "2021-06-19T10:05:09.444011Z",
     "shell.execute_reply": "2021-06-19T10:05:09.444389Z",
     "shell.execute_reply.started": "2021-06-17T11:11:47.408949Z"
    },
    "papermill": {
     "duration": 5.201065,
     "end_time": "2021-06-19T10:05:09.444538",
     "exception": false,
     "start_time": "2021-06-19T10:05:04.243473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [00:02<00:00, 1086.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (2834, 216) \n",
      "Attention Mask: (2834, 216) \n",
      "Token-type-ids: (2834, 216)\n"
     ]
    }
   ],
   "source": [
    "Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\n",
    "Xtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n",
    "Xtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecological-billion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T10:05:14.036852Z",
     "iopub.status.busy": "2021-06-19T10:05:14.036020Z",
     "iopub.status.idle": "2021-06-19T10:05:14.050401Z",
     "shell.execute_reply": "2021-06-19T10:05:14.050810Z",
     "shell.execute_reply.started": "2021-06-17T11:11:52.183033Z"
    },
    "papermill": {
     "duration": 2.340306,
     "end_time": "2021-06-19T10:05:14.050947",
     "exception": false,
     "start_time": "2021-06-19T10:05:11.710641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 854.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (7, 216) \n",
      "Attention Mask: (7, 216) \n",
      "Token-type-ids: (7, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\n",
    "Xtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\n",
    "Xtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-barcelona",
   "metadata": {
    "papermill": {
     "duration": 2.514397,
     "end_time": "2021-06-19T10:05:18.861274",
     "exception": false,
     "start_time": "2021-06-19T10:05:16.346877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize the Albert-V2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "grand-insulation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T10:05:23.642221Z",
     "iopub.status.busy": "2021-06-19T10:05:23.641477Z",
     "iopub.status.idle": "2021-06-19T10:05:41.130554Z",
     "shell.execute_reply": "2021-06-19T10:05:41.130117Z",
     "shell.execute_reply.started": "2021-06-17T11:11:56.215193Z"
    },
    "papermill": {
     "duration": 19.988852,
     "end_time": "2021-06-19T10:05:41.130674",
     "exception": false,
     "start_time": "2021-06-19T10:05:21.141822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-roberta-variants/tf-xlm-roberta-base/tf-xlm-roberta-base were not used when initializing TFXLMRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at ../input/huggingface-roberta-variants/tf-xlm-roberta-base/tf-xlm-roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "config = XLMRobertaConfig.from_pretrained(XLM_ROBERTA_BASE)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "transformer_model = TFXLMRobertaModel.from_pretrained(XLM_ROBERTA_BASE, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "automated-smith",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T10:05:45.734020Z",
     "iopub.status.busy": "2021-06-19T10:05:45.733105Z",
     "iopub.status.idle": "2021-06-19T10:05:48.067478Z",
     "shell.execute_reply": "2021-06-19T10:05:48.066901Z",
     "shell.execute_reply.started": "2021-06-17T11:12:14.023474Z"
    },
    "papermill": {
     "duration": 4.620773,
     "end_time": "2021-06-19T10:05:48.067630",
     "exception": false,
     "start_time": "2021-06-19T10:05:43.446857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CommonLit_Readability_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tfxlm_roberta_model (TFXLMRober TFBaseModelOutputWit 278043648   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 216, 768)     1536        tfxlm_roberta_model[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_12 (Weight (None, 108, 384)     2950273     layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 108, 384)     768         weight_normalization_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 108, 384)     0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_12 (SpatialDr (None, 108, 384)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_13 (Weight (None, 54, 192)      737857      spatial_dropout1d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 54, 192)      384         weight_normalization_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 54, 192)      0           layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_13 (SpatialDr (None, 54, 192)      0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 10368)        0           spatial_dropout1d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)            (None, 10368)        0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            10369       dropout_80[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 281,744,835\n",
      "Trainable params: 279,901,057\n",
      "Non-trainable params: 1,843,778\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = commonlit_model(transformer_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-plane",
   "metadata": {
    "papermill": {
     "duration": 2.291289,
     "end_time": "2021-06-19T10:05:52.847095",
     "exception": false,
     "start_time": "2021-06-19T10:05:50.555806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fit the model with K-Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "seasonal-investigator",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T10:05:58.000666Z",
     "iopub.status.busy": "2021-06-19T10:05:57.999660Z",
     "iopub.status.idle": "2021-06-19T11:18:50.761102Z",
     "shell.execute_reply": "2021-06-19T11:18:50.760691Z",
     "shell.execute_reply.started": "2021-06-17T11:12:20.101122Z"
    },
    "papermill": {
     "duration": 4375.442673,
     "end_time": "2021-06-19T11:18:50.761258",
     "exception": false,
     "start_time": "2021-06-19T10:05:55.318585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "142/142 [==============================] - 91s 530ms/step - loss: 1.6286 - rmse: 1.4229 - val_loss: 1.2190 - val_rmse: 1.0353\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 1.03531, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_1C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 1.2940 - rmse: 1.0900 - val_loss: 0.9551 - val_rmse: 0.7399\n",
      "\n",
      "Epoch 00002: val_rmse improved from 1.03531 to 0.73991, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_1C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 72s 511ms/step - loss: 1.1390 - rmse: 0.9195 - val_loss: 0.9624 - val_rmse: 0.7483\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.73991\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 1.0190 - rmse: 0.8010 - val_loss: 0.9187 - val_rmse: 0.7047\n",
      "\n",
      "Epoch 00004: val_rmse improved from 0.73991 to 0.70466, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_1C.h5\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 72s 509ms/step - loss: 0.9561 - rmse: 0.7483 - val_loss: 1.1663 - val_rmse: 0.9529\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.70466\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 1.0655 - rmse: 0.8540 - val_loss: 1.0284 - val_rmse: 0.8216\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.70466\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.8641 - rmse: 0.6526 - val_loss: 0.9242 - val_rmse: 0.7076\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.70466\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.8577 - rmse: 0.6378 - val_loss: 0.8994 - val_rmse: 0.6836\n",
      "\n",
      "Epoch 00008: val_rmse improved from 0.70466 to 0.68365, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_1C.h5\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.8136 - rmse: 0.5964 - val_loss: 0.9053 - val_rmse: 0.6932\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.68365\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.7751 - rmse: 0.5596 - val_loss: 0.8896 - val_rmse: 0.6748\n",
      "\n",
      "Epoch 00010: val_rmse improved from 0.68365 to 0.67476, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_1C.h5\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.7584 - rmse: 0.5389 - val_loss: 0.9053 - val_rmse: 0.6894\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.67476\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.7196 - rmse: 0.5008 - val_loss: 0.8874 - val_rmse: 0.6724\n",
      "\n",
      "Epoch 00012: val_rmse improved from 0.67476 to 0.67243, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_1C.h5\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.6758 - rmse: 0.4591 - val_loss: 0.9397 - val_rmse: 0.7295\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.67243\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 72s 511ms/step - loss: 0.6941 - rmse: 0.4784 - val_loss: 0.9052 - val_rmse: 0.6945\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00014: val_rmse did not improve from 0.67243\n",
      "Epoch 15/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.6465 - rmse: 0.4278 - val_loss: 0.9296 - val_rmse: 0.7196\n",
      "\n",
      "Epoch 00015: val_rmse did not improve from 0.67243\n",
      "Epoch 16/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.6297 - rmse: 0.4117 - val_loss: 0.9091 - val_rmse: 0.6975\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00016: val_rmse did not improve from 0.67243\n",
      "Epoch 17/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.6038 - rmse: 0.3869 - val_loss: 0.9172 - val_rmse: 0.7052\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_rmse did not improve from 0.67243\n",
      "Epoch 00017: early stopping\n",
      "Seed-85 | Fold-0 | OOF Score: 0.6724309328381948\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 92s 534ms/step - loss: 1.3503 - rmse: 1.1476 - val_loss: 0.7148 - val_rmse: 0.4912\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.49124, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_2C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 1.0295 - rmse: 0.8184 - val_loss: 0.6638 - val_rmse: 0.4453\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.49124 to 0.44531, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_2C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 73s 512ms/step - loss: 0.9010 - rmse: 0.6847 - val_loss: 0.6813 - val_rmse: 0.4595\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.44531\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 73s 512ms/step - loss: 0.8544 - rmse: 0.6369 - val_loss: 0.6560 - val_rmse: 0.4349\n",
      "\n",
      "Epoch 00004: val_rmse improved from 0.44531 to 0.43488, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_2C.h5\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 73s 512ms/step - loss: 0.7873 - rmse: 0.5662 - val_loss: 0.6700 - val_rmse: 0.4494\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.43488\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.8478 - rmse: 0.6317 - val_loss: 0.7344 - val_rmse: 0.5181\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.43488\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.9369 - rmse: 0.7286 - val_loss: 0.8204 - val_rmse: 0.6058\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.43488\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 72s 511ms/step - loss: 0.7735 - rmse: 0.5560 - val_loss: 0.8569 - val_rmse: 0.6392\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.43488\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.7790 - rmse: 0.5603 - val_loss: 0.7054 - val_rmse: 0.4882\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.43488\n",
      "Epoch 00009: early stopping\n",
      "Seed-85 | Fold-1 | OOF Score: 0.434881209176005\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 92s 530ms/step - loss: 1.2543 - rmse: 1.0424 - val_loss: 0.6624 - val_rmse: 0.4402\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.44016, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_3C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.9350 - rmse: 0.7169 - val_loss: 0.6280 - val_rmse: 0.4068\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.44016 to 0.40680, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_3C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 73s 512ms/step - loss: 0.8533 - rmse: 0.6343 - val_loss: 0.6185 - val_rmse: 0.3975\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.40680 to 0.39752, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_3C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 73s 512ms/step - loss: 0.8344 - rmse: 0.6156 - val_loss: 0.6075 - val_rmse: 0.3873\n",
      "\n",
      "Epoch 00004: val_rmse improved from 0.39752 to 0.38726, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_3C.h5\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 73s 512ms/step - loss: 0.7739 - rmse: 0.5553 - val_loss: 0.6231 - val_rmse: 0.4040\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.38726\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 72s 509ms/step - loss: 0.8024 - rmse: 0.5843 - val_loss: 0.6266 - val_rmse: 0.4078\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.38726\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.7188 - rmse: 0.5019 - val_loss: 0.5908 - val_rmse: 0.3724\n",
      "\n",
      "Epoch 00007: val_rmse improved from 0.38726 to 0.37238, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_3C.h5\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.6593 - rmse: 0.4405 - val_loss: 0.5949 - val_rmse: 0.3792\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.37238\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.6541 - rmse: 0.4370 - val_loss: 0.6013 - val_rmse: 0.3832\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.37238\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 73s 512ms/step - loss: 0.6235 - rmse: 0.4035 - val_loss: 0.5950 - val_rmse: 0.3785\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.37238\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 72s 511ms/step - loss: 0.6191 - rmse: 0.3979 - val_loss: 0.5850 - val_rmse: 0.3693\n",
      "\n",
      "Epoch 00011: val_rmse improved from 0.37238 to 0.36927, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_3C.h5\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 73s 512ms/step - loss: 0.5922 - rmse: 0.3736 - val_loss: 0.6167 - val_rmse: 0.4026\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.36927\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.5977 - rmse: 0.3796 - val_loss: 0.6643 - val_rmse: 0.4511\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.36927\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.5993 - rmse: 0.3800 - val_loss: 0.5888 - val_rmse: 0.3736\n",
      "\n",
      "Epoch 00014: val_rmse did not improve from 0.36927\n",
      "Epoch 15/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.5727 - rmse: 0.3529 - val_loss: 0.5905 - val_rmse: 0.3755\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00015: val_rmse did not improve from 0.36927\n",
      "Epoch 16/20\n",
      "142/142 [==============================] - 72s 511ms/step - loss: 0.5689 - rmse: 0.3496 - val_loss: 0.5946 - val_rmse: 0.3796\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00016: val_rmse did not improve from 0.36927\n",
      "Epoch 00016: early stopping\n",
      "Seed-85 | Fold-2 | OOF Score: 0.3692729401043699\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 92s 532ms/step - loss: 1.1726 - rmse: 0.9603 - val_loss: 0.6063 - val_rmse: 0.3849\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.38488, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_4C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 73s 511ms/step - loss: 0.9060 - rmse: 0.6892 - val_loss: 0.6470 - val_rmse: 0.4264\n",
      "\n",
      "Epoch 00002: val_rmse did not improve from 0.38488\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.7879 - rmse: 0.5688 - val_loss: 0.5204 - val_rmse: 0.2977\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.38488 to 0.29770, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_4C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.7534 - rmse: 0.5319 - val_loss: 0.5402 - val_rmse: 0.3172\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.29770\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 72s 509ms/step - loss: 0.7652 - rmse: 0.5619 - val_loss: 1.2446 - val_rmse: 1.0405\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.29770\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 72s 509ms/step - loss: 1.2647 - rmse: 1.0501 - val_loss: 1.2500 - val_rmse: 1.0495\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.29770\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 72s 509ms/step - loss: 1.2922 - rmse: 1.0761 - val_loss: 1.2441 - val_rmse: 1.0400\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.29770\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 1.2593 - rmse: 1.0444 - val_loss: 1.2439 - val_rmse: 1.0400\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.29770\n",
      "Epoch 00008: early stopping\n",
      "Seed-85 | Fold-3 | OOF Score: 0.2977010268270374\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 92s 532ms/step - loss: 1.2767 - rmse: 1.0732 - val_loss: 0.6178 - val_rmse: 0.3926\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.39265, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_5C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 73s 512ms/step - loss: 0.9196 - rmse: 0.7069 - val_loss: 0.6126 - val_rmse: 0.3885\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.39265 to 0.38855, saving model to ./XLM-Roberta-Base/CLRP_XLMRoberta_Base_5C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 0.8139 - rmse: 0.6035 - val_loss: 1.1220 - val_rmse: 0.9182\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.38855\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 72s 509ms/step - loss: 1.0800 - rmse: 0.8727 - val_loss: 1.2184 - val_rmse: 1.0127\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.38855\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 1.2587 - rmse: 1.0476 - val_loss: 0.8329 - val_rmse: 0.6242\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.38855\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 72s 509ms/step - loss: 1.0233 - rmse: 0.8180 - val_loss: 0.8855 - val_rmse: 0.6685\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.38855\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 72s 510ms/step - loss: 1.0225 - rmse: 0.8053 - val_loss: 0.8048 - val_rmse: 0.5894\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.38855\n",
      "Epoch 00007: early stopping\n",
      "Seed-85 | Fold-4 | OOF Score: 0.3885452823465845\n",
      "\n",
      "Seed: 85 | Aggregate OOF Score: 0.43256627825843835\n",
      "\n",
      "\n",
      "Aggregate OOF Score: 0.43256627825843835\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29)\n",
    "seeds = np.random.randint(0, 100, size=NUM_SEED)\n",
    "\n",
    "counter = 0\n",
    "oof_score = 0\n",
    "y_pred_final2 = 0\n",
    "\n",
    "\n",
    "for sidx, seed in enumerate(seeds):\n",
    "    seed_score = 0\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n",
    "        counter += 1\n",
    "\n",
    "        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n",
    "        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n",
    "        train_y, val_y = Ytrain[train], Ytrain[val]\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        model = commonlit_model(transformer_model)\n",
    "        \n",
    "        model.compile(loss=rmse_loss,\n",
    "                      metrics=[RootMeanSquaredError(name='rmse')],\n",
    "                      optimizer=Adam(lr=8e-5))\n",
    "\n",
    "        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n",
    "                              restore_best_weights=True, \n",
    "                              patience=5, verbose=VERBOSE)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n",
    "                                      min_lr=1e-7, patience=2, \n",
    "                                      verbose=VERBOSE, mode='min')\n",
    "\n",
    "        chk_point = ModelCheckpoint(f'./XLM-Roberta-Base/CLRP_XLMRoberta_Base_{counter}C.h5', \n",
    "                                    monitor='val_rmse', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='min',\n",
    "                                    save_weights_only=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            [train_x_id, train_x_mask, train_x_token], train_y, \n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            epochs=NUM_EPOCH, \n",
    "            verbose=VERBOSE, \n",
    "            callbacks=[reduce_lr, early, chk_point], \n",
    "            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n",
    "        )\n",
    "        \n",
    "        model.load_weights(f'./XLM-Roberta-Base/CLRP_XLMRoberta_Base_{counter}C.h5')\n",
    "        \n",
    "        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n",
    "        y_pred_final2 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n",
    "        \n",
    "        score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "        oof_score += score\n",
    "        seed_score += score\n",
    "        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n",
    "    \n",
    "    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n",
    "\n",
    "\n",
    "y_pred_final2 = y_pred_final2 / float(counter)\n",
    "oof_score /= float(counter)\n",
    "print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-voice",
   "metadata": {
    "papermill": {
     "duration": 4.521515,
     "end_time": "2021-06-19T11:19:00.160859",
     "exception": false,
     "start_time": "2021-06-19T11:18:55.639344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DistilRoberta-Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-change",
   "metadata": {
    "papermill": {
     "duration": 4.402303,
     "end_time": "2021-06-19T11:19:09.059997",
     "exception": false,
     "start_time": "2021-06-19T11:19:04.657694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate word tokens and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pregnant-orientation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T11:19:18.060871Z",
     "iopub.status.busy": "2021-06-19T11:19:18.060183Z",
     "iopub.status.idle": "2021-06-19T11:19:18.221562Z",
     "shell.execute_reply": "2021-06-19T11:19:18.221091Z",
     "shell.execute_reply.started": "2021-06-17T11:26:15.014729Z"
    },
    "papermill": {
     "duration": 4.511823,
     "end_time": "2021-06-19T11:19:18.221687",
     "exception": false,
     "start_time": "2021-06-19T11:19:13.709864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(DISTILROBERTA_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "arctic-repeat",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T11:19:27.687838Z",
     "iopub.status.busy": "2021-06-19T11:19:27.687062Z",
     "iopub.status.idle": "2021-06-19T11:19:34.176881Z",
     "shell.execute_reply": "2021-06-19T11:19:34.177380Z",
     "shell.execute_reply.started": "2021-06-17T11:26:16.414597Z"
    },
    "papermill": {
     "duration": 11.326562,
     "end_time": "2021-06-19T11:19:34.177580",
     "exception": false,
     "start_time": "2021-06-19T11:19:22.851018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [00:05<00:00, 516.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (2834, 216) \n",
      "Attention Mask: (2834, 216) \n",
      "Token-type-ids: (2834, 216)\n"
     ]
    }
   ],
   "source": [
    "Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\n",
    "Xtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n",
    "Xtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "prepared-great",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T11:19:42.989475Z",
     "iopub.status.busy": "2021-06-19T11:19:42.988639Z",
     "iopub.status.idle": "2021-06-19T11:19:43.007999Z",
     "shell.execute_reply": "2021-06-19T11:19:43.008408Z",
     "shell.execute_reply.started": "2021-06-17T11:26:21.841926Z"
    },
    "papermill": {
     "duration": 4.469336,
     "end_time": "2021-06-19T11:19:43.008546",
     "exception": false,
     "start_time": "2021-06-19T11:19:38.539210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 534.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (7, 216) \n",
      "Attention Mask: (7, 216) \n",
      "Token-type-ids: (7, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\n",
    "Xtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\n",
    "Xtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-rwanda",
   "metadata": {
    "papermill": {
     "duration": 4.391891,
     "end_time": "2021-06-19T11:19:52.036155",
     "exception": false,
     "start_time": "2021-06-19T11:19:47.644264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize the DistilBert-Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "grave-value",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T11:20:01.441672Z",
     "iopub.status.busy": "2021-06-19T11:20:01.441078Z",
     "iopub.status.idle": "2021-06-19T11:20:06.666099Z",
     "shell.execute_reply": "2021-06-19T11:20:06.666506Z",
     "shell.execute_reply.started": "2021-06-17T11:26:22.233971Z"
    },
    "papermill": {
     "duration": 10.045695,
     "end_time": "2021-06-19T11:20:06.666663",
     "exception": false,
     "start_time": "2021-06-19T11:19:56.620968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-roberta-variants/distilroberta-base/distilroberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/huggingface-roberta-variants/distilroberta-base/distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(DISTILROBERTA_BASE)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "transformer_model = TFRobertaModel.from_pretrained(DISTILROBERTA_BASE, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fifteen-cabinet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T11:20:15.548894Z",
     "iopub.status.busy": "2021-06-19T11:20:15.548031Z",
     "iopub.status.idle": "2021-06-19T11:20:16.679652Z",
     "shell.execute_reply": "2021-06-19T11:20:16.679012Z",
     "shell.execute_reply.started": "2021-06-17T11:26:26.633885Z"
    },
    "papermill": {
     "duration": 5.626667,
     "end_time": "2021-06-19T11:20:16.679832",
     "exception": false,
     "start_time": "2021-06-19T11:20:11.053165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CommonLit_Readability_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model_1 (TFRobertaMo TFBaseModelOutputWit 82118400    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 216, 768)     1536        tf_roberta_model_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_24 (Weight (None, 108, 384)     2950273     layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 108, 384)     768         weight_normalization_24[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 108, 384)     0           layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_24 (SpatialDr (None, 108, 384)     0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_25 (Weight (None, 54, 192)      737857      spatial_dropout1d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 54, 192)      384         weight_normalization_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 54, 192)      0           layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_25 (SpatialDr (None, 54, 192)      0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 10368)        0           spatial_dropout1d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)           (None, 10368)        0           flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            10369       dropout_105[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 85,819,587\n",
      "Trainable params: 83,975,809\n",
      "Non-trainable params: 1,843,778\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = commonlit_model(transformer_model, use_tokens_type_ids=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-render",
   "metadata": {
    "papermill": {
     "duration": 4.397643,
     "end_time": "2021-06-19T11:20:25.483261",
     "exception": false,
     "start_time": "2021-06-19T11:20:21.085618",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fit the model with K-Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "proper-consistency",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T11:20:35.015391Z",
     "iopub.status.busy": "2021-06-19T11:20:35.014533Z",
     "iopub.status.idle": "2021-06-19T11:53:08.876966Z",
     "shell.execute_reply": "2021-06-19T11:53:08.877358Z",
     "shell.execute_reply.started": "2021-06-17T11:26:31.305261Z"
    },
    "papermill": {
     "duration": 1958.846738,
     "end_time": "2021-06-19T11:53:08.877518",
     "exception": false,
     "start_time": "2021-06-19T11:20:30.030780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "142/142 [==============================] - 46s 261ms/step - loss: 1.4722 - rmse: 1.2652 - val_loss: 1.0789 - val_rmse: 0.8582\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.85821, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_1C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 1.1102 - rmse: 0.8972 - val_loss: 0.8631 - val_rmse: 0.6398\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.85821 to 0.63976, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_1C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.9385 - rmse: 0.7171 - val_loss: 0.7870 - val_rmse: 0.5657\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.63976 to 0.56569, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_1C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.8984 - rmse: 0.6783 - val_loss: 0.9492 - val_rmse: 0.7299\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.56569\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.7961 - rmse: 0.5765 - val_loss: 0.8180 - val_rmse: 0.5978\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.56569\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.7052 - rmse: 0.4830 - val_loss: 0.7549 - val_rmse: 0.5336\n",
      "\n",
      "Epoch 00006: val_rmse improved from 0.56569 to 0.53359, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_1C.h5\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 35s 248ms/step - loss: 0.6583 - rmse: 0.4363 - val_loss: 0.8153 - val_rmse: 0.5945\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.53359\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.6396 - rmse: 0.4185 - val_loss: 0.8846 - val_rmse: 0.6650\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.53359\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.6172 - rmse: 0.3959 - val_loss: 0.8243 - val_rmse: 0.6057\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.53359\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.5796 - rmse: 0.3593 - val_loss: 0.8294 - val_rmse: 0.6119\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.53359\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.5802 - rmse: 0.3581 - val_loss: 0.8205 - val_rmse: 0.6015\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.53359\n",
      "Epoch 00011: early stopping\n",
      "Seed-82 | Fold-0 | OOF Score: 0.5335939518626852\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 46s 262ms/step - loss: 1.1989 - rmse: 0.9938 - val_loss: 0.5842 - val_rmse: 0.3617\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.36170, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_2C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.9217 - rmse: 0.7060 - val_loss: 0.5599 - val_rmse: 0.3364\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.36170 to 0.33643, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_2C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.8198 - rmse: 0.6021 - val_loss: 0.5956 - val_rmse: 0.3709\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.33643\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 36s 250ms/step - loss: 0.7566 - rmse: 0.5352 - val_loss: 0.7422 - val_rmse: 0.5185\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.33643\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.6883 - rmse: 0.4670 - val_loss: 0.6056 - val_rmse: 0.3832\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.33643\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.6583 - rmse: 0.4369 - val_loss: 0.6209 - val_rmse: 0.4001\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.33643\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.6061 - rmse: 0.3861 - val_loss: 0.6734 - val_rmse: 0.4517\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.33643\n",
      "Epoch 00007: early stopping\n",
      "Seed-82 | Fold-1 | OOF Score: 0.3364318069384966\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 46s 260ms/step - loss: 1.1803 - rmse: 0.9670 - val_loss: 0.6799 - val_rmse: 0.4542\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.45418, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_3C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.9043 - rmse: 0.6848 - val_loss: 0.6429 - val_rmse: 0.4176\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.45418 to 0.41758, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_3C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.8074 - rmse: 0.5917 - val_loss: 0.6339 - val_rmse: 0.4102\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.41758 to 0.41022, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_3C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.7310 - rmse: 0.5104 - val_loss: 0.6367 - val_rmse: 0.4178\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.41022\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.6870 - rmse: 0.4682 - val_loss: 0.8412 - val_rmse: 0.6156\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.41022\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.6583 - rmse: 0.4382 - val_loss: 0.6568 - val_rmse: 0.4358\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.41022\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.6021 - rmse: 0.3788 - val_loss: 0.6724 - val_rmse: 0.4515\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.41022\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.5787 - rmse: 0.3558 - val_loss: 0.6071 - val_rmse: 0.3875\n",
      "\n",
      "Epoch 00008: val_rmse improved from 0.41022 to 0.38747, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_3C.h5\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.5687 - rmse: 0.3485 - val_loss: 0.6195 - val_rmse: 0.4021\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.38747\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.5497 - rmse: 0.3274 - val_loss: 0.5971 - val_rmse: 0.3764\n",
      "\n",
      "Epoch 00010: val_rmse improved from 0.38747 to 0.37638, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_3C.h5\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.5399 - rmse: 0.3165 - val_loss: 0.6069 - val_rmse: 0.3887\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.37638\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.5390 - rmse: 0.3162 - val_loss: 0.6153 - val_rmse: 0.3984\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.37638\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.5480 - rmse: 0.3247 - val_loss: 0.6814 - val_rmse: 0.4629\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.37638\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.5176 - rmse: 0.2945 - val_loss: 0.5494 - val_rmse: 0.3279\n",
      "\n",
      "Epoch 00014: val_rmse improved from 0.37638 to 0.32787, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_3C.h5\n",
      "Epoch 15/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.5319 - rmse: 0.3099 - val_loss: 0.6698 - val_rmse: 0.4527\n",
      "\n",
      "Epoch 00015: val_rmse did not improve from 0.32787\n",
      "Epoch 16/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.5187 - rmse: 0.2980 - val_loss: 0.6562 - val_rmse: 0.4427\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00016: val_rmse did not improve from 0.32787\n",
      "Epoch 17/20\n",
      "142/142 [==============================] - 36s 250ms/step - loss: 0.5072 - rmse: 0.2850 - val_loss: 0.6537 - val_rmse: 0.4357\n",
      "\n",
      "Epoch 00017: val_rmse did not improve from 0.32787\n",
      "Epoch 18/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.5072 - rmse: 0.2866 - val_loss: 0.6588 - val_rmse: 0.4420\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00018: val_rmse did not improve from 0.32787\n",
      "Epoch 19/20\n",
      "142/142 [==============================] - 36s 250ms/step - loss: 0.5101 - rmse: 0.2890 - val_loss: 0.6279 - val_rmse: 0.4100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_rmse did not improve from 0.32787\n",
      "Epoch 00019: early stopping\n",
      "Seed-82 | Fold-2 | OOF Score: 0.32787244656054687\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 45s 263ms/step - loss: 1.1501 - rmse: 0.9387 - val_loss: 0.5252 - val_rmse: 0.3009\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.30093, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_4C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.8065 - rmse: 0.5871 - val_loss: 0.4713 - val_rmse: 0.2511\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.30093 to 0.25105, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_4C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.7547 - rmse: 0.5316 - val_loss: 0.5633 - val_rmse: 0.3393\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.25105\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.6698 - rmse: 0.4489 - val_loss: 0.5573 - val_rmse: 0.3322\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.25105\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.6237 - rmse: 0.3997 - val_loss: 0.5419 - val_rmse: 0.3195\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.25105\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.5746 - rmse: 0.3507 - val_loss: 0.6480 - val_rmse: 0.4294\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.25105\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.5696 - rmse: 0.3470 - val_loss: 0.4973 - val_rmse: 0.2729\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.25105\n",
      "Epoch 00007: early stopping\n",
      "Seed-82 | Fold-3 | OOF Score: 0.25105166712808297\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 49s 260ms/step - loss: 1.1787 - rmse: 0.9800 - val_loss: 0.6158 - val_rmse: 0.3907\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.39074, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_5C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 36s 251ms/step - loss: 0.8267 - rmse: 0.6059 - val_loss: 0.4965 - val_rmse: 0.2713\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.39074 to 0.27125, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_5C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 35s 248ms/step - loss: 0.7347 - rmse: 0.5129 - val_loss: 0.4586 - val_rmse: 0.2323\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.27125 to 0.23227, saving model to ./DistilRoberta-Base/CLRP_DistilRoberta_Base_5C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 35s 250ms/step - loss: 0.6962 - rmse: 0.4741 - val_loss: 0.5756 - val_rmse: 0.3540\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.23227\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 35s 248ms/step - loss: 0.6584 - rmse: 0.4366 - val_loss: 0.4927 - val_rmse: 0.2684\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.23227\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 36s 250ms/step - loss: 0.5949 - rmse: 0.3736 - val_loss: 0.5553 - val_rmse: 0.3330\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.23227\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 35s 249ms/step - loss: 0.5668 - rmse: 0.3439 - val_loss: 0.5192 - val_rmse: 0.2961\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.23227\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 36s 250ms/step - loss: 0.5505 - rmse: 0.3286 - val_loss: 0.4772 - val_rmse: 0.2553\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.23227\n",
      "Epoch 00008: early stopping\n",
      "Seed-82 | Fold-4 | OOF Score: 0.23226592424908787\n",
      "\n",
      "Seed: 82 | Aggregate OOF Score: 0.3362431593477799\n",
      "\n",
      "\n",
      "Aggregate OOF Score: 0.3362431593477799\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(31)\n",
    "seeds = np.random.randint(0, 100, size=NUM_SEED)\n",
    "\n",
    "counter = 0\n",
    "oof_score = 0\n",
    "y_pred_final3 = 0\n",
    "\n",
    "\n",
    "for sidx, seed in enumerate(seeds):\n",
    "    seed_score = 0\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n",
    "        counter += 1\n",
    "\n",
    "        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n",
    "        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n",
    "        train_y, val_y = Ytrain[train], Ytrain[val]\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        model = commonlit_model(transformer_model, use_tokens_type_ids=False)\n",
    "        \n",
    "        model.compile(loss=rmse_loss,\n",
    "                      metrics=[RootMeanSquaredError(name='rmse')],\n",
    "                      optimizer=Adam(lr=8e-5))\n",
    "\n",
    "        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n",
    "                              restore_best_weights=True, \n",
    "                              patience=5, verbose=VERBOSE)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n",
    "                                      min_lr=1e-7, patience=2, \n",
    "                                      verbose=VERBOSE, mode='min')\n",
    "\n",
    "        chk_point = ModelCheckpoint(f'./DistilRoberta-Base/CLRP_DistilRoberta_Base_{counter}C.h5', \n",
    "                                    monitor='val_rmse', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='min',\n",
    "                                    save_weights_only=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            [train_x_id, train_x_mask, train_x_token], train_y, \n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            epochs=NUM_EPOCH, \n",
    "            verbose=VERBOSE, \n",
    "            callbacks=[reduce_lr, early, chk_point], \n",
    "            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n",
    "        )\n",
    "        \n",
    "        model.load_weights(f'./DistilRoberta-Base/CLRP_DistilRoberta_Base_{counter}C.h5')\n",
    "        \n",
    "        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n",
    "        y_pred_final3 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n",
    "        \n",
    "        score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "        oof_score += score\n",
    "        seed_score += score\n",
    "        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n",
    "    \n",
    "    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n",
    "\n",
    "\n",
    "y_pred_final3 = y_pred_final3 / float(counter)\n",
    "oof_score /= float(counter)\n",
    "print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-tunisia",
   "metadata": {
    "papermill": {
     "duration": 6.55085,
     "end_time": "2021-06-19T11:53:21.752695",
     "exception": false,
     "start_time": "2021-06-19T11:53:15.201845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fluid-twist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T11:53:35.164335Z",
     "iopub.status.busy": "2021-06-19T11:53:35.163762Z",
     "iopub.status.idle": "2021-06-19T11:53:35.481944Z",
     "shell.execute_reply": "2021-06-19T11:53:35.482358Z"
    },
    "papermill": {
     "duration": 7.161725,
     "end_time": "2021-06-19T11:53:35.482504",
     "exception": false,
     "start_time": "2021-06-19T11:53:28.320779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.597287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.472897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.094373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.161176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.968243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.597287\n",
       "1  f0953f0a5 -0.472897\n",
       "2  0df072751 -0.094373\n",
       "3  04caf4e0c -2.161176\n",
       "4  0e63f8bea -1.968243"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final = (y_pred_final1 + y_pred_final2 + y_pred_final3) / 3.0\n",
    "\n",
    "submit_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n",
    "submit_df['target'] = y_pred_final\n",
    "submit_df.to_csv(\"./submission.csv\", index=False)\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-manchester",
   "metadata": {
    "papermill": {
     "duration": 6.409444,
     "end_time": "2021-06-19T11:53:48.405553",
     "exception": false,
     "start_time": "2021-06-19T11:53:41.996109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10941.325901,
   "end_time": "2021-06-19T11:53:58.275241",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-19T08:51:36.949340",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
