{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "revolutionary-washington",
   "metadata": {
    "papermill": {
     "duration": 0.028762,
     "end_time": "2021-06-23T16:49:03.770434",
     "exception": false,
     "start_time": "2021-06-23T16:49:03.741672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romantic-harassment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:03.837549Z",
     "iopub.status.busy": "2021-06-23T16:49:03.836856Z",
     "iopub.status.idle": "2021-06-23T16:49:09.674414Z",
     "shell.execute_reply": "2021-06-23T16:49:09.673408Z",
     "shell.execute_reply.started": "2021-06-19T05:38:21.076360Z"
    },
    "papermill": {
     "duration": 5.876703,
     "end_time": "2021-06-23T16:49:09.674587",
     "exception": false,
     "start_time": "2021-06-23T16:49:03.797884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Input, Dropout, Activation\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from transformers import AlbertTokenizer, TFAlbertModel, AlbertConfig\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supported-optimization",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:09.733171Z",
     "iopub.status.busy": "2021-06-23T16:49:09.732618Z",
     "iopub.status.idle": "2021-06-23T16:49:11.624709Z",
     "shell.execute_reply": "2021-06-23T16:49:11.624264Z",
     "shell.execute_reply.started": "2021-06-19T05:38:37.397861Z"
    },
    "papermill": {
     "duration": 1.923274,
     "end_time": "2021-06-23T16:49:11.624841",
     "exception": false,
     "start_time": "2021-06-23T16:49:09.701567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir \"./Bert-Base-Uncased\"\n",
    "! mkdir \"./Albert-Base-V2\"\n",
    "! mkdir \"./DistilBert-Base-Uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-blank",
   "metadata": {
    "papermill": {
     "duration": 0.026675,
     "end_time": "2021-06-23T16:49:11.678420",
     "exception": false,
     "start_time": "2021-06-23T16:49:11.651745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "controlled-kuwait",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:11.739253Z",
     "iopub.status.busy": "2021-06-23T16:49:11.738678Z",
     "iopub.status.idle": "2021-06-23T16:49:11.912869Z",
     "shell.execute_reply": "2021-06-23T16:49:11.913635Z",
     "shell.execute_reply.started": "2021-06-19T05:38:39.312338Z"
    },
    "papermill": {
     "duration": 0.208784,
     "end_time": "2021-06-23T16:49:11.913828",
     "exception": false,
     "start_time": "2021-06-23T16:49:11.705044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (2834, 3)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>excerpt_wordlen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c12129c31</th>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85aa80a4c</th>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b69ac6792</th>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dd1000b26</th>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37c1b32fb</th>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     excerpt    target  \\\n",
       "id                                                                       \n",
       "c12129c31  When the young people returned to the ballroom... -0.340259   \n",
       "85aa80a4c  All through dinner time, Mrs. Fayre was somewh... -0.315372   \n",
       "b69ac6792  As Roger had predicted, the snow departed as q... -0.580118   \n",
       "dd1000b26  And outside before the palace a great garden w... -1.054013   \n",
       "37c1b32fb  Once upon a time there were Three Bears who li...  0.247197   \n",
       "\n",
       "           excerpt_wordlen  \n",
       "id                          \n",
       "c12129c31              179  \n",
       "85aa80a4c              169  \n",
       "b69ac6792              166  \n",
       "dd1000b26              164  \n",
       "37c1b32fb              147  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n",
    "train_df[\"excerpt_wordlen\"] = train_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\n",
    "train_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\n",
    "train_df.set_index(\"id\", inplace=True)\n",
    "print(f\"train_df: {train_df.shape}\\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "animal-number",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:11.977146Z",
     "iopub.status.busy": "2021-06-23T16:49:11.976474Z",
     "iopub.status.idle": "2021-06-23T16:49:11.990779Z",
     "shell.execute_reply": "2021-06-23T16:49:11.991364Z",
     "shell.execute_reply.started": "2021-06-19T05:38:39.443094Z"
    },
    "papermill": {
     "duration": 0.048893,
     "end_time": "2021-06-23T16:49:11.991541",
     "exception": false,
     "start_time": "2021-06-23T16:49:11.942648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df: (7, 2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>excerpt_wordlen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c0f722661</th>\n",
       "      <td>My hope lay in Jack's promise that he would ke...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0953f0a5</th>\n",
       "      <td>Dotty continued to go to Mrs. Gray's every nig...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0df072751</th>\n",
       "      <td>It was a bright and cheerful scene that greete...</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04caf4e0c</th>\n",
       "      <td>Cell division is the process by which a parent...</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0e63f8bea</th>\n",
       "      <td>Debugging is the process of finding and resolv...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     excerpt  excerpt_wordlen\n",
       "id                                                                           \n",
       "c0f722661  My hope lay in Jack's promise that he would ke...              149\n",
       "f0953f0a5  Dotty continued to go to Mrs. Gray's every nig...              181\n",
       "0df072751  It was a bright and cheerful scene that greete...              174\n",
       "04caf4e0c  Cell division is the process by which a parent...              180\n",
       "0e63f8bea  Debugging is the process of finding and resolv...              168"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "test_df[\"excerpt_wordlen\"] = test_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\n",
    "test_df.drop(['url_legal','license'], inplace=True, axis=1)\n",
    "test_df.set_index(\"id\", inplace=True)\n",
    "print(f\"test_df: {test_df.shape}\\n\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-alaska",
   "metadata": {
    "papermill": {
     "duration": 0.027917,
     "end_time": "2021-06-23T16:49:12.047977",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.020060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Extract target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "consolidated-demographic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:12.109888Z",
     "iopub.status.busy": "2021-06-23T16:49:12.109361Z",
     "iopub.status.idle": "2021-06-23T16:49:12.116051Z",
     "shell.execute_reply": "2021-06-23T16:49:12.116649Z",
     "shell.execute_reply.started": "2021-06-19T05:38:39.466670Z"
    },
    "papermill": {
     "duration": 0.040021,
     "end_time": "2021-06-23T16:49:12.116828",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.076807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ytrain: (2834,)\n"
     ]
    }
   ],
   "source": [
    "Ytrain = train_df['target'].values\n",
    "Ytrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\n",
    "train_df.drop(['target'], inplace=True, axis=1)\n",
    "print(f\"Ytrain: {Ytrain.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-institution",
   "metadata": {
    "papermill": {
     "duration": 0.03,
     "end_time": "2021-06-23T16:49:12.175579",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.145579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "crazy-update",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:12.244775Z",
     "iopub.status.busy": "2021-06-23T16:49:12.243946Z",
     "iopub.status.idle": "2021-06-23T16:49:12.246757Z",
     "shell.execute_reply": "2021-06-23T16:49:12.246329Z",
     "shell.execute_reply.started": "2021-06-19T05:38:39.566965Z"
    },
    "papermill": {
     "duration": 0.039789,
     "end_time": "2021-06-23T16:49:12.246882",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.207093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLD = 5\n",
    "NUM_SEED = 1\n",
    "VERBOSE = 1\n",
    "MINI_BATCH_SIZE = 16\n",
    "NUM_EPOCH = 20\n",
    "MAX_LEN = max(train_df['excerpt_wordlen'].max(), \n",
    "              test_df['excerpt_wordlen'].max()) + 11\n",
    "\n",
    "BERT_BASE_UNCASED = \"../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased\"\n",
    "ALBERT_BASE_V2 = \"../input/albert-base-v2-tf2\"\n",
    "DISTILBERT_BASE_UNCASED = \"../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-positive",
   "metadata": {
    "papermill": {
     "duration": 0.028013,
     "end_time": "2021-06-23T16:49:12.303568",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.275555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coastal-switzerland",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:12.366500Z",
     "iopub.status.busy": "2021-06-23T16:49:12.365736Z",
     "iopub.status.idle": "2021-06-23T16:49:12.368481Z",
     "shell.execute_reply": "2021-06-23T16:49:12.368074Z",
     "shell.execute_reply.started": "2021-06-19T05:38:39.988287Z"
    },
    "papermill": {
     "duration": 0.03665,
     "end_time": "2021-06-23T16:49:12.368594",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.331944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sent_encode(texts, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for text in tqdm(texts):\n",
    "        tokens = tokenizer.encode_plus(text, max_length=MAX_LEN, truncation=True, \n",
    "                                       padding='max_length', add_special_tokens=True, \n",
    "                                       return_attention_mask=True, return_token_type_ids=True, \n",
    "                                       return_tensors='tf')\n",
    "        \n",
    "        input_ids.append(tokens['input_ids'])\n",
    "        attention_mask.append(tokens['attention_mask'])\n",
    "        token_type_ids.append(tokens['token_type_ids'])\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_mask), np.array(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sought-grade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:12.429616Z",
     "iopub.status.busy": "2021-06-23T16:49:12.428897Z",
     "iopub.status.idle": "2021-06-23T16:49:12.431573Z",
     "shell.execute_reply": "2021-06-23T16:49:12.431181Z",
     "shell.execute_reply.started": "2021-06-19T05:38:40.207347Z"
    },
    "papermill": {
     "duration": 0.034858,
     "end_time": "2021-06-23T16:49:12.431681",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.396823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "changing-canal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:12.498810Z",
     "iopub.status.busy": "2021-06-23T16:49:12.498047Z",
     "iopub.status.idle": "2021-06-23T16:49:12.500356Z",
     "shell.execute_reply": "2021-06-23T16:49:12.500811Z",
     "shell.execute_reply.started": "2021-06-19T05:38:40.451121Z"
    },
    "papermill": {
     "duration": 0.040871,
     "end_time": "2021-06-23T16:49:12.500935",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.460064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def commonlit_model(transformer_model, use_tokens_type_ids=True):\n",
    "    \n",
    "    input_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    token_type_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "\n",
    "    if use_tokens_type_ids:\n",
    "        embed = transformer_model(input_id, token_type_ids=token_type_id, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    else:\n",
    "        embed = transformer_model(input_id, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    #x = embed[:, 0, :]\n",
    "    embed = LayerNormalization()(embed)\n",
    "    \n",
    "    x = WeightNormalization(\n",
    "            Conv1D(filters=384, kernel_size=5, \n",
    "                   strides=2, padding='same', \n",
    "                   kernel_regularizer=l2(0.0001),\n",
    "                   kernel_initializer='he_uniform'))(embed)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SpatialDropout1D(rate=0.25)(x)\n",
    "    \n",
    "    x = WeightNormalization(\n",
    "            Conv1D(filters=192, kernel_size=5, \n",
    "                   strides=2, padding='same', \n",
    "                   kernel_regularizer=l2(0.0001),\n",
    "                   kernel_initializer='he_uniform'))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SpatialDropout1D(rate=0.25)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    x = Dense(units=1, kernel_initializer='lecun_normal')(x)\n",
    "\n",
    "    model = Model(inputs=[input_id, attention_mask, token_type_id], outputs=x, \n",
    "                  name='CommonLit_Readability_Model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-couple",
   "metadata": {
    "papermill": {
     "duration": 0.027908,
     "end_time": "2021-06-23T16:49:12.557382",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.529474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bert-Base-Uncased Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-peter",
   "metadata": {
    "papermill": {
     "duration": 0.028145,
     "end_time": "2021-06-23T16:49:12.613835",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.585690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate word tokens and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "african-calvin",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:12.674232Z",
     "iopub.status.busy": "2021-06-23T16:49:12.673650Z",
     "iopub.status.idle": "2021-06-23T16:49:12.921157Z",
     "shell.execute_reply": "2021-06-23T16:49:12.920630Z",
     "shell.execute_reply.started": "2021-06-19T05:38:41.266793Z"
    },
    "papermill": {
     "duration": 0.279084,
     "end_time": "2021-06-23T16:49:12.921299",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.642215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_BASE_UNCASED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "copyrighted-correlation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:12.986014Z",
     "iopub.status.busy": "2021-06-23T16:49:12.985137Z",
     "iopub.status.idle": "2021-06-23T16:49:30.284096Z",
     "shell.execute_reply": "2021-06-23T16:49:30.283665Z",
     "shell.execute_reply.started": "2021-06-19T05:38:41.516885Z"
    },
    "papermill": {
     "duration": 17.333859,
     "end_time": "2021-06-23T16:49:30.284220",
     "exception": false,
     "start_time": "2021-06-23T16:49:12.950361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [00:17<00:00, 164.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (2834, 216) \n",
      "Attention Mask: (2834, 216) \n",
      "Token-type-ids: (2834, 216)\n"
     ]
    }
   ],
   "source": [
    "Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\n",
    "Xtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n",
    "Xtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "subtle-shadow",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:30.422390Z",
     "iopub.status.busy": "2021-06-23T16:49:30.421598Z",
     "iopub.status.idle": "2021-06-23T16:49:30.464844Z",
     "shell.execute_reply": "2021-06-23T16:49:30.464412Z",
     "shell.execute_reply.started": "2021-06-19T05:38:58.125246Z"
    },
    "papermill": {
     "duration": 0.114833,
     "end_time": "2021-06-23T16:49:30.464972",
     "exception": false,
     "start_time": "2021-06-23T16:49:30.350139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 192.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (7, 216) \n",
      "Attention Mask: (7, 216) \n",
      "Token-type-ids: (7, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\n",
    "Xtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\n",
    "Xtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-consortium",
   "metadata": {
    "papermill": {
     "duration": 0.065923,
     "end_time": "2021-06-23T16:49:30.598269",
     "exception": false,
     "start_time": "2021-06-23T16:49:30.532346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize the Bert-Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "marine-poster",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:30.735360Z",
     "iopub.status.busy": "2021-06-23T16:49:30.734814Z",
     "iopub.status.idle": "2021-06-23T16:49:44.389067Z",
     "shell.execute_reply": "2021-06-23T16:49:44.389453Z",
     "shell.execute_reply.started": "2021-06-19T05:38:58.176018Z"
    },
    "papermill": {
     "duration": 13.725155,
     "end_time": "2021-06-23T16:49:44.389602",
     "exception": false,
     "start_time": "2021-06-23T16:49:30.664447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at ../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "config = BertConfig.from_pretrained(\n",
    "    BERT_BASE_UNCASED,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "'''\n",
    "config = BertConfig()\n",
    "config.output_hidden_states = False\n",
    "\n",
    "transformer_model = TFBertModel.from_pretrained(BERT_BASE_UNCASED, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "external-treasure",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:44.533812Z",
     "iopub.status.busy": "2021-06-23T16:49:44.533117Z",
     "iopub.status.idle": "2021-06-23T16:49:50.960414Z",
     "shell.execute_reply": "2021-06-23T16:49:50.959589Z",
     "shell.execute_reply.started": "2021-06-19T05:39:11.007166Z"
    },
    "papermill": {
     "duration": 6.503969,
     "end_time": "2021-06-23T16:49:50.960545",
     "exception": false,
     "start_time": "2021-06-23T16:49:44.456576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CommonLit_Readability_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 216, 768)     1536        tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization (WeightNor (None, 108, 384)     2950273     layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 108, 384)     768         weight_normalization[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 108, 384)     0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 108, 384)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_1 (WeightN (None, 54, 192)      737857      spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 54, 192)      384         weight_normalization_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 54, 192)      0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 54, 192)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10368)        0           spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 10368)        0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            10369       dropout_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 113,183,427\n",
      "Trainable params: 111,339,649\n",
      "Non-trainable params: 1,843,778\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = commonlit_model(transformer_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-actor",
   "metadata": {
    "papermill": {
     "duration": 0.06776,
     "end_time": "2021-06-23T16:49:51.100427",
     "exception": false,
     "start_time": "2021-06-23T16:49:51.032667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fit the model with K-Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chronic-ottawa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T16:49:51.252589Z",
     "iopub.status.busy": "2021-06-23T16:49:51.251660Z",
     "iopub.status.idle": "2021-06-23T17:43:02.432651Z",
     "shell.execute_reply": "2021-06-23T17:43:02.432231Z",
     "shell.execute_reply.started": "2021-06-19T05:39:24.533111Z"
    },
    "papermill": {
     "duration": 3191.26382,
     "end_time": "2021-06-23T17:43:02.432792",
     "exception": false,
     "start_time": "2021-06-23T16:49:51.168972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "142/142 [==============================] - 89s 495ms/step - loss: 1.6780 - rmse: 1.4825 - val_loss: 1.2218 - val_rmse: 1.0077\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 1.00766, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_1C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 1.4131 - rmse: 1.1959 - val_loss: 0.9196 - val_rmse: 0.7013\n",
      "\n",
      "Epoch 00002: val_rmse improved from 1.00766 to 0.70125, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_1C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 471ms/step - loss: 1.0902 - rmse: 0.8721 - val_loss: 0.8342 - val_rmse: 0.6149\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.70125 to 0.61493, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_1C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.9281 - rmse: 0.7072 - val_loss: 1.0771 - val_rmse: 0.8591\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.61493\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.8671 - rmse: 0.6473 - val_loss: 0.8333 - val_rmse: 0.6166\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.61493\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.7449 - rmse: 0.5253 - val_loss: 0.8036 - val_rmse: 0.5849\n",
      "\n",
      "Epoch 00006: val_rmse improved from 0.61493 to 0.58485, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_1C.h5\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.6678 - rmse: 0.4448 - val_loss: 0.8343 - val_rmse: 0.6173\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.58485\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.6567 - rmse: 0.4338 - val_loss: 0.7972 - val_rmse: 0.5780\n",
      "\n",
      "Epoch 00008: val_rmse improved from 0.58485 to 0.57805, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_1C.h5\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.6325 - rmse: 0.4085 - val_loss: 0.8107 - val_rmse: 0.5911\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.57805\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.6306 - rmse: 0.4062 - val_loss: 0.8345 - val_rmse: 0.6171\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.57805\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.5842 - rmse: 0.3601 - val_loss: 0.8446 - val_rmse: 0.6273\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.57805\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.5742 - rmse: 0.3513 - val_loss: 0.8881 - val_rmse: 0.6699\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.57805\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5553 - rmse: 0.3320 - val_loss: 0.8556 - val_rmse: 0.6376\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.57805\n",
      "Epoch 00013: early stopping\n",
      "Seed-83 | Fold-0 | OOF Score: 0.5780486277112108\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 85s 491ms/step - loss: 1.2773 - rmse: 1.0780 - val_loss: 0.5525 - val_rmse: 0.3273\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.32734, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_2C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.9614 - rmse: 0.7431 - val_loss: 0.5203 - val_rmse: 0.2960\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.32734 to 0.29600, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_2C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.8167 - rmse: 0.5981 - val_loss: 0.5803 - val_rmse: 0.3561\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.29600\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.7680 - rmse: 0.5491 - val_loss: 0.8080 - val_rmse: 0.5907\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.29600\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6790 - rmse: 0.4567 - val_loss: 0.6029 - val_rmse: 0.3788\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.29600\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.6461 - rmse: 0.4226 - val_loss: 0.6828 - val_rmse: 0.4591\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.29600\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.6122 - rmse: 0.3903 - val_loss: 0.6035 - val_rmse: 0.3789\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.29600\n",
      "Epoch 00007: early stopping\n",
      "Seed-83 | Fold-1 | OOF Score: 0.29599539757095084\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 86s 494ms/step - loss: 1.2241 - rmse: 1.0154 - val_loss: 0.5672 - val_rmse: 0.3428\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.34277, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_3C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.8934 - rmse: 0.6772 - val_loss: 0.5730 - val_rmse: 0.3490\n",
      "\n",
      "Epoch 00002: val_rmse did not improve from 0.34277\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.8040 - rmse: 0.5830 - val_loss: 0.5832 - val_rmse: 0.3602\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.34277\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.7138 - rmse: 0.4897 - val_loss: 0.6149 - val_rmse: 0.3929\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.34277\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.6579 - rmse: 0.4365 - val_loss: 0.6289 - val_rmse: 0.4087\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.34277\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6303 - rmse: 0.4061 - val_loss: 0.6276 - val_rmse: 0.4061\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.34277\n",
      "Epoch 00006: early stopping\n",
      "Seed-83 | Fold-2 | OOF Score: 0.34276718634690434\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 85s 492ms/step - loss: 1.2145 - rmse: 1.0135 - val_loss: 0.6520 - val_rmse: 0.4280\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.42797, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_4C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.9326 - rmse: 0.7169 - val_loss: 0.6506 - val_rmse: 0.4257\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.42797 to 0.42566, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_4C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 472ms/step - loss: 0.7977 - rmse: 0.5777 - val_loss: 0.5767 - val_rmse: 0.3555\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.42566 to 0.35554, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_4C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.7214 - rmse: 0.4997 - val_loss: 0.7224 - val_rmse: 0.4981\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.35554\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.7072 - rmse: 0.4857 - val_loss: 0.5729 - val_rmse: 0.3492\n",
      "\n",
      "Epoch 00005: val_rmse improved from 0.35554 to 0.34925, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_4C.h5\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6772 - rmse: 0.4559 - val_loss: 0.5989 - val_rmse: 0.3804\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.34925\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6383 - rmse: 0.4202 - val_loss: 0.6057 - val_rmse: 0.3840\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.34925\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5922 - rmse: 0.3711 - val_loss: 0.5815 - val_rmse: 0.3605\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.34925\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5653 - rmse: 0.3432 - val_loss: 0.6694 - val_rmse: 0.4470\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.34925\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.5460 - rmse: 0.3227 - val_loss: 0.5935 - val_rmse: 0.3732\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.34925\n",
      "Epoch 00010: early stopping\n",
      "Seed-83 | Fold-3 | OOF Score: 0.3492470996393605\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 87s 496ms/step - loss: 1.2208 - rmse: 1.0208 - val_loss: 0.5158 - val_rmse: 0.2903\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.29032, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_5C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.8937 - rmse: 0.6738 - val_loss: 0.5323 - val_rmse: 0.3096\n",
      "\n",
      "Epoch 00002: val_rmse did not improve from 0.29032\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.7845 - rmse: 0.5647 - val_loss: 0.6826 - val_rmse: 0.4581\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.29032\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.7264 - rmse: 0.5042 - val_loss: 0.4700 - val_rmse: 0.2430\n",
      "\n",
      "Epoch 00004: val_rmse improved from 0.29032 to 0.24297, saving model to ./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_5C.h5\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.6587 - rmse: 0.4358 - val_loss: 0.4730 - val_rmse: 0.2474\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.24297\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6288 - rmse: 0.4057 - val_loss: 0.5341 - val_rmse: 0.3096\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.24297\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6151 - rmse: 0.3920 - val_loss: 0.5116 - val_rmse: 0.2867\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.24297\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5810 - rmse: 0.3562 - val_loss: 0.5978 - val_rmse: 0.3718\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.24297\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.5707 - rmse: 0.3483 - val_loss: 0.4901 - val_rmse: 0.2655\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.24297\n",
      "Epoch 00009: early stopping\n",
      "Seed-83 | Fold-4 | OOF Score: 0.2429676229549472\n",
      "\n",
      "Seed: 83 | Aggregate OOF Score: 0.3618051868446747\n",
      "\n",
      "\n",
      "Aggregate OOF Score: 0.3618051868446747\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "seeds = np.random.randint(0, 100, size=NUM_SEED)\n",
    "\n",
    "counter = 0\n",
    "oof_score = 0\n",
    "y_pred_final1 = 0\n",
    "\n",
    "\n",
    "for sidx, seed in enumerate(seeds):\n",
    "    seed_score = 0\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n",
    "        counter += 1\n",
    "\n",
    "        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n",
    "        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n",
    "        train_y, val_y = Ytrain[train], Ytrain[val]\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        model = commonlit_model(transformer_model)\n",
    "        \n",
    "        model.compile(loss=rmse_loss,\n",
    "                      metrics=[RootMeanSquaredError(name='rmse')],\n",
    "                      optimizer=Adam(lr=8e-5))\n",
    "\n",
    "        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n",
    "                              restore_best_weights=True, \n",
    "                              patience=5, verbose=VERBOSE)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n",
    "                                      min_lr=1e-7, patience=2, \n",
    "                                      verbose=VERBOSE, mode='min')\n",
    "\n",
    "        chk_point = ModelCheckpoint(f'./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_{counter}C.h5', \n",
    "                                    monitor='val_rmse', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='min',\n",
    "                                    save_weights_only=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            [train_x_id, train_x_mask, train_x_token], train_y, \n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            epochs=NUM_EPOCH, \n",
    "            verbose=VERBOSE, \n",
    "            callbacks=[reduce_lr, early, chk_point], \n",
    "            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n",
    "        )\n",
    "        \n",
    "        model.load_weights(f'./Bert-Base-Uncased/CLRP_Bert_Base_Uncased_{counter}C.h5')\n",
    "        \n",
    "        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n",
    "        y_pred_final1 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n",
    "        \n",
    "        score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "        oof_score += score\n",
    "        seed_score += score\n",
    "        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n",
    "    \n",
    "    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n",
    "\n",
    "\n",
    "y_pred_final1 = y_pred_final1 / float(counter)\n",
    "oof_score /= float(counter)\n",
    "print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-theme",
   "metadata": {
    "papermill": {
     "duration": 2.199982,
     "end_time": "2021-06-23T17:43:06.434854",
     "exception": false,
     "start_time": "2021-06-23T17:43:04.234872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Albert-Base-V2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-steal",
   "metadata": {
    "papermill": {
     "duration": 1.715252,
     "end_time": "2021-06-23T17:43:09.861567",
     "exception": false,
     "start_time": "2021-06-23T17:43:08.146315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate word tokens and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "subsequent-latest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T17:43:13.556304Z",
     "iopub.status.busy": "2021-06-23T17:43:13.555750Z",
     "iopub.status.idle": "2021-06-23T17:43:13.700765Z",
     "shell.execute_reply": "2021-06-23T17:43:13.700328Z",
     "shell.execute_reply.started": "2021-06-17T07:13:56.838933Z"
    },
    "papermill": {
     "duration": 1.890821,
     "end_time": "2021-06-23T17:43:13.700889",
     "exception": false,
     "start_time": "2021-06-23T17:43:11.810068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(ALBERT_BASE_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fitted-questionnaire",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T17:43:17.241614Z",
     "iopub.status.busy": "2021-06-23T17:43:17.241070Z",
     "iopub.status.idle": "2021-06-23T17:43:23.435191Z",
     "shell.execute_reply": "2021-06-23T17:43:23.434640Z",
     "shell.execute_reply.started": "2021-06-17T07:13:57.342563Z"
    },
    "papermill": {
     "duration": 7.973124,
     "end_time": "2021-06-23T17:43:23.435333",
     "exception": false,
     "start_time": "2021-06-23T17:43:15.462209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [00:05<00:00, 541.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (2834, 216) \n",
      "Attention Mask: (2834, 216) \n",
      "Token-type-ids: (2834, 216)\n"
     ]
    }
   ],
   "source": [
    "Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\n",
    "Xtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n",
    "Xtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "approved-peace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T17:43:26.987004Z",
     "iopub.status.busy": "2021-06-23T17:43:26.986211Z",
     "iopub.status.idle": "2021-06-23T17:43:27.008125Z",
     "shell.execute_reply": "2021-06-23T17:43:27.007691Z",
     "shell.execute_reply.started": "2021-06-17T07:14:03.385628Z"
    },
    "papermill": {
     "duration": 1.784101,
     "end_time": "2021-06-23T17:43:27.008246",
     "exception": false,
     "start_time": "2021-06-23T17:43:25.224145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 459.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (7, 216) \n",
      "Attention Mask: (7, 216) \n",
      "Token-type-ids: (7, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\n",
    "Xtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\n",
    "Xtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-entertainment",
   "metadata": {
    "papermill": {
     "duration": 1.801934,
     "end_time": "2021-06-23T17:43:30.540750",
     "exception": false,
     "start_time": "2021-06-23T17:43:28.738816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize the Albert-V2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "african-adoption",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T17:43:34.321937Z",
     "iopub.status.busy": "2021-06-23T17:43:34.321379Z",
     "iopub.status.idle": "2021-06-23T17:43:34.591800Z",
     "shell.execute_reply": "2021-06-23T17:43:34.592607Z",
     "shell.execute_reply.started": "2021-06-17T07:16:06.179704Z"
    },
    "papermill": {
     "duration": 2.262462,
     "end_time": "2021-06-23T17:43:34.592778",
     "exception": false,
     "start_time": "2021-06-23T17:43:32.330316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/albert-base-v2-tf2 were not used when initializing TFAlbertModel: ['encoder', 'embeddings', 'pooler']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFAlbertModel were not initialized from the model checkpoint at ../input/albert-base-v2-tf2 and are newly initialized: ['albert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "config = AlbertConfig.from_pretrained(\n",
    "    ALBERT_BASE_V2,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "'''\n",
    "config = AlbertConfig(\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072\n",
    ")\n",
    "config.output_hidden_states = False\n",
    "\n",
    "transformer_model = TFAlbertModel.from_pretrained(ALBERT_BASE_V2, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interracial-quebec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T17:43:38.555395Z",
     "iopub.status.busy": "2021-06-23T17:43:38.554606Z",
     "iopub.status.idle": "2021-06-23T17:43:41.612560Z",
     "shell.execute_reply": "2021-06-23T17:43:41.612119Z",
     "shell.execute_reply.started": "2021-06-17T07:16:08.218466Z"
    },
    "papermill": {
     "duration": 5.208974,
     "end_time": "2021-06-23T17:43:41.612688",
     "exception": false,
     "start_time": "2021-06-23T17:43:36.403714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CommonLit_Readability_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_albert_model (TFAlbertModel) TFBaseModelOutputWit 11683584    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 216, 768)     1536        tf_albert_model[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_12 (Weight (None, 108, 384)     2950273     layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 108, 384)     768         weight_normalization_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 108, 384)     0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_12 (SpatialDr (None, 108, 384)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_13 (Weight (None, 54, 192)      737857      spatial_dropout1d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 54, 192)      384         weight_normalization_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 54, 192)      0           layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_13 (SpatialDr (None, 54, 192)      0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 10368)        0           spatial_dropout1d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 10368)        0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            10369       dropout_47[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 15,384,771\n",
      "Trainable params: 13,540,993\n",
      "Non-trainable params: 1,843,778\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = commonlit_model(transformer_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-tourist",
   "metadata": {
    "papermill": {
     "duration": 2.000615,
     "end_time": "2021-06-23T17:43:45.353386",
     "exception": false,
     "start_time": "2021-06-23T17:43:43.352771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fit the model with K-Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "statistical-politics",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T17:43:48.853493Z",
     "iopub.status.busy": "2021-06-23T17:43:48.848217Z",
     "iopub.status.idle": "2021-06-23T18:49:11.159256Z",
     "shell.execute_reply": "2021-06-23T18:49:11.159875Z",
     "shell.execute_reply.started": "2021-06-17T07:16:11.069962Z"
    },
    "papermill": {
     "duration": 3924.08101,
     "end_time": "2021-06-23T18:49:11.160081",
     "exception": false,
     "start_time": "2021-06-23T17:43:47.079071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "142/142 [==============================] - 84s 495ms/step - loss: 1.6104 - rmse: 1.4075 - val_loss: 1.4447 - val_rmse: 1.2571\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 1.25711, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_1C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.4431 - rmse: 1.2314 - val_loss: 1.2484 - val_rmse: 1.0474\n",
      "\n",
      "Epoch 00002: val_rmse improved from 1.25711 to 1.04742, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_1C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.3535 - rmse: 1.1385 - val_loss: 1.2504 - val_rmse: 1.0494\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 1.04742\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.3463 - rmse: 1.1290 - val_loss: 1.2479 - val_rmse: 1.0475\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 1.04742\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.3140 - rmse: 1.0997 - val_loss: 1.2476 - val_rmse: 1.0476\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 1.04742\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 1.3172 - rmse: 1.1022 - val_loss: 1.2474 - val_rmse: 1.0476\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 1.04742\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 1.2855 - rmse: 1.0728 - val_loss: 1.2474 - val_rmse: 1.0475\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 1.04742\n",
      "Epoch 00007: early stopping\n",
      "Seed-85 | Fold-0 | OOF Score: 1.0474214807001931\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 85s 497ms/step - loss: 1.5767 - rmse: 1.3701 - val_loss: 1.2458 - val_rmse: 1.0512\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 1.05125, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.4518 - rmse: 1.2398 - val_loss: 1.2461 - val_rmse: 1.0534\n",
      "\n",
      "Epoch 00002: val_rmse did not improve from 1.05125\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 1.3869 - rmse: 1.1753 - val_loss: 1.2417 - val_rmse: 1.0494\n",
      "\n",
      "Epoch 00003: val_rmse improved from 1.05125 to 1.04939, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 1.3434 - rmse: 1.1348 - val_loss: 1.2407 - val_rmse: 1.0501\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 1.04939\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 1.3224 - rmse: 1.1118 - val_loss: 1.2426 - val_rmse: 1.0543\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 1.04939\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2705 - rmse: 1.0636 - val_loss: 1.2380 - val_rmse: 1.0493\n",
      "\n",
      "Epoch 00006: val_rmse improved from 1.04939 to 1.04933, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2726 - rmse: 1.0621 - val_loss: 1.2378 - val_rmse: 1.0495\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 1.04933\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2573 - rmse: 1.0469 - val_loss: 1.2375 - val_rmse: 1.0496\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 1.04933\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 476ms/step - loss: 1.2797 - rmse: 1.0726 - val_loss: 1.2359 - val_rmse: 1.0486\n",
      "\n",
      "Epoch 00009: val_rmse improved from 1.04933 to 1.04856, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2623 - rmse: 1.0570 - val_loss: 1.2401 - val_rmse: 1.0533\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 1.04856\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2914 - rmse: 1.0833 - val_loss: 1.2349 - val_rmse: 1.0485\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00011: val_rmse improved from 1.04856 to 1.04851, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2766 - rmse: 1.0694 - val_loss: 1.2348 - val_rmse: 1.0486\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 1.04851\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2639 - rmse: 1.0540 - val_loss: 1.2347 - val_rmse: 1.0487\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 1.04851\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2689 - rmse: 1.0638 - val_loss: 1.2346 - val_rmse: 1.0487\n",
      "\n",
      "Epoch 00014: val_rmse did not improve from 1.04851\n",
      "Epoch 15/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2675 - rmse: 1.0580 - val_loss: 1.2342 - val_rmse: 1.0485\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00015: val_rmse did not improve from 1.04851\n",
      "Epoch 16/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2639 - rmse: 1.0587 - val_loss: 1.2341 - val_rmse: 1.0485\n",
      "\n",
      "Epoch 00016: val_rmse improved from 1.04851 to 1.04849, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Epoch 17/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2857 - rmse: 1.0757 - val_loss: 1.2340 - val_rmse: 1.0485\n",
      "\n",
      "Epoch 00017: val_rmse improved from 1.04849 to 1.04845, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Epoch 18/20\n",
      "142/142 [==============================] - 68s 476ms/step - loss: 1.2485 - rmse: 1.0422 - val_loss: 1.2339 - val_rmse: 1.0484\n",
      "\n",
      "Epoch 00018: val_rmse improved from 1.04845 to 1.04843, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Epoch 19/20\n",
      "142/142 [==============================] - 67s 476ms/step - loss: 1.2553 - rmse: 1.0488 - val_loss: 1.2338 - val_rmse: 1.0484\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00019: val_rmse improved from 1.04843 to 1.04840, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Epoch 20/20\n",
      "142/142 [==============================] - 68s 476ms/step - loss: 1.3043 - rmse: 1.0946 - val_loss: 1.2337 - val_rmse: 1.0484\n",
      "\n",
      "Epoch 00020: val_rmse improved from 1.04840 to 1.04836, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_2C.h5\n",
      "Seed-85 | Fold-1 | OOF Score: 1.0483620295987142\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 87s 498ms/step - loss: 1.6263 - rmse: 1.4149 - val_loss: 1.1707 - val_rmse: 0.9732\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.97325, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_3C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.3655 - rmse: 1.1516 - val_loss: 1.1023 - val_rmse: 0.8934\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.97325 to 0.89337, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_3C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.2372 - rmse: 1.0263 - val_loss: 1.0337 - val_rmse: 0.8211\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.89337 to 0.82111, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_3C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.1739 - rmse: 0.9611 - val_loss: 0.9750 - val_rmse: 0.7589\n",
      "\n",
      "Epoch 00004: val_rmse improved from 0.82111 to 0.75885, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_3C.h5\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.9944 - rmse: 0.7802 - val_loss: 0.9194 - val_rmse: 0.7014\n",
      "\n",
      "Epoch 00005: val_rmse improved from 0.75885 to 0.70144, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_3C.h5\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.8879 - rmse: 0.6734 - val_loss: 0.9247 - val_rmse: 0.7085\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.70144\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 68s 476ms/step - loss: 0.8581 - rmse: 0.6452 - val_loss: 0.9078 - val_rmse: 0.6914\n",
      "\n",
      "Epoch 00007: val_rmse improved from 0.70144 to 0.69137, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_3C.h5\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.7782 - rmse: 0.5628 - val_loss: 0.9602 - val_rmse: 0.7466\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.69137\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.7689 - rmse: 0.5562 - val_loss: 0.8977 - val_rmse: 0.6808\n",
      "\n",
      "Epoch 00009: val_rmse improved from 0.69137 to 0.68079, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_3C.h5\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 67s 476ms/step - loss: 0.7130 - rmse: 0.4987 - val_loss: 0.9180 - val_rmse: 0.7047\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.68079\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 68s 476ms/step - loss: 0.6808 - rmse: 0.4666 - val_loss: 0.9127 - val_rmse: 0.6983\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.68079\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.6421 - rmse: 0.4275 - val_loss: 0.9118 - val_rmse: 0.6970\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.68079\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.6029 - rmse: 0.3879 - val_loss: 0.9082 - val_rmse: 0.6942\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.68079\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 68s 476ms/step - loss: 0.5974 - rmse: 0.3826 - val_loss: 0.9102 - val_rmse: 0.6961\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00014: val_rmse did not improve from 0.68079\n",
      "Epoch 00014: early stopping\n",
      "Seed-85 | Fold-2 | OOF Score: 0.6807893471273513\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 86s 494ms/step - loss: 1.3042 - rmse: 1.0963 - val_loss: 0.6134 - val_rmse: 0.3912\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.39120, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_4C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 1.0234 - rmse: 0.8062 - val_loss: 0.5783 - val_rmse: 0.3550\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.39120 to 0.35502, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_4C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.9785 - rmse: 0.7651 - val_loss: 0.6494 - val_rmse: 0.4275\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.35502\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.8855 - rmse: 0.6656 - val_loss: 0.6220 - val_rmse: 0.3994\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.35502\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.7929 - rmse: 0.5718 - val_loss: 0.6601 - val_rmse: 0.4392\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.35502\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.7759 - rmse: 0.5572 - val_loss: 0.6569 - val_rmse: 0.4346\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.35502\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.7515 - rmse: 0.5337 - val_loss: 0.5917 - val_rmse: 0.3701\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.35502\n",
      "Epoch 00007: early stopping\n",
      "Seed-85 | Fold-3 | OOF Score: 0.35502313939410673\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 84s 493ms/step - loss: 1.2773 - rmse: 1.0657 - val_loss: 0.5582 - val_rmse: 0.3368\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.33682, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_5C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.9902 - rmse: 0.7736 - val_loss: 0.6977 - val_rmse: 0.4735\n",
      "\n",
      "Epoch 00002: val_rmse did not improve from 0.33682\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.8995 - rmse: 0.6790 - val_loss: 0.5451 - val_rmse: 0.3267\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.33682 to 0.32675, saving model to ./Albert-Base-V2/CLRP_Albert_Base_V2_5C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.8226 - rmse: 0.6032 - val_loss: 0.6275 - val_rmse: 0.4050\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.32675\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 67s 474ms/step - loss: 0.7492 - rmse: 0.5304 - val_loss: 0.5742 - val_rmse: 0.3533\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.32675\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.7221 - rmse: 0.5007 - val_loss: 0.5722 - val_rmse: 0.3511\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.32675\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 67s 473ms/step - loss: 0.7008 - rmse: 0.4813 - val_loss: 0.6046 - val_rmse: 0.3828\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.32675\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 67s 475ms/step - loss: 0.6586 - rmse: 0.4376 - val_loss: 0.5804 - val_rmse: 0.3602\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.32675\n",
      "Epoch 00008: early stopping\n",
      "Seed-85 | Fold-4 | OOF Score: 0.32674624606735886\n",
      "\n",
      "Seed: 85 | Aggregate OOF Score: 0.6916684485775447\n",
      "\n",
      "\n",
      "Aggregate OOF Score: 0.6916684485775447\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29)\n",
    "seeds = np.random.randint(0, 100, size=NUM_SEED)\n",
    "\n",
    "counter = 0\n",
    "oof_score = 0\n",
    "y_pred_final2 = 0\n",
    "\n",
    "\n",
    "for sidx, seed in enumerate(seeds):\n",
    "    seed_score = 0\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n",
    "        counter += 1\n",
    "\n",
    "        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n",
    "        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n",
    "        train_y, val_y = Ytrain[train], Ytrain[val]\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        model = commonlit_model(transformer_model)\n",
    "        \n",
    "        model.compile(loss=rmse_loss,\n",
    "                      metrics=[RootMeanSquaredError(name='rmse')],\n",
    "                      optimizer=Adam(lr=4e-5))\n",
    "\n",
    "        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n",
    "                              restore_best_weights=True, \n",
    "                              patience=5, verbose=VERBOSE)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n",
    "                                      min_lr=1e-7, patience=2, \n",
    "                                      verbose=VERBOSE, mode='min')\n",
    "\n",
    "        chk_point = ModelCheckpoint(f'./Albert-Base-V2/CLRP_Albert_Base_V2_{counter}C.h5', \n",
    "                                    monitor='val_rmse', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='min',\n",
    "                                    save_weights_only=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            [train_x_id, train_x_mask, train_x_token], train_y, \n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            epochs=NUM_EPOCH, \n",
    "            verbose=VERBOSE, \n",
    "            callbacks=[reduce_lr, early, chk_point], \n",
    "            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n",
    "        )\n",
    "        \n",
    "        model.load_weights(f'./Albert-Base-V2/CLRP_Albert_Base_V2_{counter}C.h5')\n",
    "        \n",
    "        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n",
    "        y_pred_final2 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n",
    "        \n",
    "        score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "        oof_score += score\n",
    "        seed_score += score\n",
    "        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n",
    "    \n",
    "    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n",
    "\n",
    "\n",
    "y_pred_final2 = y_pred_final2 / float(counter)\n",
    "oof_score /= float(counter)\n",
    "print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-collapse",
   "metadata": {
    "papermill": {
     "duration": 3.824627,
     "end_time": "2021-06-23T18:49:19.043412",
     "exception": false,
     "start_time": "2021-06-23T18:49:15.218785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DistilBert-Base-Uncased Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-repeat",
   "metadata": {
    "papermill": {
     "duration": 4.342878,
     "end_time": "2021-06-23T18:49:27.400210",
     "exception": false,
     "start_time": "2021-06-23T18:49:23.057332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Generate word tokens and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "third-violation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T18:49:35.189779Z",
     "iopub.status.busy": "2021-06-23T18:49:35.189103Z",
     "iopub.status.idle": "2021-06-23T18:49:35.259862Z",
     "shell.execute_reply": "2021-06-23T18:49:35.260469Z",
     "shell.execute_reply.started": "2021-06-17T07:09:21.108895Z"
    },
    "papermill": {
     "duration": 4.096478,
     "end_time": "2021-06-23T18:49:35.260622",
     "exception": false,
     "start_time": "2021-06-23T18:49:31.164144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(DISTILBERT_BASE_UNCASED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "invisible-installation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T18:49:42.846051Z",
     "iopub.status.busy": "2021-06-23T18:49:42.845213Z",
     "iopub.status.idle": "2021-06-23T18:49:57.901395Z",
     "shell.execute_reply": "2021-06-23T18:49:57.902008Z",
     "shell.execute_reply.started": "2021-06-17T07:09:21.30948Z"
    },
    "papermill": {
     "duration": 18.838533,
     "end_time": "2021-06-23T18:49:57.902303",
     "exception": false,
     "start_time": "2021-06-23T18:49:39.063770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2834/2834 [00:14<00:00, 190.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (2834, 216) \n",
      "Attention Mask: (2834, 216) \n",
      "Token-type-ids: (2834, 216)\n"
     ]
    }
   ],
   "source": [
    "Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\n",
    "Xtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n",
    "Xtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cognitive-aggregate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T18:50:05.940364Z",
     "iopub.status.busy": "2021-06-23T18:50:05.939504Z",
     "iopub.status.idle": "2021-06-23T18:50:05.982807Z",
     "shell.execute_reply": "2021-06-23T18:50:05.983221Z",
     "shell.execute_reply.started": "2021-06-17T07:09:36.32679Z"
    },
    "papermill": {
     "duration": 3.885111,
     "end_time": "2021-06-23T18:50:05.983371",
     "exception": false,
     "start_time": "2021-06-23T18:50:02.098260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 189.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-ids: (7, 216) \n",
      "Attention Mask: (7, 216) \n",
      "Token-type-ids: (7, 216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n",
    "\n",
    "Xtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\n",
    "Xtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\n",
    "Xtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n",
    "    \n",
    "print(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-destruction",
   "metadata": {
    "papermill": {
     "duration": 3.835814,
     "end_time": "2021-06-23T18:50:13.933510",
     "exception": false,
     "start_time": "2021-06-23T18:50:10.097696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize the DistilBert-Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "liberal-rental",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T18:50:21.864115Z",
     "iopub.status.busy": "2021-06-23T18:50:21.863539Z",
     "iopub.status.idle": "2021-06-23T18:50:26.286432Z",
     "shell.execute_reply": "2021-06-23T18:50:26.285669Z",
     "shell.execute_reply.started": "2021-06-17T07:09:36.376323Z"
    },
    "papermill": {
     "duration": 8.275971,
     "end_time": "2021-06-23T18:50:26.286559",
     "exception": false,
     "start_time": "2021-06-23T18:50:18.010588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at ../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "config = DistilBertConfig.from_pretrained(\n",
    "    DISTILBERT_BASE_UNCASED,\n",
    "    hidden_dropout_prob=0.1, \n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "'''\n",
    "config = DistilBertConfig()\n",
    "config.output_hidden_states = False\n",
    "\n",
    "transformer_model = TFDistilBertModel.from_pretrained(DISTILBERT_BASE_UNCASED, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "operating-tanzania",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T18:50:34.688665Z",
     "iopub.status.busy": "2021-06-23T18:50:34.687868Z",
     "iopub.status.idle": "2021-06-23T18:50:36.653919Z",
     "shell.execute_reply": "2021-06-23T18:50:36.653099Z",
     "shell.execute_reply.started": "2021-06-17T07:09:39.457144Z"
    },
    "papermill": {
     "duration": 5.869615,
     "end_time": "2021-06-23T18:50:36.654060",
     "exception": false,
     "start_time": "2021-06-23T18:50:30.784445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CommonLit_Readability_Model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 216, 768)     1536        tf_distil_bert_model[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_24 (Weight (None, 108, 384)     2950273     layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 108, 384)     768         weight_normalization_24[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 108, 384)     0           layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_24 (SpatialDr (None, 108, 384)     0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_25 (Weight (None, 54, 192)      737857      spatial_dropout1d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 54, 192)      384         weight_normalization_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 54, 192)      0           layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_25 (SpatialDr (None, 54, 192)      0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 10368)        0           spatial_dropout1d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, 10368)        0           flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 216)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            10369       dropout_72[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 70,064,067\n",
      "Trainable params: 68,220,289\n",
      "Non-trainable params: 1,843,778\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = commonlit_model(transformer_model, use_tokens_type_ids=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-drilling",
   "metadata": {
    "papermill": {
     "duration": 3.957774,
     "end_time": "2021-06-23T18:50:44.708420",
     "exception": false,
     "start_time": "2021-06-23T18:50:40.750646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fit the model with K-Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "complex-purchase",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T18:50:52.634931Z",
     "iopub.status.busy": "2021-06-23T18:50:52.633888Z",
     "iopub.status.idle": "2021-06-23T19:28:15.723495Z",
     "shell.execute_reply": "2021-06-23T19:28:15.723871Z",
     "shell.execute_reply.started": "2021-06-17T07:09:44.845797Z"
    },
    "papermill": {
     "duration": 2247.209626,
     "end_time": "2021-06-23T19:28:15.724071",
     "exception": false,
     "start_time": "2021-06-23T18:50:48.514445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "142/142 [==============================] - 46s 256ms/step - loss: 1.6091 - rmse: 1.4108 - val_loss: 0.9258 - val_rmse: 0.7061\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.70613, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_1C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 1.1066 - rmse: 0.8918 - val_loss: 0.8253 - val_rmse: 0.6042\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.70613 to 0.60425, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_1C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.9179 - rmse: 0.6981 - val_loss: 0.8413 - val_rmse: 0.6206\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.60425\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.8053 - rmse: 0.5848 - val_loss: 0.7956 - val_rmse: 0.5728\n",
      "\n",
      "Epoch 00004: val_rmse improved from 0.60425 to 0.57285, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_1C.h5\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 34s 243ms/step - loss: 0.7515 - rmse: 0.5343 - val_loss: 0.8597 - val_rmse: 0.6366\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.57285\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.6983 - rmse: 0.4794 - val_loss: 0.8661 - val_rmse: 0.6466\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.57285\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.6750 - rmse: 0.4526 - val_loss: 0.8281 - val_rmse: 0.6078\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.57285\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.6128 - rmse: 0.3892 - val_loss: 0.8603 - val_rmse: 0.6396\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.57285\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5715 - rmse: 0.3482 - val_loss: 0.8180 - val_rmse: 0.5966\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.57285\n",
      "Epoch 00009: early stopping\n",
      "Seed-82 | Fold-0 | OOF Score: 0.5728470733668614\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 46s 257ms/step - loss: 1.2546 - rmse: 1.0386 - val_loss: 0.6997 - val_rmse: 0.4768\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.47676, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_2C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 35s 245ms/step - loss: 0.9634 - rmse: 0.7464 - val_loss: 0.6483 - val_rmse: 0.4275\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.47676 to 0.42750, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_2C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.8104 - rmse: 0.5905 - val_loss: 0.6077 - val_rmse: 0.3902\n",
      "\n",
      "Epoch 00003: val_rmse improved from 0.42750 to 0.39018, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_2C.h5\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.7412 - rmse: 0.5193 - val_loss: 0.6168 - val_rmse: 0.3955\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.39018\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.6963 - rmse: 0.4729 - val_loss: 0.6659 - val_rmse: 0.4503\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.39018\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.6433 - rmse: 0.4214 - val_loss: 0.6357 - val_rmse: 0.4162\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.39018\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 34s 241ms/step - loss: 0.6158 - rmse: 0.3933 - val_loss: 0.5956 - val_rmse: 0.3740\n",
      "\n",
      "Epoch 00007: val_rmse improved from 0.39018 to 0.37395, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_2C.h5\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 34s 243ms/step - loss: 0.5768 - rmse: 0.3545 - val_loss: 0.6394 - val_rmse: 0.4196\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.37395\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 34s 241ms/step - loss: 0.5681 - rmse: 0.3442 - val_loss: 0.5932 - val_rmse: 0.3723\n",
      "\n",
      "Epoch 00009: val_rmse improved from 0.37395 to 0.37228, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_2C.h5\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.5602 - rmse: 0.3371 - val_loss: 0.6229 - val_rmse: 0.4038\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.37228\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 34s 241ms/step - loss: 0.5501 - rmse: 0.3267 - val_loss: 0.5985 - val_rmse: 0.3779\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.37228\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.5342 - rmse: 0.3111 - val_loss: 0.6007 - val_rmse: 0.3802\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.37228\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5122 - rmse: 0.2880 - val_loss: 0.6185 - val_rmse: 0.3991\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.37228\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.4920 - rmse: 0.2690 - val_loss: 0.6389 - val_rmse: 0.4186\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00014: val_rmse did not improve from 0.37228\n",
      "Epoch 00014: early stopping\n",
      "Seed-82 | Fold-1 | OOF Score: 0.3722774247360091\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 46s 264ms/step - loss: 1.1855 - rmse: 0.9850 - val_loss: 0.5203 - val_rmse: 0.2948\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.29477, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_3C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.8466 - rmse: 0.6230 - val_loss: 0.5331 - val_rmse: 0.3074\n",
      "\n",
      "Epoch 00002: val_rmse did not improve from 0.29477\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 34s 243ms/step - loss: 0.7568 - rmse: 0.5326 - val_loss: 0.6983 - val_rmse: 0.4732\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.29477\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.7001 - rmse: 0.4783 - val_loss: 0.4330 - val_rmse: 0.2067\n",
      "\n",
      "Epoch 00004: val_rmse improved from 0.29477 to 0.20672, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_3C.h5\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.6438 - rmse: 0.4213 - val_loss: 0.5044 - val_rmse: 0.2829\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.20672\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.6152 - rmse: 0.3919 - val_loss: 0.4682 - val_rmse: 0.2446\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.20672\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.5934 - rmse: 0.3693 - val_loss: 0.4201 - val_rmse: 0.1996\n",
      "\n",
      "Epoch 00007: val_rmse improved from 0.20672 to 0.19957, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_3C.h5\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5675 - rmse: 0.3432 - val_loss: 0.4196 - val_rmse: 0.1958\n",
      "\n",
      "Epoch 00008: val_rmse improved from 0.19957 to 0.19580, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_3C.h5\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.5571 - rmse: 0.3341 - val_loss: 0.4953 - val_rmse: 0.2741\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.19580\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5409 - rmse: 0.3173 - val_loss: 0.4135 - val_rmse: 0.1905\n",
      "\n",
      "Epoch 00010: val_rmse improved from 0.19580 to 0.19053, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_3C.h5\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.5354 - rmse: 0.3108 - val_loss: 0.4379 - val_rmse: 0.2147\n",
      "\n",
      "Epoch 00011: val_rmse did not improve from 0.19053\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5313 - rmse: 0.3080 - val_loss: 0.4339 - val_rmse: 0.2151\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00012: val_rmse did not improve from 0.19053\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.5187 - rmse: 0.2938 - val_loss: 0.4434 - val_rmse: 0.2228\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.19053\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5105 - rmse: 0.2856 - val_loss: 0.4675 - val_rmse: 0.2475\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00014: val_rmse did not improve from 0.19053\n",
      "Epoch 15/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.5016 - rmse: 0.2778 - val_loss: 0.4249 - val_rmse: 0.2043\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00015: val_rmse did not improve from 0.19053\n",
      "Epoch 00015: early stopping\n",
      "Seed-82 | Fold-2 | OOF Score: 0.19052898729738482\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 45s 254ms/step - loss: 1.1370 - rmse: 0.9365 - val_loss: 0.4382 - val_rmse: 0.2132\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.21318, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_4C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.7867 - rmse: 0.5687 - val_loss: 0.4207 - val_rmse: 0.1932\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.21318 to 0.19322, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_4C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.6934 - rmse: 0.4721 - val_loss: 0.4178 - val_rmse: 0.1941\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.19322\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.6290 - rmse: 0.4048 - val_loss: 0.4441 - val_rmse: 0.2186\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.19322\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5986 - rmse: 0.3741 - val_loss: 0.4272 - val_rmse: 0.2027\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.19322\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 35s 245ms/step - loss: 0.5578 - rmse: 0.3342 - val_loss: 0.4064 - val_rmse: 0.1816\n",
      "\n",
      "Epoch 00006: val_rmse improved from 0.19322 to 0.18159, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_4C.h5\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5443 - rmse: 0.3211 - val_loss: 0.4101 - val_rmse: 0.1861\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.18159\n",
      "Epoch 8/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.5197 - rmse: 0.2971 - val_loss: 0.5169 - val_rmse: 0.2944\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00008: val_rmse did not improve from 0.18159\n",
      "Epoch 9/20\n",
      "142/142 [==============================] - 34s 241ms/step - loss: 0.5122 - rmse: 0.2890 - val_loss: 0.4173 - val_rmse: 0.1929\n",
      "\n",
      "Epoch 00009: val_rmse did not improve from 0.18159\n",
      "Epoch 10/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.4875 - rmse: 0.2645 - val_loss: 0.4067 - val_rmse: 0.1830\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00010: val_rmse did not improve from 0.18159\n",
      "Epoch 11/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.4881 - rmse: 0.2641 - val_loss: 0.4034 - val_rmse: 0.1802\n",
      "\n",
      "Epoch 00011: val_rmse improved from 0.18159 to 0.18018, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_4C.h5\n",
      "Epoch 12/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.4798 - rmse: 0.2571 - val_loss: 0.3830 - val_rmse: 0.1587\n",
      "\n",
      "Epoch 00012: val_rmse improved from 0.18018 to 0.15866, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_4C.h5\n",
      "Epoch 13/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.4760 - rmse: 0.2525 - val_loss: 0.4179 - val_rmse: 0.1951\n",
      "\n",
      "Epoch 00013: val_rmse did not improve from 0.15866\n",
      "Epoch 14/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.4798 - rmse: 0.2565 - val_loss: 0.4436 - val_rmse: 0.2220\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00014: val_rmse did not improve from 0.15866\n",
      "Epoch 15/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.4704 - rmse: 0.2467 - val_loss: 0.3900 - val_rmse: 0.1665\n",
      "\n",
      "Epoch 00015: val_rmse did not improve from 0.15866\n",
      "Epoch 16/20\n",
      "142/142 [==============================] - 35s 245ms/step - loss: 0.4640 - rmse: 0.2404 - val_loss: 0.4012 - val_rmse: 0.1783\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00016: val_rmse did not improve from 0.15866\n",
      "Epoch 17/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.4692 - rmse: 0.2456 - val_loss: 0.4035 - val_rmse: 0.1805\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_rmse did not improve from 0.15866\n",
      "Epoch 00017: early stopping\n",
      "Seed-82 | Fold-3 | OOF Score: 0.15866087102496532\n",
      "Epoch 1/20\n",
      "142/142 [==============================] - 49s 254ms/step - loss: 1.1750 - rmse: 0.9826 - val_loss: 0.4807 - val_rmse: 0.2554\n",
      "\n",
      "Epoch 00001: val_rmse improved from inf to 0.25537, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_5C.h5\n",
      "Epoch 2/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.7748 - rmse: 0.5544 - val_loss: 0.3660 - val_rmse: 0.1387\n",
      "\n",
      "Epoch 00002: val_rmse improved from 0.25537 to 0.13872, saving model to ./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_5C.h5\n",
      "Epoch 3/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.6668 - rmse: 0.4435 - val_loss: 0.5129 - val_rmse: 0.2870\n",
      "\n",
      "Epoch 00003: val_rmse did not improve from 0.13872\n",
      "Epoch 4/20\n",
      "142/142 [==============================] - 35s 243ms/step - loss: 0.6645 - rmse: 0.4433 - val_loss: 0.5140 - val_rmse: 0.2900\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00004: val_rmse did not improve from 0.13872\n",
      "Epoch 5/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5961 - rmse: 0.3735 - val_loss: 0.3865 - val_rmse: 0.1623\n",
      "\n",
      "Epoch 00005: val_rmse did not improve from 0.13872\n",
      "Epoch 6/20\n",
      "142/142 [==============================] - 35s 244ms/step - loss: 0.5686 - rmse: 0.3462 - val_loss: 0.4112 - val_rmse: 0.1865\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 00006: val_rmse did not improve from 0.13872\n",
      "Epoch 7/20\n",
      "142/142 [==============================] - 34s 242ms/step - loss: 0.5390 - rmse: 0.3166 - val_loss: 0.4067 - val_rmse: 0.1829\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00007: val_rmse did not improve from 0.13872\n",
      "Epoch 00007: early stopping\n",
      "Seed-82 | Fold-4 | OOF Score: 0.13871715954737712\n",
      "\n",
      "Seed: 82 | Aggregate OOF Score: 0.2866063031945195\n",
      "\n",
      "\n",
      "Aggregate OOF Score: 0.2866063031945195\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(31)\n",
    "seeds = np.random.randint(0, 100, size=NUM_SEED)\n",
    "\n",
    "counter = 0\n",
    "oof_score = 0\n",
    "y_pred_final3 = 0\n",
    "\n",
    "\n",
    "for sidx, seed in enumerate(seeds):\n",
    "    seed_score = 0\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n",
    "        counter += 1\n",
    "\n",
    "        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n",
    "        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n",
    "        train_y, val_y = Ytrain[train], Ytrain[val]\n",
    "        \n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        model = commonlit_model(transformer_model, use_tokens_type_ids=False)\n",
    "        \n",
    "        model.compile(loss=rmse_loss,\n",
    "                      metrics=[RootMeanSquaredError(name='rmse')],\n",
    "                      optimizer=Adam(lr=8e-5))\n",
    "\n",
    "        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n",
    "                              restore_best_weights=True, \n",
    "                              patience=5, verbose=VERBOSE)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n",
    "                                      min_lr=1e-7, patience=2, \n",
    "                                      verbose=VERBOSE, mode='min')\n",
    "\n",
    "        chk_point = ModelCheckpoint(f'./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_{counter}C.h5', \n",
    "                                    monitor='val_rmse', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='min',\n",
    "                                    save_weights_only=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            [train_x_id, train_x_mask, train_x_token], train_y, \n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            epochs=NUM_EPOCH, \n",
    "            verbose=VERBOSE, \n",
    "            callbacks=[reduce_lr, early, chk_point], \n",
    "            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n",
    "        )\n",
    "        \n",
    "        model.load_weights(f'./DistilBert-Base-Uncased/CLRP_DistilBert_Base_Uncased_{counter}C.h5')\n",
    "        \n",
    "        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n",
    "        y_pred_final3 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n",
    "        \n",
    "        score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "        oof_score += score\n",
    "        seed_score += score\n",
    "        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n",
    "    \n",
    "    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n",
    "\n",
    "\n",
    "y_pred_final3 = y_pred_final3 / float(counter)\n",
    "oof_score /= float(counter)\n",
    "print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-election",
   "metadata": {
    "papermill": {
     "duration": 6.540814,
     "end_time": "2021-06-23T19:28:28.445456",
     "exception": false,
     "start_time": "2021-06-23T19:28:21.904642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "talented-ensemble",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T19:28:41.076511Z",
     "iopub.status.busy": "2021-06-23T19:28:41.075919Z",
     "iopub.status.idle": "2021-06-23T19:28:41.266590Z",
     "shell.execute_reply": "2021-06-23T19:28:41.265827Z"
    },
    "papermill": {
     "duration": 6.576821,
     "end_time": "2021-06-23T19:28:41.266715",
     "exception": false,
     "start_time": "2021-06-23T19:28:34.689894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.642899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.690138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.470357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.069943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.834201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.642899\n",
       "1  f0953f0a5 -0.690138\n",
       "2  0df072751 -0.470357\n",
       "3  04caf4e0c -2.069943\n",
       "4  0e63f8bea -1.834201"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_final = (y_pred_final1 + y_pred_final2 + y_pred_final3) / 3.0\n",
    "\n",
    "submit_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n",
    "submit_df['target'] = y_pred_final\n",
    "submit_df.to_csv(\"./submission.csv\", index=False)\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-correction",
   "metadata": {
    "papermill": {
     "duration": 6.132753,
     "end_time": "2021-06-23T19:28:54.169529",
     "exception": false,
     "start_time": "2021-06-23T19:28:48.036776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9606.635405,
   "end_time": "2021-06-23T19:29:03.726932",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-23T16:48:57.091527",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
