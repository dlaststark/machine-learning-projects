{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:46:05.217921Z",
     "iopub.status.busy": "2021-06-15T10:46:05.217613Z",
     "iopub.status.idle": "2021-06-15T10:46:05.22742Z",
     "shell.execute_reply": "2021-06-15T10:46:05.226394Z",
     "shell.execute_reply.started": "2021-06-15T10:46:05.217894Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import LSTM, GRU, Conv1D\n",
    "from tensorflow.keras.layers import Activation, Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout\n",
    "from tensorflow.keras.layers import Bidirectional, Concatenate\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../input/tf-cyclic-lr-schedule')\n",
    "from clr_callback import CyclicLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:22:15.952344Z",
     "iopub.status.busy": "2021-06-15T10:22:15.952024Z",
     "iopub.status.idle": "2021-06-15T10:22:16.066785Z",
     "shell.execute_reply": "2021-06-15T10:22:16.065897Z",
     "shell.execute_reply.started": "2021-06-15T10:22:15.95231Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n",
    "train_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\n",
    "train_df.set_index(\"id\", inplace=True)\n",
    "print(f\"train_df: {train_df.shape}\\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:22:16.0691Z",
     "iopub.status.busy": "2021-06-15T10:22:16.068737Z",
     "iopub.status.idle": "2021-06-15T10:22:16.087381Z",
     "shell.execute_reply": "2021-06-15T10:22:16.086592Z",
     "shell.execute_reply.started": "2021-06-15T10:22:16.069061Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "test_df.drop(['url_legal','license'], inplace=True, axis=1)\n",
    "test_df.set_index(\"id\", inplace=True)\n",
    "print(f\"test_df: {test_df.shape}\\n\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:22:16.089441Z",
     "iopub.status.busy": "2021-06-15T10:22:16.089041Z",
     "iopub.status.idle": "2021-06-15T10:22:16.099158Z",
     "shell.execute_reply": "2021-06-15T10:22:16.098019Z",
     "shell.execute_reply.started": "2021-06-15T10:22:16.089405Z"
    }
   },
   "outputs": [],
   "source": [
    "Ytrain = train_df['target'].values\n",
    "Ytrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\n",
    "train_df.drop(['target'], inplace=True, axis=1)\n",
    "print(f\"Ytrain: {Ytrain.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:22:16.101074Z",
     "iopub.status.busy": "2021-06-15T10:22:16.100671Z",
     "iopub.status.idle": "2021-06-15T10:22:16.112063Z",
     "shell.execute_reply": "2021-06-15T10:22:16.111176Z",
     "shell.execute_reply.started": "2021-06-15T10:22:16.101031Z"
    }
   },
   "outputs": [],
   "source": [
    "def decontraction(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    punct = []\n",
    "    punct += list(string.punctuation)\n",
    "    punct += '’'\n",
    "    punct += '-'\n",
    "    punct += ','\n",
    "    punct += '.'\n",
    "    punct += '?'\n",
    "    punct += '!'\n",
    "    punct.remove('\"')\n",
    "    \n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\n",
    "        \"N\": wordnet.NOUN, \n",
    "        \"V\": wordnet.VERB, \n",
    "        \"J\": wordnet.ADJ, \n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:22:16.113721Z",
     "iopub.status.busy": "2021-06-15T10:22:16.11333Z",
     "iopub.status.idle": "2021-06-15T10:22:16.126716Z",
     "shell.execute_reply": "2021-06-15T10:22:16.125841Z",
     "shell.execute_reply.started": "2021-06-15T10:22:16.113682Z"
    }
   },
   "outputs": [],
   "source": [
    "def dialog_parser(text):\n",
    "    \n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    \n",
    "    # let's set up some lists to hold our pieces of narrative and dialog\n",
    "    parsed_dialog = []\n",
    "    parsed_narrative = []\n",
    "    \n",
    "    # and this list will be a bucket for the text we're currently exploring\n",
    "    current = []\n",
    "\n",
    "    # now let's set up values that will help us loop through the text\n",
    "    length = len(tokenized)\n",
    "    found_q = False\n",
    "    counter = 0\n",
    "    quote_open, quote_close = '``', \"''\"\n",
    "\n",
    "    # now we'll start our loop saying that as long as our sentence is...\n",
    "    while counter < length:\n",
    "        word = tokenized[counter]\n",
    "\n",
    "        # until we find a quotation mark, we're working with narrative\n",
    "        if quote_open not in word and quote_close not in word:\n",
    "            current.append(word)\n",
    "\n",
    "        # here's what we do when we find a closed quote\n",
    "        else:\n",
    "            # we append the narrative we've collected & clear our our\n",
    "            # current variable\n",
    "            parsed_narrative.append(current)\n",
    "            current = []\n",
    "            \n",
    "            # now current is ready to hold dialog and we're working on\n",
    "            # a piece of dialog\n",
    "            current.append(word)\n",
    "            found_q = True\n",
    "\n",
    "            # while we're in the quote, we're going to increment the counter\n",
    "            # and append to current in this while loop\n",
    "            while found_q and counter < length-1:\n",
    "                counter += 1\n",
    "                if quote_close not in tokenized[counter]:\n",
    "                    current.append(tokenized[counter])\n",
    "                else:\n",
    "                    # if we find a closing quote, we add our dialog to the\n",
    "                    # appropriate list, clear current and flip our found_q\n",
    "                    # variable to False\n",
    "                    current.append(tokenized[counter])\n",
    "                    parsed_dialog.append(current)\n",
    "                    current = []\n",
    "                    found_q = False\n",
    "\n",
    "        # increment the counter to move us through the text\n",
    "        counter += 1\n",
    "    \n",
    "    if len(parsed_narrative) == 0:\n",
    "        parsed_narrative.append(current)\n",
    "    \n",
    "    mean_dialog_word_len = 0\n",
    "    \n",
    "    if len(parsed_dialog) > 0:\n",
    "        for text in parsed_dialog:\n",
    "            join_text = \" \".join(text)\n",
    "            join_text = join_text.replace('\"','')\n",
    "            join_text = join_text.replace(\"''\",\"\")\n",
    "            mean_dialog_word_len += len(join_text.split())\n",
    "        \n",
    "        mean_dialog_word_len /= float(len(parsed_dialog))\n",
    "    \n",
    "    mean_narrative_word_len = 0\n",
    "    \n",
    "    if len(parsed_narrative) > 0:\n",
    "        for text in parsed_narrative:\n",
    "            join_text = \" \".join(text)\n",
    "            join_text = join_text.replace('\"','')\n",
    "            join_text = join_text.replace(\"''\",\"\")\n",
    "            mean_narrative_word_len += len(join_text.split())\n",
    "        \n",
    "        mean_narrative_word_len /= float(len(parsed_narrative))\n",
    "\n",
    "    return len(parsed_dialog), len(parsed_narrative), mean_dialog_word_len, mean_narrative_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:22:16.12828Z",
     "iopub.status.busy": "2021-06-15T10:22:16.127903Z",
     "iopub.status.idle": "2021-06-15T10:22:16.140052Z",
     "shell.execute_reply": "2021-06-15T10:22:16.138918Z",
     "shell.execute_reply.started": "2021-06-15T10:22:16.128247Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_df = train_df.append(test_df, sort=False, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:22:16.142959Z",
     "iopub.status.busy": "2021-06-15T10:22:16.1426Z",
     "iopub.status.idle": "2021-06-15T10:23:47.575198Z",
     "shell.execute_reply": "2021-06-15T10:23:47.573228Z",
     "shell.execute_reply.started": "2021-06-15T10:22:16.142925Z"
    }
   },
   "outputs": [],
   "source": [
    "punct = []\n",
    "punct += list(string.punctuation)\n",
    "punct += '’'\n",
    "punct += '-'\n",
    "punct += ','\n",
    "punct += '.'\n",
    "punct += '?'\n",
    "punct += '!'\n",
    "\n",
    "\n",
    "combined_df[\"excerpt_num_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\n",
    "combined_df[\"excerpt_num_unique_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\n",
    "combined_df[\"excerpt_num_chars\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x)))\n",
    "combined_df[\"excerpt_num_stopwords\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in set(stopwords.words('english'))]))\n",
    "combined_df[\"excerpt_num_punctuations\"] =combined_df['excerpt'].apply(lambda x: len([c for c in str(x) if c in punct]))\n",
    "combined_df[\"excerpt_num_words_upper\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "combined_df[\"excerpt_num_words_title\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "combined_df[\"excerpt_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "combined_df[\"excerpt_num_dialog\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[0])\n",
    "combined_df[\"excerpt_num_narrative\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[1])\n",
    "combined_df[\"excerpt_dialog_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[2])\n",
    "combined_df[\"excerpt_narrative_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[3])\n",
    "combined_df['excerpt_polarity'] = combined_df['excerpt'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "combined_df['excerpt_subjectivity'] = combined_df['excerpt'].apply(lambda x: TextBlob(x).sentiment[1])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:23:47.581506Z",
     "iopub.status.busy": "2021-06-15T10:23:47.579441Z",
     "iopub.status.idle": "2021-06-15T10:24:13.471613Z",
     "shell.execute_reply": "2021-06-15T10:24:13.470771Z",
     "shell.execute_reply.started": "2021-06-15T10:23:47.581465Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to lower case\n",
    "combined_df['Processed_excerpt'] = combined_df['excerpt'].apply(lambda x: str(x).lower().replace('\\\\', '').replace('_', ' '))\n",
    "\n",
    "# Remove double spaces\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda x: re.sub('\\s+',  ' ', x))\n",
    "\n",
    "# Replace contractions (\"don't\" with \"do not\" and \"we've\" with \"we have\")\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda x: decontraction(x))\n",
    "\n",
    "# Remove punctuations\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(remove_punctuations)\n",
    "\n",
    "# Lemmatize words\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "del train_df\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:24:13.473219Z",
     "iopub.status.busy": "2021-06-15T10:24:13.47288Z",
     "iopub.status.idle": "2021-06-15T10:24:13.795652Z",
     "shell.execute_reply": "2021-06-15T10:24:13.794693Z",
     "shell.execute_reply.started": "2021-06-15T10:24:13.473185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rare-words removal\n",
    "def remove_rarewords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "\n",
    "cnt = Counter()\n",
    "for text in combined_df['Processed_excerpt'].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "\n",
    "n_rare_words = 50\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda text: remove_rarewords(text))\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:24:13.797471Z",
     "iopub.status.busy": "2021-06-15T10:24:13.7971Z",
     "iopub.status.idle": "2021-06-15T10:24:13.960953Z",
     "shell.execute_reply": "2021-06-15T10:24:13.960018Z",
     "shell.execute_reply.started": "2021-06-15T10:24:13.797435Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removal of Emojis\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda text: remove_emoji(text))\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:24:13.962906Z",
     "iopub.status.busy": "2021-06-15T10:24:13.96253Z",
     "iopub.status.idle": "2021-06-15T10:24:13.970173Z",
     "shell.execute_reply": "2021-06-15T10:24:13.969203Z",
     "shell.execute_reply.started": "2021-06-15T10:24:13.962869Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"\\nOriginal Excerpt: \\n{combined_df.iloc[i]['excerpt']} \\n\\nProcessed Excerpt: \\n{combined_df.iloc[i]['Processed_excerpt']}\\n\")\n",
    "    print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:24:13.972434Z",
     "iopub.status.busy": "2021-06-15T10:24:13.971753Z",
     "iopub.status.idle": "2021-06-15T10:24:14.187121Z",
     "shell.execute_reply": "2021-06-15T10:24:14.186065Z",
     "shell.execute_reply.started": "2021-06-15T10:24:13.972395Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = combined_df[:Ytrain.shape[0]].copy()\n",
    "test_df = combined_df[Ytrain.shape[0]:].copy()\n",
    "print(f\"train_df: {train_df.shape} \\ntest_df: {test_df.shape}\")\n",
    "\n",
    "del combined_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:24:14.188862Z",
     "iopub.status.busy": "2021-06-15T10:24:14.188488Z",
     "iopub.status.idle": "2021-06-15T10:24:14.20198Z",
     "shell.execute_reply": "2021-06-15T10:24:14.200927Z",
     "shell.execute_reply.started": "2021-06-15T10:24:14.188826Z"
    }
   },
   "outputs": [],
   "source": [
    "col_list = [col for col in train_df.columns if col not in ['excerpt','Processed_excerpt']]\n",
    "Xtrain = train_df[col_list].values\n",
    "Xtest = test_df[col_list].values\n",
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:24:14.20457Z",
     "iopub.status.busy": "2021-06-15T10:24:14.20422Z",
     "iopub.status.idle": "2021-06-15T10:24:14.210702Z",
     "shell.execute_reply": "2021-06-15T10:24:14.209842Z",
     "shell.execute_reply.started": "2021-06-15T10:24:14.204536Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract excerpts from train and test datasets\n",
    "text_list = pd.concat([train_df['Processed_excerpt'], \n",
    "                       test_df['Processed_excerpt']])\n",
    "print(f\"Total number of excerpts: {len(text_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate words tokens and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:25:27.121735Z",
     "iopub.status.busy": "2021-06-15T10:25:27.121369Z",
     "iopub.status.idle": "2021-06-15T10:25:27.330611Z",
     "shell.execute_reply": "2021-06-15T10:25:27.329716Z",
     "shell.execute_reply.started": "2021-06-15T10:25:27.121702Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = max(train_df['excerpt_num_words'].max(), test_df['excerpt_num_words'].max()) + 8\n",
    "\n",
    "del train_df\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:25:27.740999Z",
     "iopub.status.busy": "2021-06-15T10:25:27.740688Z",
     "iopub.status.idle": "2021-06-15T10:25:27.746937Z",
     "shell.execute_reply": "2021-06-15T10:25:27.745988Z",
     "shell.execute_reply.started": "2021-06-15T10:25:27.740971Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_encode(texts, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode_plus(text, max_length=MAX_LEN, truncation=True, \n",
    "                                       padding='max_length', add_special_tokens=True, \n",
    "                                       return_attention_mask=True, return_token_type_ids=True, \n",
    "                                       return_tensors='tf')\n",
    "        \n",
    "        input_ids.append(tokens['input_ids'])\n",
    "        attention_mask.append(tokens['attention_mask'])\n",
    "        token_type_ids.append(tokens['token_type_ids'])\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_mask), np.array(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:25:28.61216Z",
     "iopub.status.busy": "2021-06-15T10:25:28.611727Z",
     "iopub.status.idle": "2021-06-15T10:25:45.386108Z",
     "shell.execute_reply": "2021-06-15T10:25:45.38461Z",
     "shell.execute_reply.started": "2021-06-15T10:25:28.612098Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased\")\n",
    "input_id, attention_mask, token_type_id = sent_encode(text_list, tokenizer)\n",
    "\n",
    "input_id = input_id.reshape((input_id.shape[0], input_id.shape[2]))\n",
    "attention_mask = attention_mask.reshape((attention_mask.shape[0], attention_mask.shape[2]))\n",
    "token_type_id = token_type_id.reshape((token_type_id.shape[0], token_type_id.shape[2]))\n",
    "\n",
    "input_id.shape, attention_mask.shape, token_type_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:25:45.387848Z",
     "iopub.status.busy": "2021-06-15T10:25:45.38749Z",
     "iopub.status.idle": "2021-06-15T10:25:45.394576Z",
     "shell.execute_reply": "2021-06-15T10:25:45.393502Z",
     "shell.execute_reply.started": "2021-06-15T10:25:45.387812Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain_id, Xtest_id = input_id[:Ytrain.shape[0]], input_id[Ytrain.shape[0]:]\n",
    "Xtrain_mask, Xtest_mask = attention_mask[:Ytrain.shape[0]], attention_mask[Ytrain.shape[0]:]\n",
    "Xtrain_token, Xtest_token = token_type_id[:Ytrain.shape[0]], token_type_id[Ytrain.shape[0]:]\n",
    "\n",
    "print(f\"Xtrain_id: {Xtrain_id.shape} \\nXtrain_mask: {Xtrain_mask.shape} \\nXtrain_token: {Xtrain_token.shape}\\n\")\n",
    "print(f\"Xtest_id: {Xtest_id.shape} \\nXtest_mask: {Xtest_mask.shape} \\nXtest_token: {Xtest_token.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T10:26:52.411696Z",
     "iopub.status.busy": "2021-06-15T10:26:52.411375Z",
     "iopub.status.idle": "2021-06-15T10:26:52.41647Z",
     "shell.execute_reply": "2021-06-15T10:26:52.415431Z",
     "shell.execute_reply.started": "2021-06-15T10:26:52.411669Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T11:04:32.530746Z",
     "iopub.status.busy": "2021-06-15T11:04:32.530421Z",
     "iopub.status.idle": "2021-06-15T11:04:32.547968Z",
     "shell.execute_reply": "2021-06-15T11:04:32.547039Z",
     "shell.execute_reply.started": "2021-06-15T11:04:32.530718Z"
    }
   },
   "outputs": [],
   "source": [
    "def bert_model(transformer_model, n_features):\n",
    "    \n",
    "    input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    token_type_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "    x_input = Input(shape=(n_features,), name=\"statistical_features\")\n",
    "\n",
    "    embed = transformer_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    x1 = Bidirectional(LSTM(units=192, activation='tanh',\n",
    "                            return_sequences=True, dropout=0.15,\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer='he_uniform'), \n",
    "                       merge_mode='concat')(embed)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    \n",
    "    x1 = Conv1D(filters=128, kernel_size=3, \n",
    "                strides=2, padding='same', \n",
    "                kernel_regularizer=l2(0.0001),\n",
    "                kernel_initializer='he_uniform')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation('swish')(x1)\n",
    "    x1 = SpatialDropout1D(rate=0.2)(x1)\n",
    "    \n",
    "    x2 = Bidirectional(GRU(units=192, activation='tanh',\n",
    "                           return_sequences=True, dropout=0.15,\n",
    "                           kernel_regularizer=l2(0.0001),\n",
    "                           kernel_initializer='he_uniform'), \n",
    "                       merge_mode='concat')(embed)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    \n",
    "    x2 = Conv1D(filters=128, kernel_size=3, \n",
    "                strides=2, padding='same', \n",
    "                kernel_regularizer=l2(0.0001),\n",
    "                kernel_initializer='he_uniform')(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Activation('swish')(x2)\n",
    "    x2 = SpatialDropout1D(rate=0.2)(x2)\n",
    "    \n",
    "    x3 = Dense(units=32, kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(0.0001))(x_input)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = Activation('swish')(x3)\n",
    "    \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    \n",
    "    x = Concatenate()([avg_pool1, avg_pool2, x3])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.15)(x)\n",
    "    \n",
    "    x = Dense(units=64, kernel_initializer='he_uniform', \n",
    "              kernel_regularizer=l2(0.0001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "    x = Dropout(rate=0.35)(x)\n",
    "    \n",
    "    x = Dense(units=16, kernel_initializer='he_uniform', \n",
    "              kernel_regularizer=l2(0.0001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "    x = Dropout(rate=0.15)(x)\n",
    "    \n",
    "    x = Dense(units=1, kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    model = Model(inputs=[x_input, input_ids, attention_mask, token_type_ids], outputs=x, \n",
    "                  name='CommonLit_Bert_Base_Uncased_Model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T11:04:33.094955Z",
     "iopub.status.busy": "2021-06-15T11:04:33.094698Z",
     "iopub.status.idle": "2021-06-15T11:04:33.101002Z",
     "shell.execute_reply": "2021-06-15T11:04:33.10022Z",
     "shell.execute_reply.started": "2021-06-15T11:04:33.094929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "FOLD = 5\n",
    "NUM_SEED = 2\n",
    "VERBOSE = 1\n",
    "MINI_BATCH_SIZE = 32\n",
    "\n",
    "min_lr = 1e-6\n",
    "max_lr = 1e-2\n",
    "step_size = 4 * (Xtrain.shape[0] // MINI_BATCH_SIZE)\n",
    "clr_method = 'triangular2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    hidden_dropout_prob=0.2,\n",
    "    attention_probs_dropout_prob=0.2\n",
    ")\n",
    "config.output_hidden_states = False\n",
    "\n",
    "transformer_model = TFBertModel.from_pretrained(\"../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased\", \n",
    "                                                config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T11:04:35.54321Z",
     "iopub.status.busy": "2021-06-15T11:04:35.542864Z",
     "iopub.status.idle": "2021-06-15T11:04:38.216318Z",
     "shell.execute_reply": "2021-06-15T11:04:38.21553Z",
     "shell.execute_reply.started": "2021-06-15T11:04:35.543179Z"
    }
   },
   "outputs": [],
   "source": [
    "model = bert_model(transformer_model, Xtrain.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='./CommonLit_Bert_Base_Uncased_Model.png', \n",
    "    show_shapes=True, show_layer_names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T11:10:33.888097Z",
     "iopub.status.busy": "2021-06-15T11:10:33.887775Z",
     "iopub.status.idle": "2021-06-15T11:11:36.226195Z",
     "shell.execute_reply": "2021-06-15T11:11:36.223759Z",
     "shell.execute_reply.started": "2021-06-15T11:10:33.888062Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "seeds = np.random.randint(0, 100, size=NUM_SEED)\n",
    "\n",
    "counter = 0\n",
    "oof_score = 0\n",
    "y_pred_final = 0\n",
    "\n",
    "\n",
    "for sidx, seed in enumerate(seeds):\n",
    "    seed_score = 0\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain_strat)):\n",
    "        counter += 1\n",
    "\n",
    "        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n",
    "        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n",
    "        train_x2, val_x2 = Xtrain[train], Xtrain[val]\n",
    "        train_y, val_y = Ytrain[train], Ytrain[val]\n",
    "\n",
    "        model = bert_model(transformer_model, Xtrain.shape[1])\n",
    "        \n",
    "        '''\n",
    "        for layer in model.layers[:4]:\n",
    "            layer.trainable = False\n",
    "        '''\n",
    "        \n",
    "        model.compile(loss=rmse_loss,\n",
    "                      metrics=[RootMeanSquaredError(name='rmse')],\n",
    "                      optimizer=Adamax(lr=8e-5))\n",
    "\n",
    "        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n",
    "                              restore_best_weights=True, \n",
    "                              patience=7, verbose=VERBOSE)\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.25, \n",
    "                                      min_lr=1e-6, patience=3, \n",
    "                                      verbose=VERBOSE, mode='min')\n",
    "\n",
    "        chk_point = ModelCheckpoint('./CommonLit_Bert_Base_Uncased_Model.h5', \n",
    "                                    monitor='val_rmse', verbose=VERBOSE, \n",
    "                                    save_best_only=True, mode='min',\n",
    "                                    save_weights_only=True)\n",
    "        \n",
    "        cyclic_lr = CyclicLR(base_lr=min_lr, max_lr=max_lr, \n",
    "                             mode=clr_method, step_size=step_size)\n",
    "        \n",
    "        history = model.fit(\n",
    "            [train_x2, train_x_id, train_x_mask, train_x_token], train_y, \n",
    "            batch_size=MINI_BATCH_SIZE,\n",
    "            epochs=50, \n",
    "            verbose=VERBOSE, \n",
    "            workers=5,\n",
    "            callbacks=[reduce_lr, early, chk_point], \n",
    "            validation_data=([val_x2, val_x_id, val_x_mask, val_x_token], val_y)\n",
    "        )\n",
    "        \n",
    "        model.load_weights(f'./CommonLit_Bert_Base_Uncased_Model.h5')\n",
    "        \n",
    "        y_pred = model.predict([val_x2, val_x_id, val_x_mask, val_x_token])\n",
    "        y_pred_final += model.predict([Xtest, Xtest_id, Xtest_mask, Xtest_token])\n",
    "        \n",
    "        score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "        oof_score += score\n",
    "        seed_score += score\n",
    "        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n",
    "    \n",
    "    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n",
    "\n",
    "\n",
    "y_pred_final = y_pred_final / float(counter)\n",
    "oof_score /= float(counter)\n",
    "print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n",
    "submit_df['target'] = y_pred_final\n",
    "submit_df.to_csv(\"./submission.csv\", index=False)\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
