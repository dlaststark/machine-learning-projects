{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:30:55.687953Z",
     "iopub.status.busy": "2021-06-06T05:30:55.68755Z",
     "iopub.status.idle": "2021-06-06T05:30:56.736629Z",
     "shell.execute_reply": "2021-06-06T05:30:56.735114Z",
     "shell.execute_reply.started": "2021-06-06T05:30:55.68787Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copytree(\"/kaggle/input/infersent/\", \"/kaggle/working/infersent\")\n",
    "! mv /kaggle/working/infersent/* /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:30:56.739094Z",
     "iopub.status.busy": "2021-06-06T05:30:56.738677Z",
     "iopub.status.idle": "2021-06-06T05:31:02.190934Z",
     "shell.execute_reply": "2021-06-06T05:31:02.189833Z",
     "shell.execute_reply.started": "2021-06-06T05:30:56.739054Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "import lightgbm as lgb\n",
    "from models import InferSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:31:02.193776Z",
     "iopub.status.busy": "2021-06-06T05:31:02.193272Z",
     "iopub.status.idle": "2021-06-06T05:31:02.292041Z",
     "shell.execute_reply": "2021-06-06T05:31:02.290912Z",
     "shell.execute_reply.started": "2021-06-06T05:31:02.193714Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n",
    "train_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\n",
    "train_df.set_index(\"id\", inplace=True)\n",
    "print(f\"train_df: {train_df.shape}\\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:31:02.294336Z",
     "iopub.status.busy": "2021-06-06T05:31:02.293978Z",
     "iopub.status.idle": "2021-06-06T05:31:02.318644Z",
     "shell.execute_reply": "2021-06-06T05:31:02.317429Z",
     "shell.execute_reply.started": "2021-06-06T05:31:02.294307Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "test_df.drop(['url_legal','license'], inplace=True, axis=1)\n",
    "test_df.set_index(\"id\", inplace=True)\n",
    "print(f\"test_df: {test_df.shape}\\n\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:31:02.321067Z",
     "iopub.status.busy": "2021-06-06T05:31:02.320314Z",
     "iopub.status.idle": "2021-06-06T05:31:02.333714Z",
     "shell.execute_reply": "2021-06-06T05:31:02.332011Z",
     "shell.execute_reply.started": "2021-06-06T05:31:02.321019Z"
    }
   },
   "outputs": [],
   "source": [
    "Ytrain = train_df['target'].values\n",
    "Ytrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\n",
    "train_df.drop(['target'], inplace=True, axis=1)\n",
    "print(\"Ytrain: {}\".format(Ytrain.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:31:02.336871Z",
     "iopub.status.busy": "2021-06-06T05:31:02.335924Z",
     "iopub.status.idle": "2021-06-06T05:31:02.350173Z",
     "shell.execute_reply": "2021-06-06T05:31:02.348684Z",
     "shell.execute_reply.started": "2021-06-06T05:31:02.336823Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_df = train_df.append(test_df, sort=False, ignore_index=False)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:31:02.353937Z",
     "iopub.status.busy": "2021-06-06T05:31:02.353081Z",
     "iopub.status.idle": "2021-06-06T05:32:35.638712Z",
     "shell.execute_reply": "2021-06-06T05:32:35.6377Z",
     "shell.execute_reply.started": "2021-06-06T05:31:02.353885Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_df[\"excerpt_num_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\n",
    "combined_df[\"excerpt_num_unique_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\n",
    "combined_df[\"excerpt_num_chars\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x)))\n",
    "combined_df[\"excerpt_num_stopwords\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in set(stopwords.words('english'))]))\n",
    "combined_df[\"excerpt_num_punctuations\"] =combined_df['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "combined_df[\"excerpt_num_words_upper\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "combined_df[\"excerpt_num_words_title\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "combined_df[\"excerpt_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "combined_df['excerpt_polarity'] = combined_df['excerpt'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "combined_df['excerpt_subjectivity'] = combined_df['excerpt'].apply(lambda x: TextBlob(x).sentiment[1])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:32:35.642543Z",
     "iopub.status.busy": "2021-06-06T05:32:35.642227Z",
     "iopub.status.idle": "2021-06-06T05:32:40.142586Z",
     "shell.execute_reply": "2021-06-06T05:32:40.14145Z",
     "shell.execute_reply.started": "2021-06-06T05:32:35.642512Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,5), \n",
    "                        analyzer='word', max_df=0.95, min_df=3, \n",
    "                        use_idf=1, sublinear_tf=1, \n",
    "                        max_features=500, strip_accents='ascii')\n",
    "features = tfidf.fit_transform(combined_df.excerpt).toarray()\n",
    "features_df = pd.DataFrame(features, \n",
    "                           columns=tfidf.get_feature_names(), \n",
    "                           index=combined_df.index)\n",
    "combined_df = pd.merge(combined_df, \n",
    "                       features_df, \n",
    "                       left_index=True, \n",
    "                       right_index=True)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:32:40.14535Z",
     "iopub.status.busy": "2021-06-06T05:32:40.144923Z",
     "iopub.status.idle": "2021-06-06T05:32:45.110852Z",
     "shell.execute_reply": "2021-06-06T05:32:45.109748Z",
     "shell.execute_reply.started": "2021-06-06T05:32:40.145306Z"
    }
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(stop_words='english', ngram_range=(1,5), \n",
    "                           analyzer='word', max_df=0.95, min_df=3, \n",
    "                           max_features=200, strip_accents='ascii')\n",
    "features = countvec.fit_transform(combined_df.excerpt).toarray()\n",
    "features_df = pd.DataFrame(features, \n",
    "                           columns=countvec.get_feature_names(), \n",
    "                           index=combined_df.index)\n",
    "combined_df = pd.merge(combined_df, \n",
    "                       features_df, \n",
    "                       left_index=True, \n",
    "                       right_index=True)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:32:45.114839Z",
     "iopub.status.busy": "2021-06-06T05:32:45.114531Z",
     "iopub.status.idle": "2021-06-06T05:32:48.084036Z",
     "shell.execute_reply": "2021-06-06T05:32:48.082914Z",
     "shell.execute_reply.started": "2021-06-06T05:32:45.114807Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    punct =[]\n",
    "    punct += list(string.punctuation)\n",
    "    punct += 'â€™'\n",
    "    punct += '-'\n",
    "    punct.remove(\"'\")\n",
    "    \n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\n",
    "    \"N\": wordnet.NOUN, \n",
    "    \"V\": wordnet.VERB, \n",
    "    \"J\": wordnet.ADJ, \n",
    "    \"R\": wordnet.ADV\n",
    "}\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:32:48.086186Z",
     "iopub.status.busy": "2021-06-06T05:32:48.085747Z",
     "iopub.status.idle": "2021-06-06T05:33:08.613864Z",
     "shell.execute_reply": "2021-06-06T05:33:08.612788Z",
     "shell.execute_reply.started": "2021-06-06T05:32:48.086139Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_df[\"Processed_excerpt\"] = combined_df[\"excerpt\"].str.lower()\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(remove_punctuations)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda text: \" \".join([word for word in str(text).split() if word not in stop_words]))\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:33:08.615954Z",
     "iopub.status.busy": "2021-06-06T05:33:08.615514Z",
     "iopub.status.idle": "2021-06-06T05:33:08.628774Z",
     "shell.execute_reply": "2021-06-06T05:33:08.627556Z",
     "shell.execute_reply.started": "2021-06-06T05:33:08.61588Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Original Excerpt: \\n{combined_df.iloc[i]['excerpt']} \\n\\nProcessed Excerpt: \\n{combined_df.iloc[i]['Processed_excerpt']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:33:08.631532Z",
     "iopub.status.busy": "2021-06-06T05:33:08.630646Z",
     "iopub.status.idle": "2021-06-06T05:33:38.967571Z",
     "shell.execute_reply": "2021-06-06T05:33:38.966542Z",
     "shell.execute_reply.started": "2021-06-06T05:33:08.631486Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../input/gloveembeddings/Glove_840B_300d_Embeddings.txt\", 'rb') as handle: \n",
    "    data = handle.read()\n",
    "\n",
    "processed_data = pickle.loads(data)\n",
    "embeddings_index = processed_data['embeddings_index']\n",
    "print('Word vectors found: {}'.format(len(embeddings_index)))\n",
    "\n",
    "del processed_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:33:38.971156Z",
     "iopub.status.busy": "2021-06-06T05:33:38.970694Z",
     "iopub.status.idle": "2021-06-06T05:33:38.979545Z",
     "shell.execute_reply": "2021-06-06T05:33:38.978045Z",
     "shell.execute_reply.started": "2021-06-06T05:33:38.971109Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent2vec(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    \n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:33:38.982374Z",
     "iopub.status.busy": "2021-06-06T05:33:38.981906Z",
     "iopub.status.idle": "2021-06-06T05:33:42.366799Z",
     "shell.execute_reply": "2021-06-06T05:33:42.365455Z",
     "shell.execute_reply.started": "2021-06-06T05:33:38.982326Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_vec = [sent2vec(x) for x in tqdm(combined_df[\"Processed_excerpt\"].values)]\n",
    "col_list = ['glove_'+str(i) for i in range(300)]\n",
    "glove_vec_df = pd.DataFrame(np.array(glove_vec), columns=col_list, index=combined_df.index)\n",
    "print(f\"glove_vec_df: {glove_vec_df.shape}\\n\")\n",
    "glove_vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:33:42.369115Z",
     "iopub.status.busy": "2021-06-06T05:33:42.368682Z",
     "iopub.status.idle": "2021-06-06T05:33:42.430812Z",
     "shell.execute_reply": "2021-06-06T05:33:42.429338Z",
     "shell.execute_reply.started": "2021-06-06T05:33:42.369069Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_df = pd.merge(combined_df, glove_vec_df, how=\"inner\", on=\"id\", sort=False)\n",
    "print(f\"combined_df: {combined_df.shape}\\n\")\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InferSent Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:39:44.768419Z",
     "iopub.status.busy": "2021-06-06T05:39:44.768013Z",
     "iopub.status.idle": "2021-06-06T05:39:51.369653Z",
     "shell.execute_reply": "2021-06-06T05:39:51.368513Z",
     "shell.execute_reply.started": "2021-06-06T05:39:44.76836Z"
    }
   },
   "outputs": [],
   "source": [
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
    "\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(\"../input/infersent2/encoder/infersent2.pkl\"))\n",
    "model = model.cuda()\n",
    "model.set_w2v_path(\"../input/glove840b300dtxt/glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:40:14.548896Z",
     "iopub.status.busy": "2021-06-06T05:40:14.548508Z",
     "iopub.status.idle": "2021-06-06T05:41:21.880325Z",
     "shell.execute_reply": "2021-06-06T05:41:21.878316Z",
     "shell.execute_reply.started": "2021-06-06T05:40:14.548863Z"
    }
   },
   "outputs": [],
   "source": [
    "model.build_vocab(combined_df['Processed_excerpt'].values.tolist(), tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:41:56.872986Z",
     "iopub.status.busy": "2021-06-06T05:41:56.8726Z",
     "iopub.status.idle": "2021-06-06T05:42:03.488192Z",
     "shell.execute_reply": "2021-06-06T05:42:03.486901Z",
     "shell.execute_reply.started": "2021-06-06T05:41:56.872954Z"
    }
   },
   "outputs": [],
   "source": [
    "infersent_vec = model.encode(combined_df['Processed_excerpt'].values.tolist(), \n",
    "                             bsize=128, tokenize=True, verbose=True)\n",
    "infersent_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:42:31.008042Z",
     "iopub.status.busy": "2021-06-06T05:42:31.007649Z",
     "iopub.status.idle": "2021-06-06T05:42:31.13457Z",
     "shell.execute_reply": "2021-06-06T05:42:31.13323Z",
     "shell.execute_reply.started": "2021-06-06T05:42:31.00801Z"
    }
   },
   "outputs": [],
   "source": [
    "col_list = ['infersent_'+str(i) for i in range(infersent_vec.shape[1])]\n",
    "infersent_df = pd.DataFrame(np.array(infersent_vec), columns=col_list, index=combined_df.index)\n",
    "print(f\"infersent_df: {infersent_df.shape}\\n\")\n",
    "infersent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:42:35.517973Z",
     "iopub.status.busy": "2021-06-06T05:42:35.517597Z",
     "iopub.status.idle": "2021-06-06T05:42:35.88395Z",
     "shell.execute_reply": "2021-06-06T05:42:35.882895Z",
     "shell.execute_reply.started": "2021-06-06T05:42:35.51794Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_df = pd.merge(combined_df, infersent_df, how=\"inner\", on=\"id\", sort=False)\n",
    "combined_df.drop(['excerpt','Processed_excerpt'], inplace=True, axis=1)\n",
    "print(f\"combined_df: {combined_df.shape}\\n\")\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:42:37.217809Z",
     "iopub.status.busy": "2021-06-06T05:42:37.217376Z",
     "iopub.status.idle": "2021-06-06T05:42:37.250017Z",
     "shell.execute_reply": "2021-06-06T05:42:37.248411Z",
     "shell.execute_reply.started": "2021-06-06T05:42:37.217775Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain = combined_df[:Ytrain.shape[0]].copy()\n",
    "Xtest = combined_df[Ytrain.shape[0]:].copy()\n",
    "print(f\"Xtrain: {Xtrain.shape} \\nXtest: {Xtest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:42:39.62392Z",
     "iopub.status.busy": "2021-06-06T05:42:39.623532Z",
     "iopub.status.idle": "2021-06-06T05:43:33.03726Z",
     "shell.execute_reply": "2021-06-06T05:43:33.035935Z",
     "shell.execute_reply.started": "2021-06-06T05:42:39.623872Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in tqdm(Xtrain.columns):\n",
    "    transformer = QuantileTransformer(n_quantiles=1000, \n",
    "                                      random_state=10, \n",
    "                                      output_distribution=\"normal\")\n",
    "    \n",
    "    vec_len = len(Xtrain[col].values)\n",
    "    vec_len_test = len(Xtest[col].values)\n",
    "\n",
    "    raw_vec = Xtrain[col].values.reshape(vec_len, 1)\n",
    "    test_vec = Xtest[col].values.reshape(vec_len_test, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "    \n",
    "    Xtrain[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    Xtest[col] = transformer.transform(test_vec).reshape(1, vec_len_test)[0]\n",
    "\n",
    "print(\"Xtrain: {} \\nYtrain: {} \\nXtest: {}\".format(Xtrain.shape, Ytrain.shape, Xtest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-06T05:43:38.463086Z",
     "iopub.status.busy": "2021-06-06T05:43:38.462689Z",
     "iopub.status.idle": "2021-06-06T05:48:18.129188Z",
     "shell.execute_reply": "2021-06-06T05:48:18.12706Z",
     "shell.execute_reply.started": "2021-06-06T05:43:38.463055Z"
    }
   },
   "outputs": [],
   "source": [
    "FOLD = 5\n",
    "NUM_SEED = 2\n",
    "COUNTER = 0\n",
    "\n",
    "np.random.seed(2021)\n",
    "seeds = np.random.randint(0, 2021, size=NUM_SEED)\n",
    "\n",
    "oof_score_lgb = 0\n",
    "y_pred_final_lgb = 0\n",
    "y_pred_meta_lgb = np.zeros((Ytrain.shape[0], 1))\n",
    "\n",
    "\n",
    "for sidx, seed in enumerate(seeds):\n",
    "    seed_score_lgb = 0\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain_strat)):\n",
    "        COUNTER += 1\n",
    "\n",
    "        train_x, train_y = Xtrain.iloc[train], Ytrain[train]\n",
    "        val_x, val_y = Xtrain.iloc[val], Ytrain[val]\n",
    "        \n",
    "        #====================================================================\n",
    "        #                                LightGBM\n",
    "        #====================================================================\n",
    "        \n",
    "        params = {}\n",
    "        params[\"objective\"] = 'regression'\n",
    "        params[\"metric\"] = 'rmse'\n",
    "        params[\"boosting\"] = 'gbdt'\n",
    "        params[\"device\"] = 'gpu'\n",
    "        params[\"learning_rate\"] = 0.0204\n",
    "        params[\"lambda_l2\"] = 0.225\n",
    "        params[\"num_leaves\"] = 52\n",
    "        params[\"max_depth\"] = 6\n",
    "        params[\"feature_fraction\"] = 0.75\n",
    "        params[\"bagging_fraction\"] = 0.65\n",
    "        params[\"bagging_freq\"] = 10\n",
    "        params[\"min_data_in_leaf\"] = 15\n",
    "        params[\"verbosity\"] = -1\n",
    "        num_rounds = 5000\n",
    "        \n",
    "        lgtrain = lgb.Dataset(train_x, label=train_y.ravel())\n",
    "        lgvalidation = lgb.Dataset(val_x, label=val_y.ravel())\n",
    "\n",
    "        lgb_model = lgb.train(params, lgtrain, num_rounds, \n",
    "                              valid_sets=[lgtrain, lgvalidation], \n",
    "                              early_stopping_rounds=100, verbose_eval=200)\n",
    "\n",
    "        y_pred = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n",
    "        y_pred_meta_lgb[val] = np.array([y_pred]).T\n",
    "        y_pred_final_lgb += lgb_model.predict(Xtest, num_iteration=lgb_model.best_iteration)\n",
    "        \n",
    "        score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "        oof_score_lgb += score\n",
    "        seed_score_lgb += score\n",
    "        print(f\"\\nLightGBM | Seed-{seed} | Fold-{idx+1} | OOF Score: {score}\\n\")\n",
    "        \n",
    "    print(f\"\\nLightGBM | Seed: {seed} | Aggregate OOF Score: {(seed_score_lgb / FOLD)}\\n\")\n",
    "\n",
    "\n",
    "y_pred_meta_lgb = y_pred_meta_lgb / float(NUM_SEED)\n",
    "y_pred_final_lgb = y_pred_final_lgb / float(COUNTER)\n",
    "oof_score_lgb /= float(COUNTER)\n",
    "print(f\"LightGBM | Aggregate OOF Score: {oof_score_lgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n",
    "submit_df['target'] = y_pred_final_lgb\n",
    "submit_df.to_csv(\"./submission.csv\", index=False)\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
