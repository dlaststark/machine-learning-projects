{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:28.349067Z",
     "iopub.status.busy": "2021-06-05T06:41:28.348721Z",
     "iopub.status.idle": "2021-06-05T06:41:28.357977Z",
     "shell.execute_reply": "2021-06-05T06:41:28.356769Z",
     "shell.execute_reply.started": "2021-06-05T06:41:28.349036Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow_addons.optimizers import AdamW, Lookahead\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Activation, Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:28.843488Z",
     "iopub.status.busy": "2021-06-05T06:41:28.843058Z",
     "iopub.status.idle": "2021-06-05T06:41:28.901204Z",
     "shell.execute_reply": "2021-06-05T06:41:28.900389Z",
     "shell.execute_reply.started": "2021-06-05T06:41:28.843451Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n",
    "train_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\n",
    "train_df.set_index(\"id\", inplace=True)\n",
    "print(f\"train_df: {train_df.shape}\\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:29.038196Z",
     "iopub.status.busy": "2021-06-05T06:41:29.037811Z",
     "iopub.status.idle": "2021-06-05T06:41:29.05558Z",
     "shell.execute_reply": "2021-06-05T06:41:29.054475Z",
     "shell.execute_reply.started": "2021-06-05T06:41:29.038156Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "test_df.drop(['url_legal','license'], inplace=True, axis=1)\n",
    "test_df.set_index(\"id\", inplace=True)\n",
    "print(f\"test_df: {test_df.shape}\\n\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:29.50829Z",
     "iopub.status.busy": "2021-06-05T06:41:29.507877Z",
     "iopub.status.idle": "2021-06-05T06:41:29.805945Z",
     "shell.execute_reply": "2021-06-05T06:41:29.804792Z",
     "shell.execute_reply.started": "2021-06-05T06:41:29.508218Z"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.boxplot(x='target', data=train_df, ax=ax[0])\n",
    "sns.histplot(x='target', data=train_df, ax=ax[1])\n",
    "ax[0].title.set_text('Box Plot - target')\n",
    "ax[1].title.set_text('Hist Plot - target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:29.807864Z",
     "iopub.status.busy": "2021-06-05T06:41:29.807557Z",
     "iopub.status.idle": "2021-06-05T06:41:29.817521Z",
     "shell.execute_reply": "2021-06-05T06:41:29.815476Z",
     "shell.execute_reply.started": "2021-06-05T06:41:29.807835Z"
    }
   },
   "outputs": [],
   "source": [
    "Ytrain = train_df['target'].values\n",
    "Ytrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\n",
    "train_df.drop(['target'], inplace=True, axis=1)\n",
    "print(\"Ytrain: {}\".format(Ytrain.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:30.155458Z",
     "iopub.status.busy": "2021-06-05T06:41:30.155102Z",
     "iopub.status.idle": "2021-06-05T06:41:30.164492Z",
     "shell.execute_reply": "2021-06-05T06:41:30.163443Z",
     "shell.execute_reply.started": "2021-06-05T06:41:30.155429Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    punct =[]\n",
    "    punct += list(string.punctuation)\n",
    "    punct += 'â€™'\n",
    "    punct += '-'\n",
    "    punct.remove(\"'\")\n",
    "    \n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_excerpt(df):\n",
    "    df['excerpt'] = df['excerpt'].apply(lambda x: x.lower())\n",
    "    df['excerpt'] = df['excerpt'].apply(lambda x: x.replace('\\n', ' '))\n",
    "    df['excerpt'] = df['excerpt'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    df['excerpt'] = df['excerpt'].apply(lambda x: x.replace('&gt;', ''))\n",
    "    df['excerpt'] = df['excerpt'].apply(remove_punctuations)\n",
    "    df['excerpt'] = df['excerpt'].apply(lambda x: str(x).replace(\" s \", \" \"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:30.3483Z",
     "iopub.status.busy": "2021-06-05T06:41:30.347937Z",
     "iopub.status.idle": "2021-06-05T06:41:30.589921Z",
     "shell.execute_reply": "2021-06-05T06:41:30.588796Z",
     "shell.execute_reply.started": "2021-06-05T06:41:30.348268Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = process_excerpt(train_df)\n",
    "test_df = process_excerpt(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TPU config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:30.978314Z",
     "iopub.status.busy": "2021-06-05T06:41:30.977933Z",
     "iopub.status.idle": "2021-06-05T06:41:36.276237Z",
     "shell.execute_reply": "2021-06-05T06:41:36.274977Z",
     "shell.execute_reply.started": "2021-06-05T06:41:30.978283Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:36.278306Z",
     "iopub.status.busy": "2021-06-05T06:41:36.277997Z",
     "iopub.status.idle": "2021-06-05T06:41:36.284499Z",
     "shell.execute_reply": "2021-06-05T06:41:36.283284Z",
     "shell.execute_reply.started": "2021-06-05T06:41:36.278278Z"
    }
   },
   "outputs": [],
   "source": [
    "mini_batch_size = strategy.num_replicas_in_sync * 16\n",
    "print(f'batch size: {mini_batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:36.286897Z",
     "iopub.status.busy": "2021-06-05T06:41:36.286567Z",
     "iopub.status.idle": "2021-06-05T06:41:36.295357Z",
     "shell.execute_reply": "2021-06-05T06:41:36.294444Z",
     "shell.execute_reply.started": "2021-06-05T06:41:36.286861Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_encode(texts, tokenizer, max_len=512):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode_plus(text, max_length=max_len, truncation=True, \n",
    "                                       padding='max_length', add_special_tokens=True, \n",
    "                                       return_attention_mask=True, return_token_type_ids=False, \n",
    "                                       return_tensors='tf')\n",
    "        \n",
    "        input_ids.append(tokens['input_ids'])\n",
    "        attention_mask.append(tokens['attention_mask'])\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:41:36.297682Z",
     "iopub.status.busy": "2021-06-05T06:41:36.297002Z",
     "iopub.status.idle": "2021-06-05T06:41:36.309041Z",
     "shell.execute_reply": "2021-06-05T06:41:36.307851Z",
     "shell.execute_reply.started": "2021-06-05T06:41:36.29765Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:47:50.979648Z",
     "iopub.status.busy": "2021-06-05T06:47:50.97922Z",
     "iopub.status.idle": "2021-06-05T06:47:50.992075Z",
     "shell.execute_reply": "2021-06-05T06:47:50.990666Z",
     "shell.execute_reply.started": "2021-06-05T06:47:50.979609Z"
    }
   },
   "outputs": [],
   "source": [
    "def dnn_model(transformer_model):\n",
    "    \n",
    "    input_ids = Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(512,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    embed = transformer_model(input_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    x = embed[:, 0, :]\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(units=512, kernel_initializer='he_uniform', \n",
    "                kernel_regularizer=l2(0.0001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    x = Dense(units=256, kernel_initializer='he_uniform', \n",
    "                kernel_regularizer=l2(0.0001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    x = Dense(units=128, kernel_initializer='he_uniform', \n",
    "                kernel_regularizer=l2(0.0001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    x = Dense(units=1, kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=x, \n",
    "                  name='CommonLit_DNN_Model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:47:53.427513Z",
     "iopub.status.busy": "2021-06-05T06:47:53.427163Z",
     "iopub.status.idle": "2021-06-05T06:47:53.434415Z",
     "shell.execute_reply": "2021-06-05T06:47:53.43295Z",
     "shell.execute_reply.started": "2021-06-05T06:47:53.427482Z"
    }
   },
   "outputs": [],
   "source": [
    "FOLD = 5\n",
    "NUM_SEED = 3\n",
    "VERBOSE = 1\n",
    "\n",
    "np.random.seed(3)\n",
    "seeds = np.random.randint(0, 100, size=NUM_SEED)\n",
    "\n",
    "oof_score = 0\n",
    "y_pred_meta_dnn = np.zeros((Ytrain.shape[0], 1))\n",
    "y_pred_final_dnn = 0\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-05T06:47:57.786088Z",
     "iopub.status.busy": "2021-06-05T06:47:57.78559Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    \n",
    "    transformer_model = TFAutoModel.from_pretrained(\"../input/tfbert-large-uncased\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"../input/tfbert-large-uncased\")\n",
    "    \n",
    "    Xtrain_id, Xtrain_mask = sent_encode(train_df['excerpt'].values, tokenizer)\n",
    "    Xtest_id, Xtest_mask = sent_encode(test_df['excerpt'].values, tokenizer)\n",
    "\n",
    "    Xtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\n",
    "    Xtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n",
    "\n",
    "    Xtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\n",
    "    Xtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\n",
    "\n",
    "    print(f\"Train Data: \\n   Input-ids: {Xtrain_id.shape} \\n   Attention Mask: {Xtrain_mask.shape}\\n\")\n",
    "    print(f\"Test Data: \\n   Input-ids: {Xtest_id.shape} \\n   Attention Mask: {Xtest_mask.shape}\\n\\n\")\n",
    "    \n",
    "\n",
    "    for sidx, seed in enumerate(seeds):\n",
    "        seed_score = 0\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "        for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n",
    "            counter += 1\n",
    "\n",
    "            train_x_id, train_x_mask, train_y = Xtrain_id[train], Xtrain_mask[train], Ytrain[train]\n",
    "            val_x_id, val_x_mask, val_y = Xtrain_id[val], Xtrain_mask[val], Ytrain[val]\n",
    "\n",
    "            model = dnn_model(transformer_model)\n",
    "            \n",
    "            model.layers[2].trainable = False\n",
    "\n",
    "            model.compile(loss=rmse_loss,\n",
    "                          optimizer=Lookahead(AdamW(lr=1e-2, \n",
    "                                                    weight_decay=1e-5, \n",
    "                                                    clipvalue=700), \n",
    "                                              sync_period=10))\n",
    "\n",
    "            early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                  restore_best_weights=True, \n",
    "                                  patience=10, verbose=VERBOSE)\n",
    "\n",
    "            reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, \n",
    "                                          min_lr=1e-6, patience=5, \n",
    "                                          verbose=VERBOSE, mode='min')\n",
    "            \n",
    "            #save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "\n",
    "            chk_point = ModelCheckpoint('./CommonLit_CNN_Model.h5', \n",
    "                                        monitor='val_loss', verbose=VERBOSE, \n",
    "                                        save_best_only=True, mode='min',\n",
    "                                        save_weights_only=True)  #, options=save_locally)\n",
    "\n",
    "            history = model.fit(\n",
    "                [train_x_id, train_x_mask], train_y, \n",
    "                batch_size=mini_batch_size,\n",
    "                epochs=150, \n",
    "                verbose=VERBOSE, \n",
    "                callbacks=[reduce_lr, early, chk_point], \n",
    "                validation_data=([val_x_id, val_x_mask], val_y)\n",
    "            )\n",
    "            \n",
    "            '''\n",
    "            load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "\n",
    "            model = load_model('./CommonLit_CNN_Model.h5', options=load_locally, \n",
    "                               custom_objects={ 'rmse_loss': rmse_loss(y_true=0.0, y_pred=0.0) })\n",
    "            '''\n",
    "\n",
    "            y_pred = model.predict([val_x_id, val_x_mask])\n",
    "            y_pred_meta_dnn[val] += y_pred\n",
    "            y_pred_final_dnn += model.predict([Xtest_id, Xtest_mask])\n",
    "\n",
    "            score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "            oof_score += score\n",
    "            seed_score += score\n",
    "            print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n",
    "\n",
    "        print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_meta_dnn = y_pred_meta_dnn / float(NUM_SEED)\n",
    "y_pred_final_dnn = y_pred_final_dnn / float(counter)\n",
    "oof_score /= float(counter)\n",
    "print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
