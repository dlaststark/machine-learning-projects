{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:45:33.216991Z",
     "iopub.status.busy": "2021-06-07T10:45:33.216633Z",
     "iopub.status.idle": "2021-06-07T10:45:39.432041Z",
     "shell.execute_reply": "2021-06-07T10:45:39.431203Z",
     "shell.execute_reply.started": "2021-06-07T10:45:33.216917Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Activation, Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Add\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load source datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:45:39.433735Z",
     "iopub.status.busy": "2021-06-07T10:45:39.433399Z",
     "iopub.status.idle": "2021-06-07T10:45:39.530191Z",
     "shell.execute_reply": "2021-06-07T10:45:39.529451Z",
     "shell.execute_reply.started": "2021-06-07T10:45:39.433701Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n",
    "train_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\n",
    "train_df.set_index(\"id\", inplace=True)\n",
    "print(f\"train_df: {train_df.shape}\\n\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:45:39.532031Z",
     "iopub.status.busy": "2021-06-07T10:45:39.531791Z",
     "iopub.status.idle": "2021-06-07T10:45:39.549317Z",
     "shell.execute_reply": "2021-06-07T10:45:39.548347Z",
     "shell.execute_reply.started": "2021-06-07T10:45:39.532007Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "test_df.drop(['url_legal','license'], inplace=True, axis=1)\n",
    "test_df.set_index(\"id\", inplace=True)\n",
    "print(f\"test_df: {test_df.shape}\\n\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:45:39.551474Z",
     "iopub.status.busy": "2021-06-07T10:45:39.551133Z",
     "iopub.status.idle": "2021-06-07T10:45:39.561053Z",
     "shell.execute_reply": "2021-06-07T10:45:39.559881Z",
     "shell.execute_reply.started": "2021-06-07T10:45:39.55144Z"
    }
   },
   "outputs": [],
   "source": [
    "Ytrain = train_df['target'].values\n",
    "Ytrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\n",
    "train_df.drop(['target'], inplace=True, axis=1)\n",
    "print(\"Ytrain: {}\".format(Ytrain.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:45:39.563492Z",
     "iopub.status.busy": "2021-06-07T10:45:39.562945Z",
     "iopub.status.idle": "2021-06-07T10:45:39.574585Z",
     "shell.execute_reply": "2021-06-07T10:45:39.573831Z",
     "shell.execute_reply.started": "2021-06-07T10:45:39.563451Z"
    }
   },
   "outputs": [],
   "source": [
    "def decontraction(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    punct = []\n",
    "    punct += list(string.punctuation)\n",
    "    punct += 'â€™'\n",
    "    punct += '-'\n",
    "    punct += ','\n",
    "    punct += '.'\n",
    "    punct += '?'\n",
    "    punct += '!'\n",
    "    punct.remove('\"')\n",
    "    \n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\n",
    "        \"N\": wordnet.NOUN, \n",
    "        \"V\": wordnet.VERB, \n",
    "        \"J\": wordnet.ADJ, \n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:45:39.576062Z",
     "iopub.status.busy": "2021-06-07T10:45:39.575625Z",
     "iopub.status.idle": "2021-06-07T10:45:39.739986Z",
     "shell.execute_reply": "2021-06-07T10:45:39.738899Z",
     "shell.execute_reply.started": "2021-06-07T10:45:39.576022Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_df = train_df.append(test_df, sort=False, ignore_index=False)\n",
    "\n",
    "del train_df\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:45:39.741856Z",
     "iopub.status.busy": "2021-06-07T10:45:39.741371Z",
     "iopub.status.idle": "2021-06-07T10:46:05.448256Z",
     "shell.execute_reply": "2021-06-07T10:46:05.447487Z",
     "shell.execute_reply.started": "2021-06-07T10:45:39.741822Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to lower case\n",
    "combined_df['Processed_excerpt'] = combined_df['excerpt'].apply(lambda x: str(x).lower().replace('\\\\', '').replace('_', ' '))\n",
    "\n",
    "# Remove double spaces\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda x: re.sub('\\s+',  ' ', x))\n",
    "\n",
    "# Replace contractions (\"don't\" with \"do not\" and \"we've\" with \"we have\")\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda x: decontraction(x))\n",
    "\n",
    "# Remove punctuations\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(remove_punctuations)\n",
    "\n",
    "# Lemmatize words\n",
    "combined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "# Word length\n",
    "combined_df['excerpt_wordlen'] = combined_df['Processed_excerpt'].apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:46:05.450671Z",
     "iopub.status.busy": "2021-06-07T10:46:05.450312Z",
     "iopub.status.idle": "2021-06-07T10:46:05.457378Z",
     "shell.execute_reply": "2021-06-07T10:46:05.456437Z",
     "shell.execute_reply.started": "2021-06-07T10:46:05.450635Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"\\nOriginal Excerpt: \\n{combined_df.iloc[i]['excerpt']} \\n\\nProcessed Excerpt: \\n{combined_df.iloc[i]['Processed_excerpt']}\\n\")\n",
    "    print(\"=\"*150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:46:05.45947Z",
     "iopub.status.busy": "2021-06-07T10:46:05.459096Z",
     "iopub.status.idle": "2021-06-07T10:46:05.682174Z",
     "shell.execute_reply": "2021-06-07T10:46:05.681158Z",
     "shell.execute_reply.started": "2021-06-07T10:46:05.459432Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = combined_df[:Ytrain.shape[0]].copy()\n",
    "test_df = combined_df[Ytrain.shape[0]:].copy()\n",
    "print(f\"train_df: {train_df.shape} \\ntest_df: {test_df.shape}\")\n",
    "\n",
    "del combined_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TPU config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T10:46:05.684144Z",
     "iopub.status.busy": "2021-06-07T10:46:05.683733Z",
     "iopub.status.idle": "2021-06-07T10:46:05.697029Z",
     "shell.execute_reply": "2021-06-07T10:46:05.696119Z",
     "shell.execute_reply.started": "2021-06-07T10:46:05.684107Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "    \n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T11:08:35.219111Z",
     "iopub.status.busy": "2021-06-07T11:08:35.218795Z",
     "iopub.status.idle": "2021-06-07T11:08:35.22327Z",
     "shell.execute_reply": "2021-06-07T11:08:35.222472Z",
     "shell.execute_reply.started": "2021-06-07T11:08:35.219083Z"
    }
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 16     #strategy.num_replicas_in_sync * 16\n",
    "print(f'batch size: {mini_batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T11:08:37.538107Z",
     "iopub.status.busy": "2021-06-07T11:08:37.537784Z",
     "iopub.status.idle": "2021-06-07T11:08:37.543971Z",
     "shell.execute_reply": "2021-06-07T11:08:37.543073Z",
     "shell.execute_reply.started": "2021-06-07T11:08:37.538077Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = train_df['excerpt_wordlen'].max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T11:08:37.737627Z",
     "iopub.status.busy": "2021-06-07T11:08:37.737366Z",
     "iopub.status.idle": "2021-06-07T11:08:37.744884Z",
     "shell.execute_reply": "2021-06-07T11:08:37.744175Z",
     "shell.execute_reply.started": "2021-06-07T11:08:37.737603Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_encode(texts, tokenizer, seq_len):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode_plus(text, max_length=seq_len, truncation=True, \n",
    "                                       padding='max_length', add_special_tokens=True, \n",
    "                                       return_attention_mask=True, return_token_type_ids=True, \n",
    "                                       return_tensors='tf')\n",
    "        \n",
    "        input_ids.append(tokens['input_ids'])\n",
    "        attention_mask.append(tokens['attention_mask'])\n",
    "        token_type_ids.append(tokens['token_type_ids'])\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_mask), np.array(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T11:08:38.062754Z",
     "iopub.status.busy": "2021-06-07T11:08:38.062437Z",
     "iopub.status.idle": "2021-06-07T11:08:38.068395Z",
     "shell.execute_reply": "2021-06-07T11:08:38.067518Z",
     "shell.execute_reply.started": "2021-06-07T11:08:38.062727Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T11:45:35.918547Z",
     "iopub.status.busy": "2021-06-07T11:45:35.918224Z",
     "iopub.status.idle": "2021-06-07T11:45:35.92605Z",
     "shell.execute_reply": "2021-06-07T11:45:35.925138Z",
     "shell.execute_reply.started": "2021-06-07T11:45:35.918516Z"
    }
   },
   "outputs": [],
   "source": [
    "def dnn_model(transformer_model, seq_len):\n",
    "    \n",
    "    input_ids = Input(shape=(seq_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(seq_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    token_type_ids = Input(shape=(seq_len,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "\n",
    "    embed = transformer_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(embed)\n",
    "    \n",
    "    x = Dense(units=16, kernel_initializer='he_uniform', \n",
    "              kernel_regularizer=l2(0.0001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=0.15)(x)\n",
    "    \n",
    "    x = Dense(units=1, kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    model = Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=x, \n",
    "                  name='CommonLit_Bert_Base_Uncased_Model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = dnn_model(transformer_model, max_len+16)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T11:45:38.258409Z",
     "iopub.status.busy": "2021-06-07T11:45:38.258095Z",
     "iopub.status.idle": "2021-06-07T11:45:38.263071Z",
     "shell.execute_reply": "2021-06-07T11:45:38.261931Z",
     "shell.execute_reply.started": "2021-06-07T11:45:38.25838Z"
    }
   },
   "outputs": [],
   "source": [
    "FOLD = 5\n",
    "NUM_SEED = 1\n",
    "VERBOSE = 1\n",
    "\n",
    "np.random.seed(3)\n",
    "seeds = np.random.randint(0, 100, size=NUM_SEED)\n",
    "\n",
    "counter = 0\n",
    "oof_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T11:45:38.934625Z",
     "iopub.status.busy": "2021-06-07T11:45:38.934291Z",
     "iopub.status.idle": "2021-06-07T11:49:49.334679Z",
     "shell.execute_reply": "2021-06-07T11:49:49.332414Z",
     "shell.execute_reply.started": "2021-06-07T11:45:38.934593Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    \n",
    "    config = BertConfig()\n",
    "    config.output_hidden_states = False\n",
    "    \n",
    "    transformer_model = TFBertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['Processed_excerpt'].values, tokenizer, seq_len=max_len+16)\n",
    "\n",
    "    Xtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\n",
    "    Xtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n",
    "    Xtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n",
    "    \n",
    "    print(f\"Train Data: \\n   Input-ids: {Xtrain_id.shape} \\n   Attention Mask: {Xtrain_mask.shape}\\n   Token-type-ids: {Xtrain_token.shape}\\n\")\n",
    "    \n",
    "\n",
    "    for sidx, seed in enumerate(seeds):\n",
    "        seed_score = 0\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n",
    "\n",
    "        for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n",
    "            counter += 1\n",
    "\n",
    "            train_x_id, train_x_mask, train_x_token, train_y = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train], Ytrain[train]\n",
    "            val_x_id, val_x_mask, val_x_token, val_y = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val], Ytrain[val]\n",
    "\n",
    "            model = dnn_model(transformer_model, max_len+16)\n",
    "            \n",
    "            '''\n",
    "            for layer in model.layers[:3]:\n",
    "                layer.trainable = False\n",
    "            '''\n",
    "\n",
    "            model.compile(loss=rmse_loss,\n",
    "                          metrics=[RootMeanSquaredError(name='rmse')],\n",
    "                          optimizer=Adam(lr=8e-5))\n",
    "\n",
    "            early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n",
    "                                  restore_best_weights=True, \n",
    "                                  patience=5, verbose=VERBOSE)\n",
    "\n",
    "            reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.25, \n",
    "                                          min_lr=1e-8, patience=2, \n",
    "                                          verbose=VERBOSE, mode='min')\n",
    "\n",
    "            chk_point = ModelCheckpoint(f'./CommonLit_Bert_Base_Uncased_Model_{seed}S_{(idx+1)}F.h5', \n",
    "                                        monitor='val_rmse', verbose=VERBOSE, \n",
    "                                        save_best_only=True, mode='min',\n",
    "                                        save_weights_only=True)\n",
    "\n",
    "            history = model.fit(\n",
    "                [train_x_id, train_x_mask, train_x_token], train_y, \n",
    "                batch_size=mini_batch_size,\n",
    "                epochs=15, \n",
    "                verbose=VERBOSE, \n",
    "                callbacks=[reduce_lr, early, chk_point], \n",
    "                validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n",
    "            )\n",
    "            \n",
    "            model.load_weights(f'./CommonLit_Bert_Base_Uncased_Model_{seed}S_{(idx+1)}F.h5')\n",
    "\n",
    "            y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n",
    "            score = np.sqrt(mean_squared_error(val_y, y_pred))\n",
    "            oof_score += score\n",
    "            seed_score += score\n",
    "            print(\"\\nSeed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n",
    "\n",
    "        print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score /= float(counter)\n",
    "print(\"Aggregate OOF Score: {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
