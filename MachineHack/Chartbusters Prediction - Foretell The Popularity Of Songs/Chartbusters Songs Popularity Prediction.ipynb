{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "\n",
    "One of our customers strongly believes in technology and have recently backed up its platform using Machine Learning and Artificial Intelligence. Based on data collected from multiple sources on different songs and various artist attributes our customer is excited to challenge the MachineHack community.\n",
    "\n",
    "By analyzing, the chartbusters data to predict the Views of songs, MachineHackers would advance the state of the current platform. This can help our customer understand user behaviour and personalize the user experience. \n",
    "In this hackathon, we challenge the MachineHackers to come up with a prediction algorithm that can predict the views for a given song.\n",
    "\n",
    "Can you predict how popular a song will be in the future?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "- Data_Train.csv – the training set, 78458 rows with 11 columns.\n",
    "- Data_Test.csv – the test set, 19615 rows with 10 columns, except the Views column.\n",
    "- Sample_Submission.csv – sample submission file format for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "- **Unique_ID** : Unique Identifier.\n",
    "- **Name** : Name of the Artist.\n",
    "- **Genre** : Genre of the Song.\n",
    "- **Country** : Origin Country of Artist.\n",
    "- **Song_Name** : Name of the Song.\n",
    "- **Timestamp** : Release Date and Time.\n",
    "- **Views** : Number of times the song was played/viewed (*Target/Dependent Variable*).\n",
    "- **Comments** : Count of comments for the song.\n",
    "- **Likes** : Count of Likes.\n",
    "- **Popularity** : Popularity score for the artist.\n",
    "- **Followers** : Number of Followers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningratefinder import LearningRateFinder\n",
    "from clr_callback import CyclicLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set file paths for train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"Datasets/Data_Train.csv\"\n",
    "test_dataset = \"Datasets/Data_Test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train/predict data into pandas dataframes\n",
    "train_df = pd.read_csv(train_dataset)\n",
    "predict_df = pd.read_csv(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows, from train_df, having any column value as NaN\n",
    "train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y: (78457, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract \"Views\" field from train_df into NumPy array\n",
    "train_y = np.array([train_df['Views'].values]).T\n",
    "train_df.drop(['Views'], inplace=True, axis=1)\n",
    "print(\"train_y: {}\".format(train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98072, 10)\n"
     ]
    }
   ],
   "source": [
    "# Combine the train and predict dataframes\n",
    "combined_df = train_df.append(predict_df, sort=False, ignore_index=True)\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Country</th>\n",
       "      <th>Song_Name</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Followers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>413890</td>\n",
       "      <td>Hardstyle</td>\n",
       "      <td>danceedm</td>\n",
       "      <td>AU</td>\n",
       "      <td>N-Vitral presents BOMBSQUAD - Poison Spitter (...</td>\n",
       "      <td>2018-03-30 15:24:45.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>499</td>\n",
       "      <td>97</td>\n",
       "      <td>119563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>249453</td>\n",
       "      <td>Dj Aladdin</td>\n",
       "      <td>danceedm</td>\n",
       "      <td>AU</td>\n",
       "      <td>Dj Aladdin - Old School Hip Hop Quick Mix</td>\n",
       "      <td>2016-06-20 05:58:52.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>49</td>\n",
       "      <td>17</td>\n",
       "      <td>2141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>681116</td>\n",
       "      <td>Maxximize On Air</td>\n",
       "      <td>danceedm</td>\n",
       "      <td>AU</td>\n",
       "      <td>Maxximize On Air - Mixed by Blasterjaxx - Epis...</td>\n",
       "      <td>2015-05-08 17:45:59.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>312</td>\n",
       "      <td>91</td>\n",
       "      <td>22248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>387253</td>\n",
       "      <td>GR6 EXPLODE</td>\n",
       "      <td>rbsoul</td>\n",
       "      <td>AU</td>\n",
       "      <td>MC Yago - Tenho Compromisso (DJ R7)</td>\n",
       "      <td>2017-06-08 23:50:03.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2,400</td>\n",
       "      <td>76</td>\n",
       "      <td>393655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1428029</td>\n",
       "      <td>Tritonal</td>\n",
       "      <td>danceedm</td>\n",
       "      <td>AU</td>\n",
       "      <td>Escape (feat. Steph Jones)</td>\n",
       "      <td>2016-09-17 20:50:19.000000</td>\n",
       "      <td>81</td>\n",
       "      <td>3,031</td>\n",
       "      <td>699</td>\n",
       "      <td>201030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2839</td>\n",
       "      <td>k$upreme</td>\n",
       "      <td>all-music</td>\n",
       "      <td>AU</td>\n",
       "      <td>Started Off Finessen' (Prod.Oscar100)</td>\n",
       "      <td>2017-11-27 14:55:11.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>4,500</td>\n",
       "      <td>325</td>\n",
       "      <td>71038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>414871</td>\n",
       "      <td>Hardstyle</td>\n",
       "      <td>danceedm</td>\n",
       "      <td>AU</td>\n",
       "      <td>Coone - Universal Language (Cyber Remix)</td>\n",
       "      <td>2016-01-22 17:23:26.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>1,017</td>\n",
       "      <td>226</td>\n",
       "      <td>119563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>209496</td>\n",
       "      <td>Diplo</td>\n",
       "      <td>danceedm</td>\n",
       "      <td>AU</td>\n",
       "      <td>Pick Your Poison (feat. Kay) (Figure Remix)</td>\n",
       "      <td>2012-01-17 00:00:00.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>12</td>\n",
       "      <td>7120051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>967409</td>\n",
       "      <td>Nick Vanelli</td>\n",
       "      <td>trap</td>\n",
       "      <td>AU</td>\n",
       "      <td>B l o o d s h e d</td>\n",
       "      <td>2018-11-29 22:37:07.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>171948</td>\n",
       "      <td>DeejayEcko(PNCS)</td>\n",
       "      <td>latin</td>\n",
       "      <td>AU</td>\n",
       "      <td>CHIHUAHUA MIXDOWN [ Instagram : @deejayeckoo ]</td>\n",
       "      <td>2017-09-28 04:07:47.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>622</td>\n",
       "      <td>47</td>\n",
       "      <td>2835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID              Name      Genre Country  \\\n",
       "0     413890         Hardstyle   danceedm      AU   \n",
       "1     249453        Dj Aladdin   danceedm      AU   \n",
       "2     681116  Maxximize On Air   danceedm      AU   \n",
       "3     387253       GR6 EXPLODE     rbsoul      AU   \n",
       "4    1428029          Tritonal   danceedm      AU   \n",
       "5       2839          k$upreme  all-music      AU   \n",
       "6     414871         Hardstyle   danceedm      AU   \n",
       "7     209496             Diplo   danceedm      AU   \n",
       "8     967409      Nick Vanelli       trap      AU   \n",
       "9     171948  DeejayEcko(PNCS)      latin      AU   \n",
       "\n",
       "                                           Song_Name  \\\n",
       "0  N-Vitral presents BOMBSQUAD - Poison Spitter (...   \n",
       "1          Dj Aladdin - Old School Hip Hop Quick Mix   \n",
       "2  Maxximize On Air - Mixed by Blasterjaxx - Epis...   \n",
       "3                MC Yago - Tenho Compromisso (DJ R7)   \n",
       "4                         Escape (feat. Steph Jones)   \n",
       "5              Started Off Finessen' (Prod.Oscar100)   \n",
       "6           Coone - Universal Language (Cyber Remix)   \n",
       "7        Pick Your Poison (feat. Kay) (Figure Remix)   \n",
       "8                                  B l o o d s h e d   \n",
       "9     CHIHUAHUA MIXDOWN [ Instagram : @deejayeckoo ]   \n",
       "\n",
       "                    Timestamp  Comments  Likes Popularity  Followers  \n",
       "0  2018-03-30 15:24:45.000000         4    499         97     119563  \n",
       "1  2016-06-20 05:58:52.000000        17     49         17       2141  \n",
       "2  2015-05-08 17:45:59.000000        11    312         91      22248  \n",
       "3  2017-06-08 23:50:03.000000         2  2,400         76     393655  \n",
       "4  2016-09-17 20:50:19.000000        81  3,031        699     201030  \n",
       "5  2017-11-27 14:55:11.000000         6  4,500        325      71038  \n",
       "6  2016-01-22 17:23:26.000000        15  1,017        226     119563  \n",
       "7  2012-01-17 00:00:00.000000         5     88         12    7120051  \n",
       "8  2018-11-29 22:37:07.000000         0     28          7       1892  \n",
       "9  2017-09-28 04:07:47.000000         0    622         47       2835  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size after encoding: (98072, 1229)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding for \"Name\" field\n",
    "dummy_val = pd.get_dummies(combined_df['Name'], prefix='Name')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Dataframe size after encoding: {}\".format(combined_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size after encoding: (98072, 1250)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding for \"Genre\" field\n",
    "dummy_val = pd.get_dummies(combined_df['Genre'], prefix='Name')\n",
    "combined_df = pd.concat([combined_df, dummy_val], axis=1)\n",
    "print(\"Dataframe size after encoding: {}\".format(combined_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence encoding for \"Song_Name\" field\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "song_name_embed = np.array(model(combined_df.Song_Name))\n",
    "song_name_embed_df = pd.DataFrame(song_name_embed)\n",
    "combined_df = pd.merge(combined_df, song_name_embed_df, left_index=True, right_index=True)\n",
    "print(\"Dataframe size after encoding: {}\".format(combined_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size after encoding: (98072, 1769)\n"
     ]
    }
   ],
   "source": [
    "# Extract new features from \"Timestamp\" field\n",
    "combined_df['rel_year'] = combined_df['Timestamp'].map(lambda x: pd.to_datetime(x).year)\n",
    "combined_df['rel_quarter'] = combined_df['Timestamp'].map(lambda x: pd.to_datetime(x).quarter)\n",
    "combined_df['rel_month'] = combined_df['Timestamp'].map(lambda x: pd.to_datetime(x).month)\n",
    "combined_df['rel_week'] = combined_df['Timestamp'].map(lambda x: pd.to_datetime(x).week)\n",
    "combined_df['rel_day_year'] = combined_df['Timestamp'].map(lambda x: pd.to_datetime(x).dayofyear)\n",
    "combined_df['rel_day_month'] = combined_df['Timestamp'].map(lambda x: pd.to_datetime(x).day)\n",
    "combined_df['rel_day_week'] = combined_df['Timestamp'].map(lambda x: pd.to_datetime(x).dayofweek)\n",
    "print(\"Dataframe size after encoding: {}\".format(combined_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size after encoding: (98072, 1769)\n"
     ]
    }
   ],
   "source": [
    "# Cleanse data in \"Likes\" field\n",
    "combined_df['Likes'] = combined_df['Likes'].map(lambda x: x.replace(\",\", \"\"))\n",
    "combined_df['Likes'] = (combined_df.Likes.replace(r'[KM]+$', '', regex=True).astype(float) * \n",
    "                        combined_df.Likes.str.extract(r'[\\d\\.]+([KM]+)', expand=False)\n",
    "                        .fillna(1).replace(['K','M'], [10**3, 10**6]).astype(int))\n",
    "print(\"Dataframe size after encoding: {}\".format(combined_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size after encoding: (98072, 1769)\n"
     ]
    }
   ],
   "source": [
    "# Cleanse data in \"Popularity\" field\n",
    "combined_df['Popularity'] = combined_df['Popularity'].map(lambda x: x.replace(\",\", \"\"))\n",
    "print(\"Dataframe size after encoding: {}\".format(combined_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Comments\" and \"Followers\" fields to 'float64' datatype\n",
    "combined_df['Comments'] = combined_df['Comments'].astype('float64')\n",
    "combined_df['Followers'] = combined_df['Followers'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column with NaN value: []\n"
     ]
    }
   ],
   "source": [
    "# Drop redundant columns\n",
    "combined_df.drop(['Unique_ID', 'Name', 'Genre', 'Country', 'Song_Name', 'Timestamp'], inplace=True, axis=1)\n",
    "\n",
    "# Check if any column has NaN value in dataframe\n",
    "print(\"Column with NaN value: {}\".format(combined_df.columns[combined_df.isnull().any()].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (78457, 1763)\n",
      "predict_x: (19615, 1763)\n"
     ]
    }
   ],
   "source": [
    "# Segregate combined_df into train/predict datasets\n",
    "train_x = combined_df[:78457]\n",
    "predict_x = combined_df[78457:]\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"predict_x: {}\".format(predict_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xvalidation, Ytrain, Yvalidation = train_test_split(train_x, train_y, test_size=0.05, random_state=10)\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(Xtrain, Ytrain, test_size=0.05, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (70807, 1763) \n",
      "Ytrain: (70807, 1)\n",
      "Xvalidation: (3923, 1763) \n",
      "Yvalidation: (3923, 1)\n",
      "Xtest: (3727, 1763) \n",
      "Ytest: (3727, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain: {} \\nYtrain: {}\".format(Xtrain.shape, Ytrain.shape))\n",
    "print(\"Xvalidation: {} \\nYvalidation: {}\".format(Xvalidation.shape, Yvalidation.shape))\n",
    "print(\"Xtest: {} \\nYtest: {}\".format(Xtest.shape, Ytest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the datasets in NPZ file (for reusability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('Datasets/Chartbusters_Songs_Popularity_Prediction_dataset.npz',\n",
    "                    Xtrain=Xtrain, Ytrain=Ytrain,\n",
    "                    Xvalidation=Xvalidation, Yvalidation=Yvalidation,\n",
    "                    Xtest=Xtest, Ytest=Ytest,\n",
    "                    Xpredict=predict_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets from the NPZ file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training, holdout and test datasets from processed file\n",
    "processed_dataset = np.load('Datasets/Chartbusters_Songs_Popularity_Prediction_dataset.npz', allow_pickle=True)\n",
    "Xtrain, Ytrain = processed_dataset['Xtrain'], processed_dataset['Ytrain']\n",
    "Xvalidation, Yvalidation = processed_dataset['Xvalidation'], processed_dataset['Yvalidation']\n",
    "Xtest, Ytest = processed_dataset['Xtest'], processed_dataset['Ytest']\n",
    "Xpredict = processed_dataset['Xpredict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain: (70807, 1763) \n",
      "Ytrain: (70807, 1)\n",
      "Xvalidation: (3923, 1763) \n",
      "Yvalidation: (3923, 1)\n",
      "Xtest: (3727, 1763) \n",
      "Ytest: (3727, 1)\n",
      "Xpredict: (19615, 1763)\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain: {} \\nYtrain: {}\".format(Xtrain.shape, Ytrain.shape))\n",
    "print(\"Xvalidation: {} \\nYvalidation: {}\".format(Xvalidation.shape, Yvalidation.shape))\n",
    "print(\"Xtest: {} \\nYtest: {}\".format(Xtest.shape, Ytest.shape))\n",
    "print(\"Xpredict: {}\".format(Xpredict.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(num_of_features):\n",
    "    \"\"\"\n",
    "        Description: Function to build the neural network model\n",
    "        \n",
    "        Parameters:\n",
    "            num_of_features: Number of features in input data\n",
    "        \n",
    "        Return:\n",
    "            model - Keras neural network Model\n",
    "    \"\"\"\n",
    "\n",
    "    # Input Layer\n",
    "    x_input = Input(shape=(num_of_features, ), name='INPUT')\n",
    "\n",
    "    # Fully-connected Layer 1\n",
    "    x = Dense(units=512, name='FC-1', activation='relu')(x_input)\n",
    "    x = BatchNormalization(name='BN_FC-1')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-1')(x)\n",
    "\n",
    "    # Fully-connected Layer 2\n",
    "    x = Dense(units=128, name='FC-2', activation='relu')(x)\n",
    "    x = BatchNormalization(name='BN_FC-2')(x)\n",
    "    x = Dropout(rate=0.5, name='DROPOUT_FC-2')(x)\n",
    "\n",
    "    # Output Layer\n",
    "    x = Dense(units=1, name='OUTPUT')(x)\n",
    "\n",
    "    # Create Keras Model instance\n",
    "    model = Model(inputs=x_input, outputs=x, name='Chartbusters_Songs_Popularity_Predictor')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model hyperparameters\n",
    "max_iterations = 50\n",
    "mini_batch_size = 128\n",
    "min_lr = 1e-4\n",
    "max_lr = 1e-2\n",
    "step_size = 8 * (train_x.shape[0] // mini_batch_size)\n",
    "clr_method = 'triangular2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = nn_model(Xtrain.shape[1])\n",
    "\n",
    "# Compile model to configure the learning process\n",
    "model.compile(loss='mse',\n",
    "              optimizer=Adam(lr=min_lr),\n",
    "              metrics=['mean_squared_logarithmic_error'])\n",
    "\n",
    "# Triangular learning rate policy\n",
    "clr = CyclicLR(base_lr=min_lr, max_lr=max_lr, mode=clr_method, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Chartbusters_Songs_Popularity_Predictor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "INPUT (InputLayer)           [(None, 1763)]            0         \n",
      "_________________________________________________________________\n",
      "FC-1 (Dense)                 (None, 512)               903168    \n",
      "_________________________________________________________________\n",
      "BN_FC-1 (BatchNormalization) (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "DROPOUT_FC-1 (Dropout)       (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FC-2 (Dense)                 (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "BN_FC-2 (BatchNormalization) (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "DROPOUT_FC-2 (Dropout)       (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "OUTPUT (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 971,521\n",
      "Trainable params: 970,241\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-4d025d0fd92a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m          \u001b[0mstartLR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendLR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m          \u001b[0mstepsPerEpoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m          batchSize=mini_batch_size)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mlrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\MLProjects\\ML Competitions\\MachineHack\\Chartbusters Prediction - Foretell The Popularity Of Songs\\learningratefinder.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(self, trainData, startLR, endLR, epochs, stepsPerEpoch, batchSize, sampleSize, verbose)\u001b[0m\n\u001b[0;32m    150\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m \t\t\t\tverbose=verbose)\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[1;31m# restore the original model weights and learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    607\u001b[0m   \u001b[1;31m# As a fallback for the data type that does not work with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m   \u001b[1;31m# _standardize_user_data, use the _prepare_model_with_inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[0;32m    320\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m     ))\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    412\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m--> 414\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m   2333\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2335\u001b[1;33m     \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2336\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2337\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[1;34m(element)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m           normalized_components.append(\n\u001b[1;32m--> 111\u001b[1;33m               ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[0;32m    112\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[0;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[0;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[1;32m-> 1184\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    225\u001b[0m   \"\"\"\n\u001b[0;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 227\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "# Learning Rate Finder\n",
    "lrf = LearningRateFinder(model)\n",
    "lrf.find((Xtrain, Ytrain),\n",
    "         startLR=1e-10, endLR=1e-1,\n",
    "         stepsPerEpoch=np.ceil((len(Xtrain) / float(mini_batch_size))),\n",
    "         batchSize=mini_batch_size)\n",
    "lrf.plot_loss()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=Xtrain, y=Ytrain, \n",
    "                    batch_size=256, epochs=100, \n",
    "                    callbacks=[clr], workers=5,\n",
    "                    validation_data=(Xvalidation, Yvalidation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/evaluate the model\n",
    "score = model.evaluate(x=Xtest, y=Ytest, verbose=0)\n",
    "print('Test loss: {}', format(score[0]))\n",
    "print('Test accuracy: {}', format(score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.title(\"Model Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_squared_logarithmic_error'], label='train_msle')\n",
    "plt.plot(history.history['val_mean_squared_logarithmic_error'], label='val_msle')\n",
    "plt.ylabel('Mean Squared Logarithmic Error')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.title(\"MSLE Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clr.history[\"lr\"])\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.title(\"Cyclical Learning Rate (CLR)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
